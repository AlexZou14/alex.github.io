<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>秩同道合的小站</title>
  
  <subtitle>寻找志趣相投的伙伴！</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://alexzou14.github.io/"/>
  <updated>2020-04-05T16:54:36.505Z</updated>
  <id>http://alexzou14.github.io/</id>
  
  <author>
    <name>秩同道合</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Deeply-Recursive Convolutional Network for Image Super-Resolution论文阅读笔记</title>
    <link href="http://alexzou14.github.io/2020/04/05/DRCN/"/>
    <id>http://alexzou14.github.io/2020/04/05/DRCN/</id>
    <published>2020-04-05T15:41:36.000Z</published>
    <updated>2020-04-05T16:54:36.505Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="https://cdn.jsdelivr.net/npm/aplayer@1.10/dist/APlayer.min.css"><script src="https://cdn.jsdelivr.net/npm/aplayer@1.10/dist/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="https://cdn.jsdelivr.net/npm/meting@1.2/dist/Meting.min.js"></script><h3 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h3><p>本文提出了使用深度递归卷积网络（DRCN）的图像超分辨率方法（SR）。 DRCN有一个非常深的递归层（最多16个递归）。增加递归深度可以提高性能，而不会为附加卷积引入新参数。由于网络特别深，所以存在梯度爆炸的问题，无法用标准梯度下降来学习。作者就利用率跳跃连接和递归监督的方式解决了这一问题。</p><hr><h3 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h3><p>对于图像超分辨率（SR），卷积网络的感受野决定了可以利用的上下文信息的量来推断丢失的高频分量。由于SR是一个不定逆向问题，收集和分析更多的邻域像素可对恢复下采样后可能丢失的信息提供更多线索。各种计算机视觉任务的深度卷积网络（DCN）通常使用非常大的感受野，在扩展感受野的许多方法中，增加网络深度是一种可能的方式。利用大于1×1的过滤器大小的卷积（转换）层或者减少中间表示的维度的池层可以使用的。 这两种方法都有缺点：层引入更多的参数和池层。网络层通常丢弃一些像素级别的信息。<br>对于图像恢复问题，如超分辨率和去噪，图像细节非常重要。因此，这些问题的大多数深度学习方法不能使用池化技术。通过添加新的网络层来增加深度，基本上会引入更多的参数。可能会出现两个问题。首先，很可能会过度拟合，需要更多的训练数据，使模型变得越来越大。<br>为了解决这些问题，作者使用深度递归卷积网络（DRCN）。DRCN根据需要重复地使用相同的卷积层参数。参数的数量不会增加，而执行了更多的递归。作者发现用广泛使用的随机梯度下降法优化的DRCN不容易收敛。这是由于爆炸/消失的梯度。提出了一个高性能的模型结构，能够捕捉像素长程的依赖，在保持较小模型的情况下，有更宽的感受野。<br>作者提出两种方法来缓解训练的难度：对所有递归进行监督，使用从输入到重建层的跳跃连接。</p><hr><h3 id="Related-work"><a href="#Related-work" class="headerlink" title="Related work"></a>Related work</h3><h4 id="Single-Image-Super-Resolution"><a href="#Single-Image-Super-Resolution" class="headerlink" title="Single-Image Super-Resolution"></a>Single-Image Super-Resolution</h4><p>早期单一图像超分辨率早期使用插值的方法，但是效果很差，后面又提出了统计先验和内部patch方法，最近基于学习的方法SRCN的提出，证明了端到端的方法在SR问题上的可行性。</p><h4 id="Recursive-Neural-Network-in-Computer-Vision"><a href="#Recursive-Neural-Network-in-Computer-Vision" class="headerlink" title="Recursive Neural Network in Computer Vision"></a>Recursive Neural Network in Computer Vision</h4><p>递归神经网络在语义分割和特征提取上有很多的应用，但是在图像超分上还是没有应用。</p><hr><h3 id="Proposed-Method"><a href="#Proposed-Method" class="headerlink" title="Proposed Method"></a>Proposed Method</h3><h4 id="Basic-Model"><a href="#Basic-Model" class="headerlink" title="Basic Model"></a>Basic Model</h4><p>整个网络分三个部分，每个部分都只有一层隐层，只有inference 网络是递归的。<br>结构如下图：</p><blockquote><p><img src="http://sotavision.cn/showdoc/server/../Public/Uploads/2020-04-05/5e89e70041905.png" alt="" loading="lazy"></p></blockquote><p><strong>Embedding network</strong>:用于从RGB和灰度图像产生feature map，相当于特征提取,可以用一下表达式表示：<br>\( H_{-1}=\max (0,W_{-1} \ast \text{x}+b_{-1}) \)<br>\( H_{0}=\max (0,W_{0}\ast H_{-1}+b_{0}) \)<br>\( f_{1}(\text x)=H_0 \)<br>其中 \( f_1(\text{x}) \) 表示特征提取的输出。<br><strong>Inference network</strong>:解决超分辨率，相当于特征的非线性变换,其中 \( g(H)=\max(0,W \ast H+b) \) ，递归可以表示为：<br>\( H_d=g(H_{d-1})=\max(0,W \ast H_{d-1}+b) \),<br>故递归层输出为：\( f_2(H)=(g\circ g\circ g\circ \cdots \circ)g(H)=g^D(H) \)<br><strong>reconstruction network</strong>:用于把多个通道的转成三个通道的图片。重建层可以表达为：<br>$$H_{D+1}=\max(0,W_{D+1} \ast H_D+b_{D+1})$$<br>\( \hat{y}=\max(0,W_{D+2} \ast H_{D+1}+b_{D+2}) \)，\( f_3(H)=\hat{y} \)</p><p>这个模型的优点就是简单强大，缺点就是递归层数多的时候很难训练，会产生梯度爆炸。</p><h4 id="Advanced-Model"><a href="#Advanced-Model" class="headerlink" title="Advanced Model"></a>Advanced Model</h4><p>进阶网络结构如下：</p><blockquote><p><img src="http://sotavision.cn/showdoc/server/../Public/Uploads/2020-04-05/5e89ed8fe90af.png" alt="" loading="lazy"></p></blockquote><p><strong>Recursive-Supervision</strong>：<br>为了解决梯度爆炸和寻找最佳递归次数的问题，提出了一种进一步的模型，所以递归上增加一个监督。监督可以表示为: </p><div>\[ \hat{ y}_d = f_3(\text{x},g^{(d)}(f_1(\text x))) \]</div>，其中 <div>\[ f_3(\text x, H_d)=\text x+f_3(H_d) \]</div>，<p>然后最终输出可以表达为: \( \hat{y}= \displaystyle\sum_{d=1}^D {w_d \cdot \hat{\text y}_d} \)</p><p><strong>skip-connection</strong>:使用该技巧的原因是超分辨率重建，低分辨率图像和网络输出的高分辨率图像有很高的相关性，而随着递归网络深度加深学习不到这种线性映射，所以通过该技巧可以使用大部分低分辨率信息用于重建高分辨率图像。</p><p>其中的\( H_1 \)到是\( H_D \)个共享参数的卷积层。DRCN将每一层的卷积结果都通过同一个Reconstruction Net得到一个重建结果，从而共得到D个重建结果，再把它们加权平均得到最终的输出。另外，受到ResNet的启发，DRCN通过skip connection将输入图像与\( H_d \)的输出相加后再作为Reconstruction Net的输入，相当于使Inference Net去学习高分辨率图像与低分辨率图像的差，即恢复图像的高频部分。</p><h4 id="Loss-function"><a href="#Loss-function" class="headerlink" title="Loss function"></a>Loss function</h4><p>中间层的loss function为：</p><div>    \[ l_1(\theta)= \displaystyle\sum_{d=1}^D \displaystyle\sum_{i=1}^N{ \dfrac{1}{2DN}|| \text y^{(i)}-\hat{\text y}_d^{(i)}||^2} \]</div>最后一层的loss function为：<div>    \[ l_2(\theta)= \displaystyle\sum_{i=1}^N{ \dfrac{1}{2N}|| \text y^{(i)}- \displaystyle\sum_{d=1}^D w_d \cdot \hat{\text y}_d^{(i)}||^2} \]</div><p>最终的loss function为：\( L(\theta)=\alpha l_1(\theta)+(1-\alpha)l_2(\theta)+\beta||\theta||^2 \)其中\( \alpha \)和\( \beta \)为可以调参数。</p><hr><h3 id="Experiment"><a href="#Experiment" class="headerlink" title="Experiment"></a>Experiment</h3><h4 id="Experiments-Details"><a href="#Experiments-Details" class="headerlink" title="Experiments Details"></a>Experiments Details</h4><p>实现细节待复现</p><h4 id="Study-of-Deep-Recursions"><a href="#Study-of-Deep-Recursions" class="headerlink" title="Study of Deep Recursions"></a>Study of Deep Recursions</h4><p>作者发现recursion d越大，psnr越高</p><blockquote><p><img src="http://sotavision.cn/showdoc/server/../Public/Uploads/2020-04-05/5e89f8ca3719c.png" alt="" loading="lazy"></p></blockquote><p>并且发现如果不是recursion的unit全部连到final output，而是单一的某一层，效果会变差，证明了recursion ensemble的效果</p><blockquote><p><img src="http://sotavision.cn/showdoc/server/../Public/Uploads/2020-04-05/5e89f945719f7.png" alt="" loading="lazy"></p></blockquote><h4 id="Comparisons-with-state-of-the-arts"><a href="#Comparisons-with-state-of-the-arts" class="headerlink" title="Comparisons with state-of-the-arts"></a>Comparisons with state-of-the-arts</h4><blockquote><p><img src="http://sotavision.cn/showdoc/server/../Public/Uploads/2020-04-05/5e89f97d92e5b.png" alt="" loading="lazy"></p></blockquote><hr><h3 id="Conclusions"><a href="#Conclusions" class="headerlink" title="Conclusions"></a>Conclusions</h3><ol><li>作者提出了一种使用深度递归卷积网络的超分辨率方法，有效地共享权重参数</li><li>使用递归监督和跳跃连接解决梯度爆炸和最佳递归层数问题</li><li>作者的方法在当时超分问题取得了较好的效果</li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;https://cdn.jsdelivr.net/npm/aplayer@1.10/dist/APlayer.min.css&quot;&gt;&lt;script 
      
    
    </summary>
    
    
      <category term="论文笔记" scheme="http://alexzou14.github.io/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="深度学习" scheme="http://alexzou14.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="笔记" scheme="http://alexzou14.github.io/tags/%E7%AC%94%E8%AE%B0/"/>
    
      <category term="超分辨率" scheme="http://alexzou14.github.io/tags/%E8%B6%85%E5%88%86%E8%BE%A8%E7%8E%87/"/>
    
  </entry>
  
  <entry>
    <title>FSRNet：End-to-End Learning Face Super-Resolution with Facial Priors论文阅读笔记</title>
    <link href="http://alexzou14.github.io/2020/04/02/FSRNet/"/>
    <id>http://alexzou14.github.io/2020/04/02/FSRNet/</id>
    <published>2020-04-02T09:03:11.000Z</published>
    <updated>2020-04-02T09:34:32.089Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="https://cdn.jsdelivr.net/npm/aplayer@1.10/dist/APlayer.min.css"><script src="https://cdn.jsdelivr.net/npm/aplayer@1.10/dist/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="https://cdn.jsdelivr.net/npm/meting@1.2/dist/Meting.min.js"></script><h3 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h3><p>人脸超分辨率是图像超分辨率的一个特殊任务，特别的人脸先验信息可以更好的被利用，还原出更好的超清图像。作者发现，现有的方法在恢复非常低的超分辨率人脸图像存在问题，恢复不清晰的情况。因此提出了一个端到端的训练方式和充分利用人脸的几何先验知识来组成的一个网络。该网络先构建了一个粗SR网络来恢复出一个粗高分辨率图片，再将粗HR图像送去两个分支网络中，一个是细SR编码，另一个是先验信息评估，细SR编码用来提取特征图，先验信息评估用来评价估计解析图。再将得到的特征图和先验信息送入细SR编码还原HR图像。为了生成更真实的图像，作者还提出了一个FSRGAN将对抗损失引入FSRNet中。</p><hr><h3 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h3><p>面部超分辨率（SR）是特定的一类图像超分辨率问题。目前大多数人脸图像超分辨算法是由通用的图像超分辨算法加以适当修改得到的。之前的面部超分辨率方法大多是由通用的图像超分辨率算法加以适当修改得到的，大多数没有加人脸先验知识，所以导致恢复效果不好。并且用于人脸超分辨率的图像不是端到端的训练。这篇文章受到人脸几何先验和端到端的训练的启发，提出了一个端到端的深度可训练面部超分辨网络，充分利用人脸图像的几何先验信息，即面部landmark的heatmap和人脸解析图，来对低分辨率人脸图像进行超分辨率。<br>本文主要贡献有：</p><ul><li>第一个提出用人脸几何先验的知识进行端到端学习的人脸超分辨率方法。</li><li>同时引入了两种几何先验，face landmark 和面部解析</li><li>提出的FSRnet在模糊未对齐和非常低的分辨率的图像，通过8倍放大，是目前最好的水平。同时用FSRnetGAN网络可以进一步生成更加逼真的images。</li><li>对于人脸超分辨率，人脸对齐和面部解析 最为新的评价标准。进一步证明，该方法可以解决传统的视觉感知度量方法的不一致性。</li></ul><p>选择形状作为先验的两种考虑：首先，当分辨率从高到低时，形状比纹理保存得更好，因此更有可能被提取出来以提高超分辨率。形状的表示要比纹理的表示好一些 。人脸解析 是不同人脸组成的分割估计。</p><blockquote><p><img src="http://sotavision.cn/showdoc/server/../Public/Uploads/2020-03-26/5e7c114a3c0a8.png" alt="FSRNet1" loading="lazy"></p></blockquote><hr><h3 id="Related-work"><a href="#Related-work" class="headerlink" title="Related work"></a>Related work</h3><h4 id="Facial-Prior-Knowledge"><a href="#Facial-Prior-Knowledge" class="headerlink" title="Facial Prior Knowledge"></a>Facial Prior Knowledge</h4><p>有很多使用了人脸先验信息的人脸超分方法，都是为了更好的从低分辨率图像恢复成高分辨率图像。但是早期的方法在低放缩倍率下估计人脸先验是非常困难的。随着近期深度卷积网络在人脸超分上的引用成功， 宋等人提出了一种两阶段的方法，首先生成面部COMP由CNNs合成，然后通过成分增强方法合成细粒度的面部结构。与上述方法不同的是，我们的FSRNet完全LEV 擦除面部地标热图和解析地图的端到端培训方式。</p><h4 id="End-to-end-Training"><a href="#End-to-end-Training" class="headerlink" title="End-to-end Training"></a>End-to-end Training</h4><p>端到端的训练模式被广泛运用在一般的图像超分问题中，DRRN的提出有效的解决了网络参数数量和网络准确率的问题。与上述仅依靠深层模型力量的方法不同，我们的FSRNet不仅是一个端到端的可训练神经网络，而且结合了来自人脸先验的丰富信息，更好的将低分辨率人脸图像还原成高分辨率图像。</p><hr><h3 id="Proposed-Method"><a href="#Proposed-Method" class="headerlink" title="Proposed Method"></a>Proposed Method</h3><h4 id="Network-structure"><a href="#Network-structure" class="headerlink" title="Network structure"></a>Network structure</h4><p>网络结构如下图：</p><blockquote><p><img src="http://sotavision.cn/showdoc/server/../Public/Uploads/2020-03-26/5e7c16555126e.png" alt="FSRNet2" loading="lazy"></p></blockquote><p>FSRNet是由四个部分组成：coarse SR network, fine SR encoder, prior estimation network和fine SR decoder</p><p>粗SR网络将输入的LR图像初步还原成一个粗SR图像,这个粗SR网络可以表达成：$$y_c=C(\text{x})$$<br><strong>先验信息估计网络</strong>：从最近成功的叠加热图回归在人体姿势估计中受到启发，文章提出在先验信息估计网络中使用一个 HourGlass 结构来估计面部landmark 的 heatmap 和解析图。因为这两个先验信息都可以表示 2D的人脸形状，所以在先验信息估计网络中，特征在两个任务之间是共享的， 除了最后一层。为了有效整合各种尺度的特征并保留不同尺度的空间信息，HourGlass block 在对称层之间使用 skip-connection 机制。最后，共享的 HG 特征连接到两个分离的 1×1 卷积层来生成 landmark heatmap和解析图。<br><strong>精细的SR编码器</strong>：受到 ResNet 在超分辨任务中的成功的启发，文章使用 residual block 进行特征提取。考虑到计算的开销，先验信息的特征会降采样到 64×64。为了使得特征尺寸一致，编码器首先经过一个 3×3，stride为 2 的卷积层来把特征图降采样到 64×64。然后再使用 ResNet 结构提取图像特征。</p><p>对应的先验评估网络P和细SR编码网络F分别可以表示成：<br>$$\text{p}=P(\text{y}_c)$$,$$\text{f}=F(\text{y}_c)$$<br><strong>精细的SR解码器</strong>：解码器把先验信息和图像特征组合为输入，首先将先验特征 p 和图像特征 f 进行concatenate，作为输入。然后通过 3×3 的卷积层把特征图的通道数减少为 64。然后一个 4×4 的反卷积层被用来把特征图的 size 上采样到 128×128。然后使用 3 个 residual block 来对特征进行解码。最后的 3×3 卷积层被用来得到最终的 HR 图像。</p><p>将coarse SRimages送入特征提取和先验估计网络中:<br>$$\text{y}=D(\text{f},\text{p})$$</p><h4 id="Loss-function"><a href="#Loss-function" class="headerlink" title="Loss function"></a>Loss function</h4><p>给定训练集：\( \{\text{x}^{(i)},\tilde{\text{y}}^{(i)},\tilde{\text{p}}^{(i)}\}_{i=1}^N \),FSRNet的损失函数为( \( \tilde{\text{y}},\tilde{\text{p}} \)为ground truth):</p><p>$$L_F(\Theta)=\dfrac{1}{2N}\sum_{i=1}^N { {\left|{\tilde{\text{y}}^{(i)}-\text{y}_c^{(i)}}\right|^2+\left|{\tilde{\text{y}}^{(i)}-\text{y}^{(i)}}\right|^2+\lambda\left|{\tilde{\text{p}}^{(i)}-\text{p}^{(i)}}\right|^2} } $$</p><h4 id="FSRGAN"><a href="#FSRGAN" class="headerlink" title="FSRGAN"></a>FSRGAN</h4><p>FSRGAN网络结构如下图：</p><blockquote><p><img src="http://sotavision.cn/showdoc/server/../Public/Uploads/2020-03-26/5e7c1ea4770b2.png" alt="FSRNet3" loading="lazy"></p></blockquote><p>GAN LOSS为：$$L_C(\text{F,C})=\mathbb{E}[\log\text{C}(\tilde{\text{y}},\text{x})]+\mathbb{E}[\log(1-\text{C}({\text{F(x)}},\text{x}))]$$<br>感知损失为：$$L_P=\left|{\phi(\text{y})-\phi(\tilde\text{y})}\right|^2$$</p><hr><h3 id="Experiment"><a href="#Experiment" class="headerlink" title="Experiment"></a>Experiment</h3><h4 id="Implementation-Details"><a href="#Implementation-Details" class="headerlink" title="Implementation Details"></a>Implementation Details</h4><p>实验详情可以参考代码：<a href="https://github.com/cydiachen/FSRNET_pytorch" target="_blank" rel="noopener">https://github.com/cydiachen/FSRNET_pytorch</a></p><h4 id="Prior-Knowledge-for-Face-Super-Resolution"><a href="#Prior-Knowledge-for-Face-Super-Resolution" class="headerlink" title="Prior Knowledge for Face Super-Resolution"></a>Prior Knowledge for Face Super-Resolution</h4><blockquote><p><img src="http://sotavision.cn/showdoc/server/../Public/Uploads/2020-03-26/5e7c20908f199.png" alt="FSRNet4" loading="lazy"><br><img src="http://sotavision.cn/showdoc/server/../Public/Uploads/2020-03-26/5e7c20a18f698.png" alt="FSRNet5" loading="lazy"></p></blockquote><p>把先验信息估计网络移除以后，构建了一个 Baseline 网络。基于 Baseline 网络，引入 ground truth 人脸先验信息（landmark heatmap 和解析图）到拼接层，得到一个新的网络。<br>结论：</p><ul><li>解析图比 landmark heatmap 含有更多人脸图像超分辨的信息，带来的提升更大；</li><li>全局的解析图比局部的解析图更有用；</li><li>landmark 数量增加所带来的提升很小</li></ul><h4 id="Network-analysis"><a href="#Network-analysis" class="headerlink" title="Network analysis"></a>Network analysis</h4><h5 id="Effects-of-Estimated-Priors"><a href="#Effects-of-Estimated-Priors" class="headerlink" title="Effects of Estimated Priors"></a>Effects of Estimated Priors</h5><blockquote><p><img src="http://sotavision.cn/showdoc/server/../Public/Uploads/2020-03-26/5e7c2210976f2.png" alt="FSRNet6" loading="lazy"></p></blockquote><p>Baseline_v1：完全不包含先验信息<br>Baseline_v2：包含先验信息，但不进行监督训练<br>结论：</p><ol><li>即使不进行监督训练，先验信息也能帮助到SR任务，可能是因为先验信息提供了更多的高频信息。</li><li>越多先验信息，越好。</li><li>最佳性能为25.85dB，但是使用ground truth信息时，能达到26.55dB。说明估计得到的先验信息并不完美，更好的先验信息估计网络可能会得到更好的结果。</li></ol><h5 id="Effects-of-Hourglass-Numbers"><a href="#Effects-of-Hourglass-Numbers" class="headerlink" title="Effects of Hourglass Numbers"></a>Effects of Hourglass Numbers</h5><p>强大的先验信息预测网络会得到更好的结果，所以探究Hourglass数量h对网络性能的影响。分别取1，2，4，结果为25.69，25.87，25.95。<br>不同的Hourglass数量对landmark估计的影响：</p><blockquote><p><img src="http://sotavision.cn/showdoc/server/../Public/Uploads/2020-03-26/5e7c2276370ed.png" alt="FSRNet7" loading="lazy"></p></blockquote><p>可以看到 h 数量增加时，先验信息估计网络结构越深，学习能力越强，性能越好。</p><h4 id="Comparisons-with-state-of-the-arts"><a href="#Comparisons-with-state-of-the-arts" class="headerlink" title="Comparisons with state-of-the-arts"></a>Comparisons with state-of-the-arts</h4><blockquote><p><img src="http://sotavision.cn/showdoc/server/../Public/Uploads/2020-03-26/5e7c22b5ad9fa.png" alt="FSRNet8" loading="lazy"></p></blockquote><p>放大8倍后的性能比较，虽然FSRGAN的两项指标（PSNR/SSIM）都不如FSRNet，但是从视觉效果上看更加真实。这也与目前的一个共识相对应：基于生成对抗网络的模型可以恢复视觉上合理的图像，但是在一些指标上（PSNR , SSIM）的值会低。而基于MSE的深度模型会生成平滑的图像，但是有高的PSNR/SSIIM。</p><hr><h3 id="Conclusions"><a href="#Conclusions" class="headerlink" title="Conclusions"></a>Conclusions</h3><ol><li>本文提出了深度端到端的可训练的人脸超分辨网络FSRNet</li><li>FSRNet的关键在于先验信息估计网络，这个网络不仅有助于改善PSNR/SSIM，还提供从非常低分辨率的图像精确估计几何先验信息（landmark heatmap和解析图）的解决方案。</li><li>实验结果表明FSRNet比当前的SOTA的方法要更好，即使在未对齐的人脸图像上。</li></ol><p>未来的工作可以有以下几个方面：</p><ol><li>设计一个更好的先验信息估计网络。</li><li>迭代地学习精细的SR网络。</li><li>调研其他有用的脸部先验信息。 </li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;https://cdn.jsdelivr.net/npm/aplayer@1.10/dist/APlayer.min.css&quot;&gt;&lt;script 
      
    
    </summary>
    
    
      <category term="论文笔记" scheme="http://alexzou14.github.io/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="深度学习" scheme="http://alexzou14.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="笔记" scheme="http://alexzou14.github.io/tags/%E7%AC%94%E8%AE%B0/"/>
    
      <category term="超分辨率" scheme="http://alexzou14.github.io/tags/%E8%B6%85%E5%88%86%E8%BE%A8%E7%8E%87/"/>
    
  </entry>
  
</feed>
