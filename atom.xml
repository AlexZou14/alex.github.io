<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>秩同道合的小站</title>
  
  <subtitle>寻找志趣相投的伙伴！</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://alexzou14.github.io/"/>
  <updated>2021-09-28T08:59:41.671Z</updated>
  <id>http://alexzou14.github.io/</id>
  
  <author>
    <name>秩同道合</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>mmediting 使用手册</title>
    <link href="http://alexzou14.github.io/2021/09/28/mmediting/"/>
    <id>http://alexzou14.github.io/2021/09/28/mmediting/</id>
    <published>2021-09-28T08:57:08.000Z</published>
    <updated>2021-09-28T08:59:41.671Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="https://cdn.jsdelivr.net/npm/aplayer@1.10/dist/APlayer.min.css"><script src="https://cdn.jsdelivr.net/npm/aplayer@1.10/dist/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="https://cdn.jsdelivr.net/npm/meting@1.2/dist/Meting.min.js"></script><h3 id="安装MMEditing"><a href="#安装MMEditing" class="headerlink" title="安装MMEditing"></a>安装MMEditing</h3><h4 id="需要安装的库"><a href="#需要安装的库" class="headerlink" title="需要安装的库"></a>需要安装的库</h4><ul><li>pytorch 1.3 或者更高</li><li>CUDA9.0 及以上</li><li>mmcv</li><li>GCC4.9及以上</li><li>NCCL2及以上</li></ul><h4 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h4><p>创建一个conda的虚拟环境</p><pre><code>conda create -n open-mmlab python=3.7 -yconda activate open-mmlab</code></pre><p>进入open-mmlab后安装对应的pytorch库</p><pre><code>对应不同的CUDA版本安装对应的pytorch，例如：conda install pytorch cudatoolkit=10.1 torchvision -c pytorchconda install pytorch=1.3.1 cudatoolkit=9.2 torchvision=0.4.2 -c pytorch</code></pre><p>将mmediting仓库下载到本地</p><pre><code>git clone http://github.com/open-mmlab/mmediting.git</code></pre><p>安装mmediting中需要的库文件</p><pre><code>cd mmeditingpip install -r requirements.txt</code></pre><p>其中这里安装mmcv-full可能会安装失败，所以我们可以通过以下方式将mmcv-full安装上(参考自<a href="https://github.com/open-mmlab/mmcv/blob/master/README.md)：" target="_blank" rel="noopener">https://github.com/open-mmlab/mmcv/blob/master/README.md)：</a></p><table><thead><tr><th>CUDA</th><th>torch1.7</th><th>torch1.6</th><th>torch1.5</th><th>torch1.4</th><th>torch1.3</th></tr></thead><tbody><tr><td>11.0</td><td><details><pre><code>pip install mmcv-full=={mmcv_version} -f <a href="https://download.openmmlab.com/mmcv/dist/cu110/torch1.7.0/index.html" target="_blank" rel="noopener">https://download.openmmlab.com/mmcv/dist/cu110/torch1.7.0/index.html</a></code></pre></details></td><td></td><td></td><td></td><td></td></tr><tr><td>10.2</td><td><details><pre><code>pip install mmcv-full=={mmcv_version} -f <a href="https://download.openmmlab.com/mmcv/dist/cu102/torch1.7.0/index.html" target="_blank" rel="noopener">https://download.openmmlab.com/mmcv/dist/cu102/torch1.7.0/index.html</a></code></pre></details></td><td><details><pre><code>pip install mmcv-full=={mmcv_version} -f <a href="https://download.openmmlab.com/mmcv/dist/cu102/torch1.6.0/index.html" target="_blank" rel="noopener">https://download.openmmlab.com/mmcv/dist/cu102/torch1.6.0/index.html</a></code></pre></details></td><td><details><pre><code>pip install mmcv-full=={mmcv_version} -f <a href="https://download.openmmlab.com/mmcv/dist/cu102/torch1.5.0/index.html" target="_blank" rel="noopener">https://download.openmmlab.com/mmcv/dist/cu102/torch1.5.0/index.html</a></code></pre></details></td><td></td><td></td></tr><tr><td>10.1</td><td><details><pre><code>pip install mmcv-full=={mmcv_version} -f <a href="https://download.openmmlab.com/mmcv/dist/cu101/torch1.7.0/index.html" target="_blank" rel="noopener">https://download.openmmlab.com/mmcv/dist/cu101/torch1.7.0/index.html</a></code></pre></details></td><td><details><pre><code>pip install mmcv-full=={mmcv_version} -f <a href="https://download.openmmlab.com/mmcv/dist/cu101/torch1.6.0/index.html" target="_blank" rel="noopener">https://download.openmmlab.com/mmcv/dist/cu101/torch1.6.0/index.html</a></code></pre></details></td><td><details><pre><code>pip install mmcv-full=={mmcv_version} -f <a href="https://download.openmmlab.com/mmcv/dist/cu101/torch1.5.0/index.html" target="_blank" rel="noopener">https://download.openmmlab.com/mmcv/dist/cu101/torch1.5.0/index.html</a></code></pre></details></td><td><details><pre><code>pip install mmcv-full=={mmcv_version} -f <a href="https://download.openmmlab.com/mmcv/dist/cu101/torch1.4.0/index.html" target="_blank" rel="noopener">https://download.openmmlab.com/mmcv/dist/cu101/torch1.4.0/index.html</a></code></pre></details></td><td><details><pre><code>pip install mmcv-full=={mmcv_version} -f <a href="https://download.openmmlab.com/mmcv/dist/cu101/torch1.3.0/index.html" target="_blank" rel="noopener">https://download.openmmlab.com/mmcv/dist/cu101/torch1.3.0/index.html</a></code></pre></details></td></tr><tr><td>9.2</td><td><details><pre><code>pip install mmcv-full=={mmcv_version} -f <a href="https://download.openmmlab.com/mmcv/dist/cu92/torch1.7.0/index.html" target="_blank" rel="noopener">https://download.openmmlab.com/mmcv/dist/cu92/torch1.7.0/index.html</a></code></pre></details></td><td><details><pre><code>pip install mmcv-full=={mmcv_version} -f <a href="https://download.openmmlab.com/mmcv/dist/cu92/torch1.6.0/index.html" target="_blank" rel="noopener">https://download.openmmlab.com/mmcv/dist/cu92/torch1.6.0/index.html</a></code></pre></details></td><td><details><pre><code>pip install mmcv-full=={mmcv_version} -f <a href="https://download.openmmlab.com/mmcv/dist/cu92/torch1.5.0/index.html" target="_blank" rel="noopener">https://download.openmmlab.com/mmcv/dist/cu92/torch1.5.0/index.html</a></code></pre></details></td><td><details><pre><code>pip install mmcv-full=={mmcv_version} -f <a href="https://download.openmmlab.com/mmcv/dist/cu92/torch1.4.0/index.html" target="_blank" rel="noopener">https://download.openmmlab.com/mmcv/dist/cu92/torch1.4.0/index.html</a></code></pre></details></td><td><details><pre><code>pip install mmcv-full=={mmcv_version} -f <a href="https://download.openmmlab.com/mmcv/dist/cu92/torch1.3.0/index.html" target="_blank" rel="noopener">https://download.openmmlab.com/mmcv/dist/cu92/torch1.3.0/index.html</a></code></pre></details></td></tr><tr><td>cpu</td><td><details><pre><code>pip install mmcv-full=={mmcv_version} -f <a href="https://download.openmmlab.com/mmcv/dist/cpu/torch1.7.0/index.html" target="_blank" rel="noopener">https://download.openmmlab.com/mmcv/dist/cpu/torch1.7.0/index.html</a></code></pre></details></td><td><details><pre><code>pip install mmcv-full=={mmcv_version} -f <a href="https://download.openmmlab.com/mmcv/dist/cpu/torch1.6.0/index.html" target="_blank" rel="noopener">https://download.openmmlab.com/mmcv/dist/cpu/torch1.6.0/index.html</a></code></pre></details></td><td><details><pre><code>pip install mmcv-full=={mmcv_version} -f <a href="https://download.openmmlab.com/mmcv/dist/cpu/torch1.5.0/index.html" target="_blank" rel="noopener">https://download.openmmlab.com/mmcv/dist/cpu/torch1.5.0/index.html</a></code></pre></details></td><td><details><pre><code>pip install mmcv-full=={mmcv_version} -f <a href="https://download.openmmlab.com/mmcv/dist/cpu/torch1.4.0/index.html" target="_blank" rel="noopener">https://download.openmmlab.com/mmcv/dist/cpu/torch1.4.0/index.html</a></code></pre></details></td><td><details><pre><code>pip install mmcv-full=={mmcv_version} -f <a href="https://download.openmmlab.com/mmcv/dist/cpu/torch1.3.0/index.htmll" target="_blank" rel="noopener">https://download.openmmlab.com/mmcv/dist/cpu/torch1.3.0/index.htmll</a></code></pre></details></td></tr></tbody></table><p>其中对应的{mmcv_version}我们可以替换成我们需要安装mmcv的版本，例如1.2.2版本，CUDA 11, pytorch 1.7.0，安装命令如下：</p><pre><code>pip install mmcv-full==1.2.2 -f https://download.openmmlab.com/mmcv/dist/cu110/torch1.7.0/index.html</code></pre><p>安装好mmcv-full后再进行后面的一条命令：</p><pre><code>pip install -v -e .</code></pre><hr><h3 id="使用MMEditing"><a href="#使用MMEditing" class="headerlink" title="使用MMEditing"></a>使用MMEditing</h3><h4 id="图像超分辨率数据集："><a href="#图像超分辨率数据集：" class="headerlink" title="图像超分辨率数据集："></a>图像超分辨率数据集：</h4><ul><li>Training dataset: <a href="https://data.vision.ee.ethz.ch/cvl/DIV2K/" target="_blank" rel="noopener" title="DIV2K dataset">DIV2K dataset</a></li><li>Validation dataset: Set5 and Set14.</li></ul><p>这里为了方便起见，我们按照以下文档给出的文件目录把数据集放入mmediting文件夹中的data文件夹中</p><pre><code>mmediting├── mmedit├── tools├── configs├── data│   ├── DIV2K│   │   ├── DIV2K_train_HR│   │   ├── DIV2K_train_LR_bicubic│   │   │   ├── X2│   │   │   ├── X3│   │   │   ├── X4│   ├── val_set5│   │   ├── Set5_bicLRx2│   │   ├── Set5_bicLRx3│   │   ├── Set5_bicLRx4│   │   ├── Set5_mod12│   ├── val_set14│   │   ├── Set14_bicLRx2│   │   ├── Set14_bicLRx3│   │   ├── Set14_bicLRx4│   │   ├── Set14_mod12│   ├── ...  ————&gt;这里可以加入需要的数据集，格式如上</code></pre><p>注意这里每一个数据集中都应该包含2，3和4倍下采样的图片，不然会报错！！！要获得每个数据集中的不同倍数的图像，我们可以利用这个脚本来对HR图像进行处理得到：<br>脚本链接（参考mmsr得到）：<a href="http://sotavision.cn:8000/f/c34315c21f/?raw=1" target="_blank" rel="noopener" title="generate_mod_LR_bic.py">generate_mod_LR_bic.py</a>，通过修改里面对应的输入文件夹和输出文件夹就行:</p><pre><code>通过修改对应的dataset_path再运行就可以获取2，3，4倍图像for i in [2,3,4]:    generate_mod_LR_bic(up_scale=i, mod_scale=12, sourcedir={datasets_path})</code></pre><p>然后我们直接通过命令对训练集直接获取LMDB文件：</p><pre><code>python tools/preprocess_div2k_dataset.py --data-root ./data/DIV2K --make-lmdb</code></pre><p>这里会先对图像进行裁剪得到_sub文件夹然后再进行转换成lmdb文件，所以我们可以得到如下的文件夹：</p><pre><code>mmediting├── mmedit├── tools├── configs├── data│   ├── DIV2K│   │   ├── DIV2K_train_HR│   │   ├── DIV2K_train_HR_sub│   │   ├── DIV2K_train_HR_sub.lmdb│   │   │   ├── data.mdb│   │   │   ├── lock.mdb│   │   │   ├── meta_info.txt│   │   ├── DIV2K_train_LR_bicubic│   │   │   ├── X2│   │   │   ├── X3│   │   │   ├── X4│   │   │   ├── X2_sub│   │   │   ├── X3_sub│   │   │   ├── X4_sub│   │   ├── DIV2K_train_LR_bicubic_X2_sub.lmdb│   │   ├── DIV2K_train_LR_bicubic_X3_sub.lmdb│   │   ├── DIV2K_train_LR_bicubic_X4_sub.lmdb│   │   ├── ...</code></pre><p>这样就数据准备完毕了！！！</p><h4 id="训练模型"><a href="#训练模型" class="headerlink" title="训练模型"></a>训练模型</h4><h5 id="直接使用SRAnnotationDataset训练"><a href="#直接使用SRAnnotationDataset训练" class="headerlink" title="直接使用SRAnnotationDataset训练"></a>直接使用SRAnnotationDataset训练</h5><p>我们这里先用EDSRx2为例，我们先打开<code>config/resotrers/edsr/</code>文件夹中的<code>edsr_x2c64b16_g1_300k_div2k.py</code>，修改文件中的下面信息再保存：</p><pre><code>训练集数据位置保证是正确的#trainlq_folder=&apos;data/DIV2K/DIV2K_train_LR_bicubic/X2_sub&apos;,gt_folder=&apos;data/DIV2K/DIV2K_train_HR_sub&apos;,ann_file=&apos;data/DIV2K/DIV2K_train_HR_sub.lmdb/meta_info.txt&apos; #这个文件就是上面处理为lmdb文件后得到的.txt文件#vallq_folder=&apos;./data/val_set5/Set5_bicLRx2&apos;,gt_folder=&apos;./data/val_set5/Set5_mod12&apos;, #--&gt;这里可以替换成你要测试的数据集的文件夹#testlq_folder=&apos;./data/val_set5/Set5_bicLRx2&apos;,gt_folder=&apos;./data/val_set5/Set5_mod12&apos;, #--&gt;这个数据集是最后训练结束测试的数据集# learning policytotal_iters = 300000 #--&gt;这里调节训练的轮数lr_config = dict(policy=&apos;Step&apos;, by_epoch=False, step=[200000], gamma=0.5) #--&gt;这里调节训练策略</code></pre><p>修改完成后就可以在open-mmlab环境下通过下面的命令行训练：</p><pre><code>cd mmediting./tools/dist_train.sh {CONFIG_FILE} {GPU_NUM} [optional arguments] #——&gt;其中CONFIG_FILE是表示我们需要训练的配置文件，GPU_NUM是代表我们使用GPU数量</code></pre><p>后面的<code>[optional arguments]</code>是表示可以输入的参数，其中参数列表如下：</p><ul><li><code>--no-validate</code>: 添加这个参数，就是使得训练过程中没有valuation</li><li><code>--work-dir {WORK_DIR}</code>: 这里就可以指定训练出来的结果保存在哪个文件夹</li><li><code>--resume-from {CHECKPOINT_FILE}</code>: 这里是前面的训练的权重和优化器的状态都加到模型中</li></ul><p>然后，我们以EDSR为例，可以写为：</p><pre><code>./tools/dist_train.sh config/resotrers/edsr/edsr_x2c64b16_g1_300k_div2k.py 1 --work-dir EDSRx2 --resume-from xxxxxxxxx.pt</code></pre><h5 id="使用LMDB环境进行训练"><a href="#使用LMDB环境进行训练" class="headerlink" title="使用LMDB环境进行训练"></a>使用LMDB环境进行训练</h5><p>相应的打开<code>config/resotrers/edsr/</code>文件夹中<code>edsr_x2c64b16_g1_300k_div2k.py</code>，需要修改以下参数：</p><pre><code>1.找到train_dataset_type这个参数，我们修改为如下：train_dataset_type = &apos;SRLmdbDataset&apos;2.找到data对应的字典，修改train就行，如下： # trainsamples_per_gpu=16,workers_per_gpu=6,drop_last=True,train=dict(    type=&apos;RepeatDataset&apos;,    times=1000,    dataset=dict(        type=train_dataset_type,        lq_folder=&apos;data/DIV2K/DIV2K_train_LR_bicubic_X2_sub.lmdb&apos;,--------&gt;这里修改成你得到.lmdb格式lq文件夹        gt_folder=&apos;data/DIV2K/DIV2K_train_HR_sub.lmdb&apos;,--------&gt;这里修改成你得到.lmdb格式gt文件夹        # ann_file=&apos;data/DIV2K/meta_info_DIV2K800sub_GT.txt&apos;,--------&gt;这里注释掉ann_file，因为SRLmdbDataset没有这个参数！        pipeline=train_pipeline,        scale=scale)),3.找到train_pipeline，修改这个参数下面第一个字典和第二个字典参数，如下：dict(    type=&apos;LoadImageFromFile&apos;,    io_backend=&apos;lmdb&apos;,    key=&apos;lq&apos;,    db_path=&apos;data/DIV2K/DIV2K_train_LR_bicubic_X2_sub.lmdb&apos;,    flag=&apos;unchanged&apos;),dict(    type=&apos;LoadImageFromFile&apos;,    io_backend=&apos;lmdb&apos;,    key=&apos;gt&apos;,    db_path=&apos;data/DIV2K/DIV2K_train_HR_sub.lmdb&apos;,    flag=&apos;unchanged&apos;),</code></pre><h4 id="模型参数计算"><a href="#模型参数计算" class="headerlink" title="模型参数计算"></a>模型参数计算</h4><p>mmediting计算模型的参数和FLOPS用如下命令:</p><pre><code>python tools/get_flops.py {CONFIG_FILE} [--shape {INPUT_SHAPE}]</code></pre><p>例如：</p><pre><code>python tools/get_flops.py configs/resotorer/xxxxxxxxxxx.py --shape 40 40</code></pre><p>得到结果如下所示：</p><pre><code>==============================Input shape: (3, 40, 40)Flops: 4.07 GMacParams: 1.52 M==============================</code></pre><h3 id="模型构建"><a href="#模型构建" class="headerlink" title="模型构建"></a>模型构建</h3><p>我们将我们自己写好的模型加入进<code>mmediting/mmedit/models/backbones/sr_backbones</code>文件夹中, 对照EDSR中的格式改成相应<code>.py</code>文件，再在<code>__init__</code>文件中添加对应的模型。<br>如果要设计对应的loss函数，可以将写好的Loss文件加入<code>mmediting/mmedit/models/losses/</code>文件加中，对照其他文件改成相应的格式，再在<code>__init__</code>文件中添加对应的loss名称。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;https://cdn.jsdelivr.net/npm/aplayer@1.10/dist/APlayer.min.css&quot;&gt;&lt;script 
      
    
    </summary>
    
    
      <category term="编程工具" scheme="http://alexzou14.github.io/categories/%E7%BC%96%E7%A8%8B%E5%B7%A5%E5%85%B7/"/>
    
    
      <category term="pytorch" scheme="http://alexzou14.github.io/tags/pytorch/"/>
    
      <category term="代码框架" scheme="http://alexzou14.github.io/tags/%E4%BB%A3%E7%A0%81%E6%A1%86%E6%9E%B6/"/>
    
  </entry>
  
  <entry>
    <title>ICCV2021 盲图像超分MANet:空间可变模糊核估计的互仿射网络</title>
    <link href="http://alexzou14.github.io/2021/09/28/MANet/"/>
    <id>http://alexzou14.github.io/2021/09/28/MANet/</id>
    <published>2021-09-28T07:29:33.000Z</published>
    <updated>2021-09-28T08:50:44.874Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="https://cdn.jsdelivr.net/npm/aplayer@1.10/dist/APlayer.min.css"><script src="https://cdn.jsdelivr.net/npm/aplayer@1.10/dist/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="https://cdn.jsdelivr.net/npm/meting@1.2/dist/Meting.min.js"></script><h3 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h3><p>目前大多数盲图像超分方法都假设模糊核在整个图像中都是一样的，然而真实场景图像中的模糊核是复杂多变的，这导致其方法很少适用于真实图像，从而导致这些方法在实际应用过程中性能下降。为了解决这些问题，作者提出了一个MANet(Mutual Affine Network)用于估计空间可变的模糊核。具体来说，MANet有两个显著的特点：</p><ul><li>它具有一个合适的感受野来保证退化的局部性。</li><li>它包含一个新的MAConv (Mutual Affine Conv)层，该层在不增加感受野、模型大小和计算负担的情况下增强了特征表达能力。</li></ul><p>在合成图像和真实图像上的大量实验表明，所提出的MANet不仅具有良好的空间变异和不变核估计性能，而且在与非盲SR方法相结合时还具有最先进的盲SR性能。</p><h3 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h3><img src="https://cdn.nlark.com/yuque/0/2021/png/22543740/1632795728697-1bf31255-c9e5-4c5a-8f86-9429124121ec.png" referrerpolicy="no-referrer" loading="lazy"><p>目前盲图像超分方法假设模糊核在空间上是不变的，并且只估计整个图像的单个核，这导致了以下两个问题：</p><ul><li>现实世界中的模糊核通常在空间上是不同的，因此图像不同位置的模糊核应该是不同的。</li><li>即使在空间不变的假设下，估计整个图像的单个核也容易受到图像平坦区域的不利影响。因为在平坦区域估计出来的模糊核和边角区域的模糊核往往不尽相同，如图所示，因此估计单一模糊核是比较片面的。</li></ul><h3 id="Contributions"><a href="#Contributions" class="headerlink" title="Contributions"></a>Contributions</h3><p>本文主要贡献如下：</p><ul><li>作者提出了一个MANet的空间变换模糊核的估计框架。通过适当的感受野，他可以从较小的图块上估计模糊核。</li><li>作者提出了MAConv层，在不增加网络感受野的情况下，利用信道相互依赖性来增强特征表达能力，使其适合于模糊核的特征提取。与普通卷积层相比，模型参数和计算量降低了约30%。</li><li>与现有方法相比，MANet在空间变异和不变核估计方面都表现出良好的性能，与非盲SR模型相结合时可以获得最先进的盲SR性能。</li></ul><h3 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h3><h4 id="Problem-Formulation"><a href="#Problem-Formulation" class="headerlink" title="Problem Formulation"></a>Problem Formulation</h4><p>LR图像\( I^{LR} \)可以通过HR图像\( I^{HR} \)根据以下的图像降质量模型得到：</p><div>    \[ I^{LR} = (k \otimes I^{HR})\downarrow_s + n \]</div><p>由于模糊核在空间上是变化的，在这种情况下，图像和模糊核可以以向量形式写入。所以降质过程建模为：</p><div>    \[ I^{LR} = (K \otimes I^{HR})\downarrow_s + n \]</div><p>其中K表示与卷积矩阵类似的模糊矩阵。</p><h4 id="Proposed-Method"><a href="#Proposed-Method" class="headerlink" title="Proposed Method"></a>Proposed Method</h4><p>由不同核模糊的图像块具有不同的块分布。KernelGAN利用内部GAN的这一特性，并使用鉴别器将图像块区分为真假。然而，它只适用于空间不变的核估计，不能估计较小图像块的核。为了更进一步，作者提出了直接从图像块中估计核。</p><h5 id="MANet"><a href="#MANet" class="headerlink" title="MANet"></a>MANet</h5><p>目前神经网络通常堆叠多层以建立具有大感受野的深层模型。然而，对于空间可变核估计任务，我们需要保持退化的局部性。因此，我们提出了一种具有合适感受野的互仿射网络（MANet）。如下图所示：</p><img src="https://cdn.nlark.com/yuque/0/2021/png/22543740/1632797273533-4662f65f-9373-45e6-97e3-7ef820e79a81.png" referrerpolicy="no-referrer" loading="lazy"><p>上图给出了MANet架构示意图，它包含特征提取与核重建两个模块。特征提取模块是一种类似UNet架构，由卷积、残差模块以及上/下采样构成；核重建模块由卷积、Softmax以及最近邻插值构成。预测得到的模糊核表示为\( K \in \mathcal{R}^{hw\times H \times W} \)。基于上述架构设计，MANet的感受野为\( 22\times 22 \)。</p><h5 id="Mutual-affine-convolution"><a href="#Mutual-affine-convolution" class="headerlink" title="Mutual affine convolution"></a>Mutual affine convolution</h5><p>一般来讲，小感受野意味着小网络、弱表达能力。一种可能的方案是提升通道数量，但这会带来指数级的参数量与计算量提升。为解决该问题，我们提出了MAConv，见下图。<br><img src="https://cdn.nlark.com/yuque/0/2021/png/22543740/1632799670040-dc1a7106-b0fb-4d63-9d44-f37214e741cf.png" referrerpolicy="no-referrer" loading="lazy"></p><p>作者首先将输入的\( x \in \mathcal{R}^{C_{in}\times H_f \times W_f} \)通道分离成S份，如下表达式所示：</p><div>    \[ x_1, x_2, \cdots,x_S = \text{split}(x) \]</div><p>对于\( x_i \in \mathcal{R}^{\frac{C_{in}}{S}\times H_f \times W_f} \), 我们采用\( x \in \mathcal{R}^{\frac{C_{in}(S-1)}{S}\times H_f \times W_f} \)为其互补信息。我们将上述两者送入到MAConv层进行处理：</p><div>    \[ \beta_i, \gamma_i = \text{split}(\mathcal{F}(\bar{x}_i)) \]</div><div>    \[ y_i=\beta_i \odot x_i + \gamma_i \]</div><p>在完成上述变换后，作者采用\( 3\times 3 \)卷积生成特征\( z_i = conv_i(y_i) \)。最后将所得特征进行拼接生成MAConv的输出：</p><div>    \[ z = concat(z_1, z_2, \cdots, z_S) \]</div>MAConv通过互仿射变换探索了不同通道之间的相互关系，这种设计可以有提升特征表达能力，同时极大降低模型大小与计算复杂度。<h3 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h3><p>下表对比了卷积、组卷积以及MAConv在参数量、内存占用、FLOPs以及推理耗时方面的对比。注：由于仿射变换不会提升感受野，MAConv的感受野仍为\( 3\times 3 \)；而稠密与SE模块会导致感受野极大提升而不适合于核估计。</p><img src="https://cdn.nlark.com/yuque/0/2021/png/22543740/1632813293355-cc809ab5-da2c-45ac-b9b5-3eee1659f264.png" referrerpolicy="no-referrer" loading="lazy"><p>此外，从上表还可以看到：</p><ul><li>MAConv在LR图像上取得了最佳PSNR/SSIM指标，这说明所生成的模糊核可以更好的保持数据一致性；</li><li>提升通道数，MAConv的性能可以进一步提升，但同时也带来了参数量与FLOPs提升；</li><li>提升MAConv的split数同样会带来性能提升，这说明更多的split可以更好的探索通道相关性、提升特征表达能力。为平衡精度与推理耗时，我们将通道数与split数分别设置为\( [128,256,128],2 \)。</li></ul><img src="https://cdn.nlark.com/yuque/0/2021/png/22543740/1632813441744-1424df8b-67b5-449b-8324-eccc46bff3e4.png" referrerpolicy="no-referrer" loading="lazy"><p>在真实应用场景，图像还可能存在噪声与压缩伪影。为测试在更复杂场景下的核估计性能，我们在训练过程中添加高斯与JPEG压缩噪声并在不同噪声水平下进行测试，参见上表。从表中可以看到：相比无噪情况，尽管出现了性能下降，但LR图像的PSNR范围仍为40.59-45.45dB，这无疑说明了所提方案在重度噪声干扰下的核估计性能。<br><img src="https://cdn.nlark.com/yuque/0/2021/png/22543740/1632813530485-7919cf59-8d4d-455e-b420-10184606f662.png" referrerpolicy="no-referrer" loading="lazy"></p><p>上表对比了不同盲超分方案的性能，从中可以看到：</p><ul><li>对比不同类型的空间可变核，所提MANet均取得了最佳性能；</li><li>当模糊核出现差异后，极具代表的BicubicSR模型RCAN与HAN出现了严重性能下降；</li><li>类似地，DIP也难以生成令人满意的记过，因其模糊核是固定的；</li><li>通过逐块优化核，SRSVD可以处理空间可变SR问题，但无疑会极大提升运行耗时；</li><li>IKC能够生成比其他方案更好的结果，但它对每个图像仅估计一个核，这无疑限制了其性能；</li><li>MANet在每个位置预测一个核，因此它可以处理空间可变退化问题，取得了大幅优于IKC的性能</li></ul><img src="https://cdn.nlark.com/yuque/0/2021/png/22543740/1632813589380-128f875a-747e-4849-a6b5-d6cbd9a40f83.png" referrerpolicy="no-referrer" loading="lazy"><p>上图给出了几种方案在空间可变核与真实场景数据上的视觉效果对比，从中可以看到：MANet可以生成具有最佳视觉效果的结果 ，而其他方案要么存在过度模糊，要么存在过度锐化问题。<br><img src="https://cdn.nlark.com/yuque/0/2021/png/22543740/1632813708731-47ee3781-0817-4e53-83eb-3eedcb99ac57.png" referrerpolicy="no-referrer" loading="lazy"></p><p>上表对比了空间不变超分方案的性能，从中可以看到：</p><ul><li>在不同数据集、不同超分倍率下，所提MANet均取得了最佳性能 。</li><li>尽管KernelGAN可以从LR图像估计模糊核，但其性能与HAN、DIP相近；</li><li>IKC具有比其他方案更优的性能，但仍弱于所提MANet。</li></ul><h3 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h3><ul><li>作者提出了一种互仿射网络（MANet）用于空间变异的盲SR核估计。</li><li>MANet由特征提取和核重构模块组成，具有适度的感受野，以保持退化的局部性。</li><li>提出的互仿射卷积（MAConv）层，通过学习不同信道分裂之间的仿射变换来利用信道的相互依赖性，这可以在不增加模型感受野、模型大小和计算负担的情况下增强模型的表达能力。</li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;https://cdn.jsdelivr.net/npm/aplayer@1.10/dist/APlayer.min.css&quot;&gt;&lt;script 
      
    
    </summary>
    
    
      <category term="论文笔记" scheme="http://alexzou14.github.io/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="深度学习" scheme="http://alexzou14.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="笔记" scheme="http://alexzou14.github.io/tags/%E7%AC%94%E8%AE%B0/"/>
    
      <category term="超分辨率" scheme="http://alexzou14.github.io/tags/%E8%B6%85%E5%88%86%E8%BE%A8%E7%8E%87/"/>
    
  </entry>
  
  <entry>
    <title>TIP2021 AMPRN:图像轻量超分辨率对抗网络</title>
    <link href="http://alexzou14.github.io/2021/09/25/AMPRN/"/>
    <id>http://alexzou14.github.io/2021/09/25/AMPRN/</id>
    <published>2021-09-25T07:04:07.000Z</published>
    <updated>2021-09-25T14:39:57.208Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="https://cdn.jsdelivr.net/npm/aplayer@1.10/dist/APlayer.min.css"><script src="https://cdn.jsdelivr.net/npm/aplayer@1.10/dist/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="https://cdn.jsdelivr.net/npm/meting@1.2/dist/Meting.min.js"></script><p>论文地址：<a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=9490520" target="_blank" rel="noopener">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=9490520</a></p><h3 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h3><p>目前大多数超分辨率方法使用更深更广的网络来提高SR性能，因为复杂度高，计算成本高，效率低，这很难应用在现实场景里。此外，它们不能同时提供高感知质量和保证客观质量。为了解决这些局限性，本文提出了一种用于图像超分辨率的多路径残差对抗网络，这个方法有效的降低了网络参数并且获得更高的SR性能。具体来说，作者提出了一种多路径残差块（MPRB），该块具有较少的网络参数，可以充分利用通道切片生成的不同路径的特征来提取丰富的局部特征。然后，通过全局渐进特征融合，将来自所有MPRB的这些分层特征联合聚合。作者还提出了一个梯度对抗网络和梯度损失，使得生成SR图像和真实图像的梯度分布更接近。</p><hr><h3 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h3><ul><li>尽管大多数基于CNN的方法中的一些低频信息可以直接用于重建SR图像，但这些方法处理通道特征时没有区别。导致这些轻量级SISR方法丢失一些有用的低频信息并存在一些冗余计算。</li><li>所有轻量级方法在原始图像上应用像素级损失，像素级损失很难处理恢复丢失的高频细节时的固有不确定性，因为它认为每个像素之间的误差是独立的。</li><li>感知损失和基于GAN的模型往往会增加像素值的误差，偏离最小均方误差损失的最优解，从而降低生成图像的客观质量。</li></ul><hr><h3 id="Contributions"><a href="#Contributions" class="headerlink" title="Contributions"></a>Contributions</h3><ul><li>提出了一种具有对抗性梯度网络的轻量级多径残差网络（MPRN）用于图像超分辨率，该网络可以在充分利用原始LR图像特征的同时，大大抑制网络参数的数量。</li><li>MPRN模型采用了一种新的多径残差块（MPRB）结构，该结构可以从级联卷积层中提取多径特征，并通过局部特征融合充分利用这些特征，提高表达能力。同时，采用全局渐进的特征融合策略，充分利用各MPRB的层次特征。</li><li>提出了一种新的具有梯度损失的对抗性梯度网络，使生成的SR图像具有丰富的纹理细节和较小的像素值误差，同时具有较高的感知质量和客观质量。</li></ul><hr><h3 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h3><p><img src="https://cdn.jsdelivr.net/gh/AlexZou14/CDN/AMPRN/stucture.png" alt="structure" loading="lazy"></p><p>上图给出了所提方案整体架构示意图。整个网络可以分为两个部分: </p><ul><li>多路径残差网络 (MPRN): 多路径残差网络旨在生成具有丰富纹理细节的SR图像，这些纹理细节在梯度上尽可能接近真实HR图像，因此使得鉴别器无法区分SR图像和真实HR图像的梯度。</li><li>梯度对抗网络 (Adversarial Gradient Network): 我们构造了一个包含两个梯度层和一个鉴别器的对抗梯度网络。鉴别器由8个卷积层组成，卷积核的大小从64增加到512，每次增加一倍，类似于VGG网络。</li></ul><h4 id="Multi-Path-Residual-Network"><a href="#Multi-Path-Residual-Network" class="headerlink" title="Multi-Path Residual Network"></a>Multi-Path Residual Network</h4><p><img src="https://cdn.jsdelivr.net/gh/AlexZou14/CDN/AMPRN/MPRN.png" alt="MPRN" loading="lazy"></p><p>与目前大多数神经网络超分辨率方法都采用直接堆叠多个层来获得更强的表达能力不同，作者提出的一个多路径的残差网络结构，这个网络将每一层的MPRB的特征都保留下来，利用全局渐进特征融合的方式将所有特征融合从而可以保留对应的低频细节。对应各个网络可以表达如下：</p><ul><li><p>网络中浅层特征提取模块：<br>\( M_0 = F_{SFEM2}(M_{-1})=F_{SFEM2}(F_{SFEM1}(x)) \)</p></li><li><p>多路径残差学习：<br>\( M_n = F_{MPRB,n}(F_{MPRB, n-1}(\cdots (F_{MPRB, 1}(M_0))\cdots )) \)<br>\( M_{GF} = F_{GFC}([M_{1}, M_2, \cdots, M_N, M_{1}’, M_2’, \cdots, M_N’)]) + M_{-1} \) </p></li><li><p>重建模块：<br>\( \hat{h_{y}} = F_{RE}(M_{GF}) \)</p></li></ul><h4 id="Multi-Path-Residual-Block-MPRB"><a href="#Multi-Path-Residual-Block-MPRB" class="headerlink" title="Multi-Path Residual Block (MPRB)"></a>Multi-Path Residual Block (MPRB)</h4><p><img src="https://cdn.jsdelivr.net/gh/AlexZou14/CDN/AMPRN/MPRB.png" alt="MPRB" loading="lazy"></p><p>在此，我们给出了MPRB的详细设计，如图所示。卷积过程可以表示为：<br>\( M_{n, d} = \sigma(W_{n, d}(M_{n, d-1})+b_{n, d}) \)<br>\( M_{n, d}^{branch} = F_s(M_{n, d}, s) \)<br>\( M_{n, d}^{trunk} = F_s(M_{n, d}, s\times (D-d)) \)</p><p>紧接着利用一个\(1\times 1\)的卷积，将所有的特征融合，如下所示：<br>\( M_{n, LFC} = F_{LFC, n}([M_{n, 1}^{trunk}, M_{n, 2}^{trunk}, \cdots, M_{n, D}^{trunk}]) \)<br>最终输出可以表示为：\( M_n = M_{n-1} + M_{n,LFC} \)</p><hr><h3 id="Loss-Function"><a href="#Loss-Function" class="headerlink" title="Loss Function"></a>Loss Function</h3><ul><li><p>Content Loss on Original Image：</p><div>  \[ L_{co} = \frac{1}{\bar{N}} \displaystyle \sum_{i=1}^{\bar{N}} || I_i - \hat{I_i} ||_1 \]</div></li><li><p>Texture Gradient Loss：</p><div>  \[ L_{teg} = \frac{1}{\bar{N}} \displaystyle \sum_{i=1}^{\bar{N}} || Sobel(I_i) - Sobel(\hat{I_i}) ||_1 \]</div></li><li><p>Adversarial gradient loss:</p><div>  \[ L_{adg} = \displaystyle \min_{G} \max_{D} (L_G^{Ra}, L_{D}^{Ra}), \]</div></li></ul><p>其中\( L_G^{Ra} \)和\( L_D^{Ra} \)表示为：</p><div>    \[ L_D^{Ra} = -\mathbb{E}_I [ \log(D^{Ra}(I, G(X))) ] - \mathbb{E}_{G(X)}[\log(1-D^{Ra}(G(X), I))], \]</div><div>    \[ L_G^{Ra} = -\mathbb{E}_I [ \log(1-D^{Ra}(I, G(X))) ] - \mathbb{E}_{G(X)}[\log(D^{Ra}(G(X), I))], \]</div><p>最终的loss function为：\( L_{all} = L_{co} + \alpha L_{teg} + \beta L_{adg} \)其中\( \alpha \)和\( \beta \)为可以调参数。</p><hr><h3 id="Experiment"><a href="#Experiment" class="headerlink" title="Experiment"></a>Experiment</h3><p><img src="https://cdn.jsdelivr.net/gh/AlexZou14/CDN/AMPRN/experiments1.png" alt="table" loading="lazy"></p><p>上表给出了所提方案与其他轻量型方案的性能对比，从中可以看出：在轻量网络中AMPRN取得了最佳性能。</p><p><img src="https://cdn.jsdelivr.net/gh/AlexZou14/CDN/AMPRN/experiments2.png" alt="cpblocks" loading="lazy"></p><p><img src="https://cdn.jsdelivr.net/gh/AlexZou14/CDN/AMPRN/blockcp.png" alt="cpblocks" loading="lazy"></p><p><img src="https://cdn.jsdelivr.net/gh/AlexZou14/CDN/AMPRN/parablockcp.png" alt="cpblocksp" loading="lazy"></p><p>从上图可以看出作者提出的MPRB可以获得RDB的性能，并且参数量要小于RDB。</p><p><img src="https://cdn.jsdelivr.net/gh/AlexZou14/CDN/AMPRN/abblocks.png" alt="abblocks" loading="lazy"></p><hr><h3 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h3><ul><li>提出了一种新的用于图像超分辨率的对抗性多径残差网络(AMPRN)</li><li>该模型在构造多路径残差块（MPRB）后，可以从所有层的输出中提取丰富的多路径特征。</li><li>提出了两种梯度损失方法来生成高感知质量和高客观质量的SR图像。</li></ul><hr><h3 id="Some-Thinkings"><a href="#Some-Thinkings" class="headerlink" title="Some Thinkings"></a>Some Thinkings</h3><ul><li>其中MPRB与IMDN中的IMDB模块很类似，只改动了一点。</li><li>是否可以将全局的这种结构转换为blocks然后获得好的性能。</li><li>提出了一种新颖的对抗网络思路，我们可以借鉴到其他地方如：获得一定的先验图来保证其分布一致。</li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;https://cdn.jsdelivr.net/npm/aplayer@1.10/dist/APlayer.min.css&quot;&gt;&lt;script 
      
    
    </summary>
    
    
      <category term="论文笔记" scheme="http://alexzou14.github.io/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="深度学习" scheme="http://alexzou14.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="笔记" scheme="http://alexzou14.github.io/tags/%E7%AC%94%E8%AE%B0/"/>
    
      <category term="超分辨率" scheme="http://alexzou14.github.io/tags/%E8%B6%85%E5%88%86%E8%BE%A8%E7%8E%87/"/>
    
  </entry>
  
  <entry>
    <title>EDSR代码使用手册</title>
    <link href="http://alexzou14.github.io/2020/12/22/EDSRTools/"/>
    <id>http://alexzou14.github.io/2020/12/22/EDSRTools/</id>
    <published>2020-12-22T09:00:38.000Z</published>
    <updated>2021-09-28T09:06:10.799Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="https://cdn.jsdelivr.net/npm/aplayer@1.10/dist/APlayer.min.css"><script src="https://cdn.jsdelivr.net/npm/aplayer@1.10/dist/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="https://cdn.jsdelivr.net/npm/meting@1.2/dist/Meting.min.js"></script><h3 id="获取EDSR代码"><a href="#获取EDSR代码" class="headerlink" title="获取EDSR代码"></a>获取EDSR代码</h3><p><code>git clone https://github.com/thstkdgus35/EDSR-PyTorch.git</code></p><h3 id="准备数据"><a href="#准备数据" class="headerlink" title="准备数据"></a>准备数据</h3><p>我们以DIV2K数据集为例，我们将获得到的训练数据集按照以下文件放置自己选择好的文件目录中：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">├── DIV2K</span><br><span class="line">│   ├── DIV2K_train_HR</span><br><span class="line">│   ├── DIV2K_train_LR_bicubic</span><br><span class="line">│   │   ├── X4</span><br><span class="line">│   │   │   ├── *****x4.png</span><br><span class="line">│   ├── DIV2K_train_LR_bicubicL</span><br><span class="line">│   │   ├── X4</span><br><span class="line">│   │   │   ├── *****x4.png</span><br></pre></td></tr></table></figure><p>为了减少代码修改工作，我们把对应的HR图像方法到DIV2K_train_HR文件夹中，对于赛道1我们把LR图<code>像放到DIV2K_train_LR_bicubic</code>下的X4文件夹中。对于赛道2我们把LR图像放到<code>DIV2K_train_LR_bicubicL</code> 下的x4文件夹中。<br>对于测试数据我们按照以下方式创建并放入benchmark文件夹中就行：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">├── DIV2K</span><br><span class="line">├── benchmark</span><br><span class="line">│   ├── test</span><br><span class="line">│   │   ├── HR</span><br><span class="line">│   │   ├── LR_bicubic</span><br><span class="line">│   │   │   ├── X4</span><br><span class="line">│   │   ├── LR_bicubicL</span><br><span class="line">│   │   │   ├── X4</span><br></pre></td></tr></table></figure><h3 id="放置模型到model中"><a href="#放置模型到model中" class="headerlink" title="放置模型到model中"></a>放置模型到model中</h3><p>这里我们将自己写好的模型以小写英文命名放入EDSR项目中src文件夹下的model文件夹中中，例如如下ourmdel：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">├── EDSR</span><br><span class="line">│   ├── experiment</span><br><span class="line">│   │   ├── src</span><br><span class="line">│   │   ├── data</span><br><span class="line">│   │   ├── loss</span><br><span class="line">│   │   ├── model</span><br><span class="line">│   │   │   ├── __init__.py</span><br><span class="line">│   │   │   ├── edsr.py</span><br><span class="line">│   │   │   ├── ourmodel.py</span><br><span class="line">│   │   │   ├── ...</span><br></pre></td></tr></table></figure><h3 id="修改data中的文件参数："><a href="#修改data中的文件参数：" class="headerlink" title="修改data中的文件参数："></a>修改data中的文件参数：</h3><p>找到data中的<code>__init__.py</code>文件， 我们打开后，我们修改如下几行：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">for d in args.data_test:</span><br><span class="line">if d in [&#39;Set5&#39;, &#39;Set14&#39;, &#39;B100&#39;, &#39;Urban100&#39;, &#39;test&#39;]: #这里加入test数据集，要不然无法测试test中的数据</span><br><span class="line">m &#x3D; import_module(&#39;data.benchmark&#39;)</span><br><span class="line">testset &#x3D; getattr(m, &#39;Benchmark&#39;)(args, train&#x3D;False, name&#x3D;d)</span><br></pre></td></tr></table></figure><p>我们找到<code>div2k.py</code>和<code>benchmark.py</code>的文件，打开后如下修改：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">如果是进行赛道1则无需修改，直接下一步，如果是赛道2的话在这两个文件中的最后一行修改或加入</span><br><span class="line">self.ext&#x3D;(&#39;.png&#39;, &#39;jpg&#39;) #主要第一个.png是表示HR的文件格式，.jpg是表示LR的文件格式</span><br></pre></td></tr></table></figure><p>针对赛道2我们修改一下<code>srdata.py</code>中的代码:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">self.input_large &#x3D; (args.model &#x3D;&#x3D; &#39;ourmodel&#39;) # 将ourmodel改成自己的模型名称</span><br></pre></td></tr></table></figure><h3 id="调整最终训练的参数"><a href="#调整最终训练的参数" class="headerlink" title="调整最终训练的参数"></a>调整最终训练的参数</h3><p>我们找到src文件夹下的<code>option.py</code>文件，打开修改以下参数：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">parser.add_argument(&#39;--dir_data&#39;, type&#x3D;str, default&#x3D;&#39;..&#x2F;..&#x2F;..&#x2F;dataset&#39;, help&#x3D;&#39;dataset directory&#39;) # 修改default后面的参数，修改成你放DIV2K文件的目录</span><br><span class="line">parser.add_argument(&#39;--data_test&#39;, type&#x3D;str, default&#x3D;&#39;test&#39;, help&#x3D;&#39;test dataset name&#39;) # 放入你设置的测试集名称如test</span><br><span class="line">parser.add_argument(&#39;--epochs&#39;, type&#x3D;int, default&#x3D;300, help&#x3D;&#39;number of epochs to train&#39;) # 修改你所需要的epoch大小</span><br><span class="line">parser.add_argument(&#39;--batch_size&#39;, type&#x3D;int, default&#x3D;16, help&#x3D;&#39;input batch size for training&#39;) # 修改你所需要的batch_size大小</span><br><span class="line">parser.add_argument(&#39;--decay&#39;, type&#x3D;str, default&#x3D;&#39;200-400&#39;, help&#x3D;&#39;learning rate decay type&#39;) #edsr的学习率策略是到了对应的轮数学习率减半，设置你要减半学习率的轮数，如果需要多个轮数降低就用-连接，例如 200-400代表200轮和400轮会分别降低一次学习率</span><br></pre></td></tr></table></figure><h3 id="开始训练或者测试"><a href="#开始训练或者测试" class="headerlink" title="开始训练或者测试"></a>开始训练或者测试</h3><p>训练就是打开<code>demo.sh</code>文件把命令改成如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python main.py --model ourmodel --scale 4 --save ourmodel --pre_train ***&#x2F;***&#x2F;权重地址&#x2F; --chop --patch_size 192 #其中第一次训练可以不用--pre_train参数，chop参数是加速训练，可以不用</span><br></pre></td></tr></table></figure><p>测试用以下命令在<code>demo.sh</code>中：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python main.py --model ourmodel --scale 4 --data_test test --pre_train ***&#x2F;***&#x2F;权重地址&#x2F; --test_only # data_test 参数中的文件夹一定要在benchmark中有名字。 可以加--self_ensemble 参数生成多个结果来求和平均得到更高质量图像</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;https://cdn.jsdelivr.net/npm/aplayer@1.10/dist/APlayer.min.css&quot;&gt;&lt;script 
      
    
    </summary>
    
    
      <category term="编程工具" scheme="http://alexzou14.github.io/categories/%E7%BC%96%E7%A8%8B%E5%B7%A5%E5%85%B7/"/>
    
    
      <category term="pytorch" scheme="http://alexzou14.github.io/tags/pytorch/"/>
    
      <category term="代码框架" scheme="http://alexzou14.github.io/tags/%E4%BB%A3%E7%A0%81%E6%A1%86%E6%9E%B6/"/>
    
  </entry>
  
  <entry>
    <title>Deep Unfolding Network for Image Super-Resolution</title>
    <link href="http://alexzou14.github.io/2020/07/09/USRNet/"/>
    <id>http://alexzou14.github.io/2020/07/09/USRNet/</id>
    <published>2020-07-09T10:01:31.000Z</published>
    <updated>2020-07-09T16:55:16.550Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="https://cdn.jsdelivr.net/npm/aplayer@1.10/dist/APlayer.min.css"><script src="https://cdn.jsdelivr.net/npm/aplayer@1.10/dist/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="https://cdn.jsdelivr.net/npm/meting@1.2/dist/Meting.min.js"></script><h3 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h3><p>基于学习的超分辨率方法是一个端到端的学习方法，近期不断发展，展现了优于传统基于模型的方法的性能和效率。但是对比传统的方法，基于学习方法缺少了传统的基于模型方法的灵活性。作者将传统的基于模型方法和基于学习方法结合得到一种新的端到端的方法，通过半二次分裂算法将最大先验（MAP）问题展开成两个子问题：数据子问题和先验子问题。通过不断迭代求解这两个子问题得到最终的结果。这样设计的好处可以充分理用基于模型方法的灵活性和基于学习方法的性能优势。只需要训练一个网络就可以进行不同尺度超分和去模糊、去噪的效果。</p><hr><h3 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h3><p>单一图像超分是指将低分辨率图像恢复到高分辨率图像的一个严重病态问题。图像超分辨率有非常广泛的应用：提升视觉质量，提高其他高级计算机视觉任务性能等等。<br>由于之前的研究都是将图像降置过程直接最简化成双三插值下采样，但是现实生活中的将置与实验中的降置不一致，导致所提出来的网络在真实场景中的效果很差。现实生活的降置可以用以下表达式描述：\( y=(x\otimes k)\downarrow _s + n \)<br>传统的基于模型的方法在处理模糊和噪声上是有效的，但是由于双三插值的在数学上是复杂的，所以进一步阻碍了基于模型的方法的发展。然而基于学习的方法就很好的可以通过端到端的学习方法提高双三插值退化下的PSNR。<br>所以作者提出了一个USRNet来结合基于学习方法和基于模型方法的优势，既可以有效的处理经典图像退化问题，又可以有效的进行端到端的训练保证有效性和效率。作者通过半二次分裂算法将基于模型的能量函数分成数据项和先验项两个子问题，通过不断迭代求解这两个子问题得到最终的结果。<br>本文主要贡献有：</p><ul><li>USRNet是第一个尝试处理经典退化模型与不同的尺度因子，模糊内核和噪声水平通过一个单一的端到端训练模型。</li><li>为弥合基于模型方法和基于学习的方法之间的差距提供了途径。</li><li>本质上强加了一个降级约束(即，估计的HR图像应符合退化过程)和一个先验约束(即，估计的HR图像应该具有自然特征)上的解决方案。</li><li>在不同退化设置的LR图像上表现良好，显示了巨大的实际应用潜力。</li></ul><hr><h3 id="Related-work"><a href="#Related-work" class="headerlink" title="Related work"></a>Related work</h3><h4 id="Degradation-models"><a href="#Degradation-models" class="headerlink" title="Degradation models"></a>Degradation models</h4><p>退化模型对SISR起到了很重要的作用，因为它定义了LR图像如何从HR图像退化。除了经典退化模型和双三插值退化模型外，SISR文献中还提出了其他几个模型。退化模型假设LR图像是模糊的、具有高斯噪声的双次采样的HR图像。通过假设双三次下采样的干净HR图像也是干净的，将退化模型视为LR图像的去模糊和双三次退化的SISR的组合。虽然已经提出了许多退化模型，但是对于经典退化模型，基于cnn的SISR还没有得到足够的重视，值得进一步研究。</p><h4 id="Flexible-SISR-methods"><a href="#Flexible-SISR-methods" class="headerlink" title="Flexible SISR methods"></a>Flexible SISR methods</h4><p>尽管基于cnn的SISR方法在处理双三次退化方面取得了令人瞩目的成功，但将它们应用于处理其他更实际的退化模型效果并不好。为了实用性的考虑，最好设计一种灵活的超级解析器，它采用三个关键因素，即尺度因子、模糊核和噪声水平。<br>在处理双三插值降置的方法中有很多比较好的工作被提出：LapSRN、MDSR、Meta-SR<br>为了更好的处理模糊的LR图像，RealSR提出了PCA降维模糊核作为输入，但是这些方法都局限于高斯模糊内核。这些方法的主要思想是将学习到的CNN prior插入到MAP框架下的迭代求解中。不幸的是，这些基本上都是基于模型的方法，它们承受着很高的计算负担，并且涉及到手动选择超参数。如何设计一个端到端可训练的模型，以便在更少的迭代次数下获得更好的结果，目前还没有研究。<br>事实上，非盲SISR仍是一个活跃的研究方向。首先，模糊核和噪声水平可以估计，或者是已知的基于其他信息(例如，相机设置)。其次，用户可以通过调整模糊内核和噪声级别来控制锐度和平滑度的偏好。第三，非盲SISR可以作为解决盲SISR的中间步骤。</p><h4 id="Deep-unfolding-image-restoration"><a href="#Deep-unfolding-image-restoration" class="headerlink" title="Deep unfolding image restoration"></a>Deep unfolding image restoration</h4><p>除了深度即插即用方法，深度展开方法还可以集成基于模型的方法和基于学习的方法。它们的主要区别是后者通过在一个大的训练集上最小化损失函数，以端到端方式优化参数，因此，即使迭代次数更少，通常也会产生更好的结果。与单纯的学习方法相比，深度展开方法具有可解释性，能够将退化约束融合到学习模型中。然而，它们中的大多数都有以下一个或几个缺点。(i)未使用深度CNN的先验子问题的解不够强大，无法获得良好的性能。(ii)数据子问题没有采用封闭式解，可能会阻碍收敛。(iii)整个推理是通过一个阶段和微调的方式训练的，而不是一个完整的端到端方式。此外，考虑到经典退化模型不存在深度展开的SISR方法，提出一种克服上述缺点的方法是特别值得关注的。</p><hr><h3 id="Proposed-Method"><a href="#Proposed-Method" class="headerlink" title="Proposed Method"></a>Proposed Method</h3><h4 id="Degradation-model-classical-vs-bicubic"><a href="#Degradation-model-classical-vs-bicubic" class="headerlink" title="Degradation model: classical vs. bicubic"></a>Degradation model: classical vs. bicubic</h4><p>由于现在双三插值降置已经有很多方法可以得到很好的性能，所以研究经典退化模型很有必要。通过以下表达式来近似：\( k_{bicubic}^{\times s}=\arg\min_k||(x\otimes k)\downarrow_s -y||.\)<br>下图表示了近似的2倍3倍4倍的模糊核：</p><blockquote><p><img src="http://sotavision.cn/showdoc/server/../Public/Uploads/2020-07-09/5f07260db887e.png" alt="" loading="lazy"></p></blockquote><h4 id="Unfolding-optimization"><a href="#Unfolding-optimization" class="headerlink" title="Unfolding optimization"></a>Unfolding optimization</h4><p>根据MAP框架可以得到HR的能量函数为：<br>\( E(x)=\dfrac{1}{2\sigma^2}||y-(x\otimes k)\downarrow_s||^2+\lambda \Phi(x) \)<br>其中\(\dfrac{1}{2\sigma^2}||y-(x\otimes k)\downarrow_s||^2\)是数据项，\(\Phi(x)\)是正则化项，通过半二次分裂算法引入中间变量z得到变化式如下：<br>\(E_{\mu}(x,z)=\dfrac{1}{2\sigma^2}||y-(z\otimes k)\downarrow_s||^2+\lambda\Phi(x)+\dfrac{\mu}{2}||z-x||^2\)<br>这个问题可以通过迭代求解以下两个子问题解决：<br>\(z_k=\arg\min_z||y-(z\otimes k)\downarrow_s||^2+\mu\sigma^2||z-x_{k-1}||^2\)<br>\(x_k=\arg\min_x \dfrac{\mu}{2}||z_k-x||^2+\lambda\Phi(x)\)<br>作者使用傅里叶变换来求解数据项中的\(z_k\)如下式：<br>\(z_k=F^{-1}(\dfrac{1}{\alpha_k}(d-\overline{F(k)}\odot_s\dfrac{(F(k)d)\Downarrow_s}{(\overline{F(k)F(k)\Downarrow_s+\alpha_k})})) \)<br>其中d为：<br>\(d=\overline{F(k)}F(y\uparrow_s)+\alpha_k F(x_{k-1})\)</p><h4 id="Deep-unfolding-network"><a href="#Deep-unfolding-network" class="headerlink" title="Deep unfolding network"></a>Deep unfolding network</h4><p>整体网络如下图所示：</p><blockquote><p><img src="http://sotavision.cn/showdoc/server/../Public/Uploads/2020-07-09/5f0735332aeaa.png" alt="" loading="lazy"></p></blockquote><p>Data module：<br>\(z_k=D(x_{k-1},s,k,y,\alpha_k)\)对应数据项的傅里叶变换求解。<br>Prior module:<br>\(x_k=P(z_k,\beta_k)\)对应先验项求解，使用端到端的深度网络求解<br>Hyper-parameter module:<br>\([\alpha, \beta]=H(\sigma,s)\)</p><hr><h3 id="Experiment"><a href="#Experiment" class="headerlink" title="Experiment"></a>Experiment</h3><h4 id="Experiments-Details"><a href="#Experiments-Details" class="headerlink" title="Experiments Details"></a>Experiments Details</h4><p>网络具体实施细节可以参考官网代码：<a href="https://github.com/cszn/USRNet" target="_blank" rel="noopener">https://github.com/cszn/USRNet</a></p><h4 id="Comparisons-with-state-of-the-arts"><a href="#Comparisons-with-state-of-the-arts" class="headerlink" title="Comparisons with state-of-the-arts"></a>Comparisons with state-of-the-arts</h4><blockquote><p><img src="http://sotavision.cn/showdoc/server/../Public/Uploads/2020-07-09/5f0737b6db0b5.png" alt="" loading="lazy"></p></blockquote><p>USRNet在不同的模糊核和噪声上都取得了比较好的结果。</p><blockquote><p><img src="http://sotavision.cn/showdoc/server/../Public/Uploads/2020-07-09/5f073875346d6.png" alt="" loading="lazy"></p></blockquote><p>在bicubic降置上一样取得了比较好的结果。</p><h4 id="Network-analysis"><a href="#Network-analysis" class="headerlink" title="Network analysis"></a>Network analysis</h4><blockquote><p><img src="http://sotavision.cn/showdoc/server/../Public/Uploads/2020-07-09/5f0738c7ad7ed.png" alt="" loading="lazy"></p></blockquote><p>可以看出D模块和P模块在不断相互促进回复图像，D模块主要用来去噪和去模糊，P模块用来回复细节。</p><blockquote><p><img src="http://sotavision.cn/showdoc/server/../Public/Uploads/2020-07-09/5f073a9659afe.png" alt="" loading="lazy"></p></blockquote><p>可以看出设置的超参数和之前设定的一样。</p><hr><h3 id="Conclusions"><a href="#Conclusions" class="headerlink" title="Conclusions"></a>Conclusions</h3><ul><li>这篇文章主要关注经典的SISR退化模型，并提出了一个深展开超分辨率网络。</li><li>受传统基于模型方法展开优化的启发，我们设计了一个集模型方法的灵活性和基于学习方法优点于一体的端到端可训练深度网络</li><li>该网络可以通过单一模型处理经典的退化模型，并且取得了SOTA的效果</li><li>大量的实验结果证明了该方法的灵活性、有效性和通用性，用于超分辨各种降级LR图像。</li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;https://cdn.jsdelivr.net/npm/aplayer@1.10/dist/APlayer.min.css&quot;&gt;&lt;script 
      
    
    </summary>
    
    
      <category term="论文笔记" scheme="http://alexzou14.github.io/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="深度学习" scheme="http://alexzou14.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="笔记" scheme="http://alexzou14.github.io/tags/%E7%AC%94%E8%AE%B0/"/>
    
      <category term="超分辨率" scheme="http://alexzou14.github.io/tags/%E8%B6%85%E5%88%86%E8%BE%A8%E7%8E%87/"/>
    
  </entry>
  
  <entry>
    <title>Image Super-Resolution Using Dense Skip Connections 论文阅读笔记</title>
    <link href="http://alexzou14.github.io/2020/04/30/SRDenseNet/"/>
    <id>http://alexzou14.github.io/2020/04/30/SRDenseNet/</id>
    <published>2020-04-30T04:19:35.000Z</published>
    <updated>2020-04-30T04:40:43.000Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="https://cdn.jsdelivr.net/npm/aplayer@1.10/dist/APlayer.min.css"><script src="https://cdn.jsdelivr.net/npm/aplayer@1.10/dist/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="https://cdn.jsdelivr.net/npm/meting@1.2/dist/Meting.min.js"></script><h3 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h3><p>近期单一超分领域中，使用深度卷积网络极大的增强了网络的性能。所以本文提出了一种密集跳跃连接方法将高频特征和低频特征联系起来，然后通过一个反卷积将图像还原，加速图像超分重建。本文方法的有点有：</p><ul><li>通过级联Dense Block大大减小了参数数量</li><li>提高了计算效益</li><li>性能显著提升</li></ul><hr><h3 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h3><p>本文这里主要简单介绍了超分辨率的问题是有低分辨率图像到高分辨率重建的问题，这个问题主要存在就是重建过程中的不适定问题，恢复出来的图像有多个可能。在超分领域中，大缩放因子下，图像的细节会丢失很多。LR图像可以给HR图像提供很多必要的高频细节。本文还提出更深的一个网络有更大的receptive field(接受野)来获取更多的信息，但是更深的网络会存在梯度爆炸的问题，梯度爆炸的问题可以通过增加跳跃连接来解决。本文提出的密集跳层连接改善了网络中的信息传递，缓解了梯度爆炸的问题。</p><hr><h3 id="Related-work"><a href="#Related-work" class="headerlink" title="Related work"></a>Related work</h3><h4 id="Single-image-super-resolution"><a href="#Single-image-super-resolution" class="headerlink" title="Single image super-resolution"></a>Single image super-resolution</h4><p>作者在这一部分介绍了单一图像超分辨率的主要方法，其中有基于插值的方法，这个方法易于实现，但是图像还原的能力有限，生成图像比较模糊。还有就是基于稀疏编码的方法可以得到比较好的结果，但是求解稀疏编码的过程通常计算量很大。在当时最近，卷积神经网络的方法从SRCNN方法提出后就飞速发展，并且在超分辨率的问题上效果很好。</p><h4 id="Skip-connection"><a href="#Skip-connection" class="headerlink" title="Skip connection"></a>Skip connection</h4><p>随着神经网络的深度加深，梯度爆炸的问题也随之产生了。很多方法为了解决这个问题都引用了跳跃连接的方法，例如：ResNets。DenseNet利用了所有层相互连接的方式来充分探索跳跃连接的优点。<br>因此使用合理数量的跳跃连接可以提高网络的性能。</p><h4 id="Contribution"><a href="#Contribution" class="headerlink" title="Contribution"></a>Contribution</h4><ol><li>通过密集的跳跃连接实现不同层次的特征融合可以进一步提高SISR的性能。</li><li>该框架不仅取得了比较好的性能，而且执行起来更快。</li></ol><hr><h3 id="Proposed-Method"><a href="#Proposed-Method" class="headerlink" title="Proposed Method"></a>Proposed Method</h3><h4 id="Overviewing-network-structure"><a href="#Overviewing-network-structure" class="headerlink" title="Overviewing network structure"></a>Overviewing network structure</h4><p>网络可以有一下几个部分构成：<br>卷积层：学习低频特征<br>DenseNet块：学习高频特征<br>反卷积层：用于上采样恢复图像<br>以下作者提出了三种不同的网络结构如图所示：</p><blockquote><p><img src="http://sotavision.cn/showdoc/server/../Public/Uploads/2020-01-20/5e2571f402122.png" alt="" loading="lazy"></p></blockquote><p>主要的优化的损失函数用了MSEloss：</p><div>    \[ l(\Theta)=\dfrac{1}{N}\displaystyle\sum_{k=1}^N || F(I_L^k,\Theta)-I_H^k ||^2 \] <div><h4 id="DenseNet-blocks"><a href="#DenseNet-blocks" class="headerlink" title="DenseNet blocks"></a>DenseNet blocks</h4><p>在每一个对应的DenseNet block中每层卷积都相互连接，其中第i层的输出可以表达为：<br>\[X_i=\max(0, w_i * [X_1, X_2,…,X_{i-1}]+b_i)\]</p><blockquote><p><img src="http://sotavision.cn/showdoc/server/../Public/Uploads/2020-01-20/5e2574be5b2a6.png" alt="" loading="lazy"></p></blockquote><p>这里的密集跳层加强了信息在深度网络的流动，充分利用了图像信息。</p><h4 id="Deconvoplution-layers"><a href="#Deconvoplution-layers" class="headerlink" title="Deconvoplution layers"></a>Deconvoplution layers</h4><p>反卷积的引入加速了图像的重建和训练的时间</p><hr><h3 id="Experiment"><a href="#Experiment" class="headerlink" title="Experiment"></a>Experiment</h3><h4 id="Experiments-Details"><a href="#Experiments-Details" class="headerlink" title="Experiments Details"></a>Experiments Details</h4><p>实验主要代码可以参考：暂无</p><h4 id="Network-analysis"><a href="#Network-analysis" class="headerlink" title="Network analysis"></a>Network analysis</h4><p>对比设计三个网络的PSNR和SSIM的收敛图；</p><blockquote><p><img src="http://sotavision.cn/showdoc/server/../Public/Uploads/2020-01-20/5e25bc73c038a.png" alt="" loading="lazy"><br><img src="http://sotavision.cn/showdoc/server/../Public/Uploads/2020-01-20/5e25bcc898c1b.png" alt="" loading="lazy"></p></blockquote><p>可以看出每个地方都加入跳层的效果最好，这样充分利用了每个阶段的信息。</p><h4 id="Comparisons-with-state-of-the-arts"><a href="#Comparisons-with-state-of-the-arts" class="headerlink" title="Comparisons with state-of-the-arts"></a>Comparisons with state-of-the-arts</h4><blockquote><p><img src="http://sotavision.cn/showdoc/server/../Public/Uploads/2020-01-20/5e25bce186362.png" alt="" loading="lazy"></p></blockquote><hr><h3 id="Conclusions"><a href="#Conclusions" class="headerlink" title="Conclusions"></a>Conclusions</h3><p>文章中针对用于最后重建的输入内容不同，设计了三种结构并做了比较。一是反卷积层只输入最顶层稠密块的输出。二是添加了一个跳跃连接，将最底层卷积层的输出特征和最顶层稠密块的输出特征串联起来，再输入反卷积层。三是添加了稠密跳跃连接，就是把稠密块看成一个整体，第一个卷积层的输出以及每个稠密块的输出，都输入给在之后的所有稠密块，像是把在反卷积层之前的整个网络也设计成像稠密块那样的结构。由于这样做，所有的特征都串联起来，这样直接输入反卷积层会产生巨大的计算开销，因此添加了一个核大小为1×1的卷积层来减小特征数量，这个卷积层被称为瓶颈层。最后的结果是越复杂的越好，3&gt;2&gt;1。文章中分析的是，受益于低层特征和高层特征的结合，超分辨率重建的性能得到了提升。像第三种结构把所有深度层的特征都串联起来，得到了最佳的结果，说明不同深度层的特征之间包含的信息是互补的。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;https://cdn.jsdelivr.net/npm/aplayer@1.10/dist/APlayer.min.css&quot;&gt;&lt;script 
      
    
    </summary>
    
    
      <category term="论文笔记" scheme="http://alexzou14.github.io/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="深度学习" scheme="http://alexzou14.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="笔记" scheme="http://alexzou14.github.io/tags/%E7%AC%94%E8%AE%B0/"/>
    
      <category term="超分辨率" scheme="http://alexzou14.github.io/tags/%E8%B6%85%E5%88%86%E8%BE%A8%E7%8E%87/"/>
    
  </entry>
  
  <entry>
    <title>Image Super-Resolution Using Deep Convolutional Networks 论文阅读笔记</title>
    <link href="http://alexzou14.github.io/2020/04/30/SRCNN/"/>
    <id>http://alexzou14.github.io/2020/04/30/SRCNN/</id>
    <published>2020-04-30T03:55:28.000Z</published>
    <updated>2020-04-30T04:18:38.000Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="https://cdn.jsdelivr.net/npm/aplayer@1.10/dist/APlayer.min.css"><script src="https://cdn.jsdelivr.net/npm/aplayer@1.10/dist/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="https://cdn.jsdelivr.net/npm/meting@1.2/dist/Meting.min.js"></script><h3 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h3><p>作者提出了一种基于深度学习的单影像超分辨率重建方法。我们直接以端对端的方式学习高分辨率影像和低分辨率影像之间的mapping。Mapping可以用一个深度卷积神经网络来表示，通过输入低分辨率的影像输出高分辨率的影像。</p><hr><h3 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h3><p>当时，单幅图像的超分辨率重建大多都是基于样本学习的，如稀疏编码就是典型的方法之一。这种方法一般先对图像进行特征提取，然后编码成一个低分辨率字典，稀疏系数传到高分辨率字典中重建高分辨率部分，然后将这些部分汇聚作为输出。以往的SR方法都关注学习和优化字典或者建立模型，很少去优化或者考虑统一的优化框架。<br>为了解决上述问题，本文中提出了一种深度卷积神经网络（SRCNN），即一种LR到HR的端对端映射，具有如下性质：</p><ol><li>结构简单，与其他现有方法相比具有优越的正确性，对比结果如下：</li><li>滤波器和层的数量适中，即使在CPU上运行速度也比较快，因为它是一个前馈网络，而且在使用时不用管优化问题；</li><li>实验证明，该网络的复原质量可以在大的数据集或者大的模型中进一步提高。</li></ol><p>本文的主要贡献：</p><ol><li>我们提出了一个卷积神经网络用于图像超分辨率重建，这个网络直接学习LR到HR图像之间端对端映射，几乎没有优化后的前后期处理。</li><li>将深度学习的SR方法与基于传统的稀疏编码相结合，为网络结构的设计提供指导。</li><li>深度学习在超分辨率问题上能取得较好的质量和速度。</li></ol><hr><h3 id="Related-work"><a href="#Related-work" class="headerlink" title="Related work"></a>Related work</h3><h4 id="Image-Super-Resolution"><a href="#Image-Super-Resolution" class="headerlink" title="Image Super-Resolution"></a>Image Super-Resolution</h4><p>指通过低分辨率图像或图像序列恢复出高分辨率图像。高分辨率图像意味着图像具有更多的细节信息、更细腻的画质,，这些细节在高清电视、医学成像、遥感卫星成像等领域有着重要的应用价值。</p><h4 id="Convolutional-Neural-Networks"><a href="#Convolutional-Neural-Networks" class="headerlink" title="Convolutional Neural Networks"></a>Convolutional Neural Networks</h4><p>许多因素是CNN风靡的关键原因：</p><ol><li>在现在的GPU高效的训练 </li><li>矫正线性单元RELU的提出使得在同样实现高精度的同时收敛更快；</li><li>对于训练大模型获取的丰富数据集更加简单。我们的方法同样收益与这些成就。</li></ol><h4 id="Deep-Learning-for-Image-Restoration"><a href="#Deep-Learning-for-Image-Restoration" class="headerlink" title="Deep Learning for Image Restoration"></a>Deep Learning for Image Restoration</h4><p>有很多的深度学习方法用于影像的恢复。多层感知机（MLP）所有的图层都是全连接的（与卷积相反），应用于自然影像的降噪和后模糊降噪。</p><hr><h3 id="Proposed-Method"><a href="#Proposed-Method" class="headerlink" title="Proposed Method"></a>Proposed Method</h3><h4 id="Network-structure"><a href="#Network-structure" class="headerlink" title="Network structure"></a>Network structure</h4><blockquote><p><img src="http://sotavision.cn/showdoc/server/../Public/Uploads/2020-01-22/5e27ffd852093.png" alt="" loading="lazy"></p></blockquote><p>使用双三次插值将单幅低分辨率图像变成我们想要的大小，假设这个内插值的图像为Y,我们的目标是从Y中恢复图像F(Y)使之尽可能与高分辨率图像X相似，为了便于区分，我们仍然把Y称为低分辨率图像，尽管它与X大小相同，我们希望学习到这个映射函数F，需要以下三部：</p><h4 id="Patch-extraction-and-representation"><a href="#Patch-extraction-and-representation" class="headerlink" title="Patch extraction and representation"></a>Patch extraction and representation</h4><p>这个操作将一个高维向量映射到另一个高维向量，每一个映射向量表示一个高分辨率patch,这些向量组成另一个特征映射。<br>第一层定义为函数\(F_1\)：<br>\[F_1(Y) = \max(0,W_1 \times Y + B_1)\]<br>其中，\(W_1\)和\(B_1\)分别代表滤波器和偏差，\(W_1\)的大小为\(c \times f_1 \times f_1 \times n_1\), c是输入图像的通道数，\(f_1\)是滤波器的空间大小，\(n_1\)是滤波器的数量。从直观上看，\(W_1\)使用\(n_1\)个卷积，每个卷积核大小为\(c \times f_1 \times f_1\)。输出是\(n_1\)给特征映射。\(B_1\)是一个\(n_1\)维的向量，每个元素都与一个滤波器有关，在滤波器响应中使用Rectiﬁed Linear Unit (ReLU,\(\max(0,x)\)) </p><h4 id="Non-linear-mapping"><a href="#Non-linear-mapping" class="headerlink" title="Non-linear mapping"></a>Non-linear mapping</h4><p>这个操作将一个高维向量映射到另一个高维向量，每一个映射向量表示一个高分辨率patch,这些向量组成另一个特征映射。<br>第二步将\(n_1\)维的向量映射到\(n_2\)维，这相当于使用\(n_2\)个\(1 \times 1\)的滤波器，第二层的操作如下：<br>\[F_2(Y) = max(0,W_2 \times F_1(Y) + B_2)\]<br>其中，\(W_2\)的大小为\(n_1*1*1*n_2\)，\(B_2\)是\(n_2\)维的向量，每个输出的\(n_2\)维向量都表示一个高分辨率块（patch）用于后续的重建。<br>当然，也可以添加更多的卷积层（1*1的）来添加非线性特征，但会增加模型的复杂度，也需要更多的训练数据和时间，在本文中，我们采用单一的卷积层，因为它已经能取得较好的效果。</p><h4 id="Reconstruction"><a href="#Reconstruction" class="headerlink" title="Reconstruction"></a>Reconstruction</h4><p>这个操作汇聚所有的高分辨率patch构成最够的高分辨率图像，我们期望这个图像能与X相似。<br>在传统的方法中，预测的重叠高分辨率块经常取平均后得到最后的图像，这个平均化可以看作是预先定义好的用于一系列特征映射的滤波器（每个位置都是高分辨率块的“扁平”矢量形式），因此，我们定义一个卷积层产生最后的超分辨率图像：<br>\[F(Y) = W_3 \times F_2(Y) + B_3\]<br>W3的大小为\(n_2 \times f_3 \times f_3 \times c\)，\(B_3\)是一个c维向量。<br>如果这个高分辨率块都在图像域，我们把这个滤波器当成均值滤波器；如果这些高分辨率块在其他域，则\(W_3\)首先将系数投影到图像域然后再做均值，无论哪种情况，\(W_3\)都是一个线性滤波器。 </p><h4 id="Relationship-to-Sparse-Coding-Based-Methods"><a href="#Relationship-to-Sparse-Coding-Based-Methods" class="headerlink" title="Relationship to Sparse-Coding-Based Methods"></a>Relationship to Sparse-Coding-Based Methods</h4><p>基于稀疏编码的图像超分辨率方法也可以看作是一个卷积神经网络，如图：</p><blockquote><p><img src="http://sotavision.cn/showdoc/server/../Public/Uploads/2020-01-22/5e2802fc8daf9.png" alt="" loading="lazy"></p></blockquote><p>上图展示了基于稀疏编码的SR方法可以看成是一种卷积神经网络（非线性映射不同），但在稀疏编码中，被不是所有的操作都有优化，而卷积神经网络中，低分辨率字典、高分辨率字典、非线性映射，以及减去均值和求平均值等经过滤波器进行优化，所以我们的方法是一种端对端的映射。</p><h4 id="loss-function"><a href="#loss-function" class="headerlink" title="loss function"></a>loss function</h4><p>学习端对端的映射函数F需要评估以下参数：\(\theta =\{W_1,W_2,W_3,B_1,B_2,B_3\}\)。最小化重建函数\(F(Y;\theta)\) 与对于的高分辨率图像X之间的损失，给出一组高分辨率图像 \(\{X_i\}\) 和对应得低分辨率图像 \(\{Y_i\}\)，使用 均方误差（Mean Squared Error，MSE)作为损失函数：</p><blockquote><p><img src="http://sotavision.cn/showdoc/server/../Public/Uploads/2020-01-22/5e28044c70151.png" alt="" loading="lazy"></p></blockquote><hr><h3 id="Experiment"><a href="#Experiment" class="headerlink" title="Experiment"></a>Experiment</h3><h4 id="Experiments-Details"><a href="#Experiments-Details" class="headerlink" title="Experiments Details"></a>Experiments Details</h4><p>论文官方代码：<a href="http://mmlab.ie.cuhk.edu.hk/projects/SRCNN.html" target="_blank" rel="noopener">http://mmlab.ie.cuhk.edu.hk/projects/SRCNN.html</a></p><h4 id="Network-analysis"><a href="#Network-analysis" class="headerlink" title="Network analysis"></a>Network analysis</h4><h5 id="Filter-number"><a href="#Filter-number" class="headerlink" title="Filter number"></a>Filter number</h5><blockquote><p><img src="http://sotavision.cn/showdoc/server/../Public/Uploads/2020-01-22/5e2805178eb8a.png" alt="" loading="lazy"></p></blockquote><h5 id="Filter-size"><a href="#Filter-size" class="headerlink" title="Filter size"></a>Filter size</h5><blockquote><p><img src="http://sotavision.cn/showdoc/server/../Public/Uploads/2020-01-22/5e2805412afe6.png" alt="" loading="lazy"></p></blockquote><h5 id="Number-of-layers"><a href="#Number-of-layers" class="headerlink" title="Number of layers"></a>Number of layers</h5><blockquote><p><img src="http://sotavision.cn/showdoc/server/../Public/Uploads/2020-01-22/5e280574a57f2.png" alt="" loading="lazy"><br><img src="http://sotavision.cn/showdoc/server/../Public/Uploads/2020-01-22/5e28058918dc4.png" alt="" loading="lazy"></p></blockquote><h4 id="Comparisons-with-state-of-the-arts"><a href="#Comparisons-with-state-of-the-arts" class="headerlink" title="Comparisons with state-of-the-arts"></a>Comparisons with state-of-the-arts</h4><blockquote><p><img src="http://sotavision.cn/showdoc/server/../Public/Uploads/2020-01-22/5e2805cb6b141.png" alt="" loading="lazy"></p></blockquote><hr><h3 id="Conclusions"><a href="#Conclusions" class="headerlink" title="Conclusions"></a>Conclusions</h3><ul><li>提出了一种新的深度学习方法用于单幅图像的超分辨率重建，传统的基于稀疏编码的方法可以看作一个深的卷积神经网络。</li><li>提出的SRCNN方法是一种在LR和HR图像之间的端对端映射，在优化时几乎不需要额外的预处理和后处理，结构也比较简单，比以往的方法都要好。</li><li>推测通过采取更多的滤波器和不同的训练策略可以获得另外的表现效果。</li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;https://cdn.jsdelivr.net/npm/aplayer@1.10/dist/APlayer.min.css&quot;&gt;&lt;script 
      
    
    </summary>
    
    
      <category term="论文笔记" scheme="http://alexzou14.github.io/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="深度学习" scheme="http://alexzou14.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="笔记" scheme="http://alexzou14.github.io/tags/%E7%AC%94%E8%AE%B0/"/>
    
      <category term="超分辨率" scheme="http://alexzou14.github.io/tags/%E8%B6%85%E5%88%86%E8%BE%A8%E7%8E%87/"/>
    
  </entry>
  
  <entry>
    <title>Progressive Fusion Video Super-Resolution Network via Exploiting Non-Local Spatio-Temporal Correlations论文阅读笔记</title>
    <link href="http://alexzou14.github.io/2020/04/30/PFNL/"/>
    <id>http://alexzou14.github.io/2020/04/30/PFNL/</id>
    <published>2020-04-30T03:46:18.000Z</published>
    <updated>2020-04-30T03:53:11.000Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="https://cdn.jsdelivr.net/npm/aplayer@1.10/dist/APlayer.min.css"><script src="https://cdn.jsdelivr.net/npm/aplayer@1.10/dist/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="https://cdn.jsdelivr.net/npm/meting@1.2/dist/Meting.min.js"></script><h3 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h3><p>以前的大多数融合策略要么无法充分利用时间信息，要么花费太多时间，而如何有效地融合连续帧中的时间信息在视频超分辨率（SR）中起着重要作用。 在这项研究中，我们提出了一种用于视频SR的新型渐进融合网络，该网络旨在更好地利用时空信息，并被证明比现有的直接融合，慢速融合或3D卷积策略更加有效。 在这种渐进融合框架下，我们进一步引入了一种改进的非局部操作，以避免像以前的视频SR方法那样复杂的运动估计和运动补偿（ME＆MC）程序。并且在现有的方法基础上提高了0.96dB和加快了3倍的运行速度。</p><hr><h3 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h3><p>图像超分是一个有低分辨率输入重建成高分辨率图像的问题。对于单一图像超分问题而言，输入仅仅只有一张图像，所以这个问题主要在于利用图像的空间信息。对于视频超分而言，这个问题强调内部帧的时间联系。现有的大多数视频超分方法是基于时间和基于空间量两种方法来利用时间信息。<br>基于时间的方法将帧作为时间序列数据，然后一帧一帧的传入网络，缺点是运行速度慢，不能并行处理帧信息。<br>基于空间的方法采用了多帧补充来帮助重建目标帧的图像。大多数基于空间的方法采用了直接融合，缓慢融合和3D卷积的方法来进行时间信息的融合。</p><blockquote><p><img src="http://sotavision.cn/showdoc/server/../Public/Uploads/2020-04-30/5eaa3225a4609.png" alt="" loading="lazy"></p></blockquote><p>但是现在视频超分方法主要的存在的问题有两个：</p><ol><li>引入了过多的参数，导致训练和测试困难。</li><li>不正确的运动估计会让效果表现变差</li></ol><p>本文主要贡献为：</p><ol><li>作者通过一些连的渐进融合残差块来建立了一个新奇的渐进融合网络。</li><li>作者提出的PFRB更好的从多帧中利用是空间信息。</li><li>网络利用参数共享的方式有效的减少了参数数量</li><li>提出了一种non-local的操作，通过这种方式可以避免复杂的运动估计和运动补偿，更好的从多帧中利用时空信息来重建图像。</li></ol><hr><h3 id="Related-work"><a href="#Related-work" class="headerlink" title="Related work"></a>Related work</h3><p>作者在这部分主要介绍了从董超老师提出的SRCNN开始就有很多的基于CNN的方法提出。分别从基于时域的方法和空间域方法来介绍视频超分方法的发展过程。并且得出结论，之前的融合方法没能完全利用时序信息或是太耗时；所以作者提出了PFNL方法。</p><hr><h3 id="Proposed-Method"><a href="#Proposed-Method" class="headerlink" title="Proposed Method"></a>Proposed Method</h3><h4 id="Network-structure"><a href="#Network-structure" class="headerlink" title="Network structure"></a>Network structure</h4><p>网络结构如下图所示：</p><blockquote><p><img src="http://sotavision.cn/showdoc/server/../Public/Uploads/2020-04-30/5eaa39ba517e0.png" alt="" loading="lazy"></p></blockquote><h4 id="Progressive-Fusion-Network"><a href="#Progressive-Fusion-Network" class="headerlink" title="Progressive Fusion Network"></a>Progressive Fusion Network</h4><p>PFRB可以有如下表达式描述：<br>\[I_t^1=C_t^1(I_t^0)\]<br>其中\(I_t^0\)代表当前输入的帧，\(C_t^1(\cdot)\)代表卷积操作。最后得到了特征图\(\{I_{t-2}^1,…,I_{t+2}^1\}\)然后有<br>\[I^2=C^2(I^1)=C^2(\{I_{t-2}^1,…,I_{t+2}^1\})\]<br>\[O_t=C_t^3(I_t^3)+I_t^0=C_t^3({I^2,I_t^1 })\]</p><h4 id="Non-Local-Residual-Block"><a href="#Non-Local-Residual-Block" class="headerlink" title="Non-Local Residual Block"></a>Non-Local Residual Block</h4><blockquote><p><img src="http://sotavision.cn/showdoc/server/../Public/Uploads/2020-04-30/5eaa40bb9b156.png" alt="" loading="lazy"></p></blockquote><p>non-local操作可以表示为:\(y_i=\dfrac{1}{C(x)}\displaystyle\sum_{\forall j}f(x_i,x_j)g(x_j)\)<br>最后输出z可以表示为\(z_i=W_z y_i+x_i\)</p><h4 id="loss-function"><a href="#loss-function" class="headerlink" title="loss function"></a>loss function</h4><p>作者使用了Charbonnier loss function:<br>\[L_{SR}=\sqrt{(H-SR(I))^2+\varepsilon^2}\]</p><hr><h3 id="Experiment"><a href="#Experiment" class="headerlink" title="Experiment"></a>Experiment</h3><h4 id="Experiments-Details"><a href="#Experiments-Details" class="headerlink" title="Experiments Details"></a>Experiments Details</h4><p>实验细节可以参考官方代码：<a href="http://github.com/psychopa4/PFNL" target="_blank" rel="noopener">http://github.com/psychopa4/PFNL</a></p><h4 id="Non-Local-Operation-vs-Motion-Estimation-and-Compensation"><a href="#Non-Local-Operation-vs-Motion-Estimation-and-Compensation" class="headerlink" title="Non-Local Operation vs Motion Estimation and Compensation"></a>Non-Local Operation vs Motion Estimation and Compensation</h4><blockquote><p><img src="http://sotavision.cn/showdoc/server/../Public/Uploads/2020-04-30/5eaa476b3c3ca.png" alt="" loading="lazy"></p></blockquote><p>实验还证明了采用Non-Local Residual Block比用运动补偿能够更好的提高效果。NLRB相比于运动估算的方法，可以减少参数量，而且很容易插入到现有的网络中，也不需要额外的损失函数</p><h4 id="Comparisons-with-state-of-the-arts"><a href="#Comparisons-with-state-of-the-arts" class="headerlink" title="Comparisons with state-of-the-arts"></a>Comparisons with state-of-the-arts</h4><p>与其他算法对比：</p><blockquote><p><img src="http://sotavision.cn/showdoc/server/../Public/Uploads/2020-04-30/5eaa480589346.png" alt="" loading="lazy"></p></blockquote><p>运行时间和参数量对比：</p><blockquote><p><img src="http://sotavision.cn/showdoc/server/../Public/Uploads/2020-04-30/5eaa4812b273e.png" alt="" loading="lazy"></p></blockquote><hr><h3 id="Conclusions"><a href="#Conclusions" class="headerlink" title="Conclusions"></a>Conclusions</h3><ol><li>作者提出了一个新奇的渐进融合网络来充分利用时间和空间信息。</li><li>作者引入并改进了适合视频SR的NLRB，直接捕获远程依赖关系，而不是采用传统的ME&amp;MC。</li><li>作者提出的网络用较少的参数和更快的速度表现出了更好的性能。</li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;https://cdn.jsdelivr.net/npm/aplayer@1.10/dist/APlayer.min.css&quot;&gt;&lt;script 
      
    
    </summary>
    
    
      <category term="论文笔记" scheme="http://alexzou14.github.io/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="深度学习" scheme="http://alexzou14.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="笔记" scheme="http://alexzou14.github.io/tags/%E7%AC%94%E8%AE%B0/"/>
    
      <category term="超分辨率" scheme="http://alexzou14.github.io/tags/%E8%B6%85%E5%88%86%E8%BE%A8%E7%8E%87/"/>
    
  </entry>
  
  <entry>
    <title>Accelerating the Super-Resolution Convolutional Neural Network论文阅读笔记</title>
    <link href="http://alexzou14.github.io/2020/04/23/FSRCNN/"/>
    <id>http://alexzou14.github.io/2020/04/23/FSRCNN/</id>
    <published>2020-04-23T11:47:42.000Z</published>
    <updated>2020-04-23T12:06:07.000Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="https://cdn.jsdelivr.net/npm/aplayer@1.10/dist/APlayer.min.css"><script src="https://cdn.jsdelivr.net/npm/aplayer@1.10/dist/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="https://cdn.jsdelivr.net/npm/meting@1.2/dist/Meting.min.js"></script><h3 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h3><p>FSRCNN仍然是由港中文大学的Dong Chao, Tang XiaoOu等人做出来的文章，是SRCNN（将CNN引入超分辨率处理的开山之作）之后的又一力作。<br>该文章发表在CVPR2016上的文章，声称能在CPU上进行实时处理视频超分辨率。</p><hr><h3 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h3><h4 id="FSRCNN改进之处"><a href="#FSRCNN改进之处" class="headerlink" title="FSRCNN改进之处"></a>FSRCNN改进之处</h4><ol><li>并不是将三次插值后的图像当做输入，而是直接将LR图像丢入到网络中，最后选用deconv进行放大</li><li>在映射Layer进行了改进，先shrink再将其复原</li><li>更多的映射layer和更小的kernel</li><li>共享其中的mapping layer，如果需要训练不同的upscale model，最后仅需要fine-tuning最后的deconvLayer</li></ol><h4 id="SRCN中存在的问题"><a href="#SRCN中存在的问题" class="headerlink" title="SRCN中存在的问题"></a>SRCN中存在的问题</h4><ol><li>一般网络都直接对Interpolated LR投入到网络中，这便带来复杂的计算开销，假设要放大n倍，那么计算复杂度则上升到了n^2。</li><li>非线性层映射的参数太过臃肿。</li></ol><h4 id="FSRCN处理的方法"><a href="#FSRCN处理的方法" class="headerlink" title="FSRCN处理的方法"></a>FSRCN处理的方法</h4><ol><li>取消ILR输入，而是采用LR输入，在最后引用deconv </li><li>改进mapping layer</li></ol><h4 id="本文主要贡献"><a href="#本文主要贡献" class="headerlink" title="本文主要贡献"></a>本文主要贡献</h4><ol><li>设计漏斗结构的卷积网络，不需要预处理操作</li><li>速度提升</li><li>训练速度快，只要改变最后的解卷积层就可以。</li></ol><hr><h3 id="Related-work"><a href="#Related-work" class="headerlink" title="Related work"></a>Related work</h3><p>这里介绍过程和之前作者发的SRCN的大同小异，详细可以看SRCN的阅读笔记</p><hr><h3 id="Proposed-Method"><a href="#Proposed-Method" class="headerlink" title="Proposed Method"></a>Proposed Method</h3><h4 id="SRCNN和FSRCNN的对比图例"><a href="#SRCNN和FSRCNN的对比图例" class="headerlink" title="SRCNN和FSRCNN的对比图例"></a>SRCNN和FSRCNN的对比图例</h4><blockquote><p><img src="http://sotavision.cn/showdoc/server/../Public/Uploads/2020-01-22/5e280945aa4df.png" alt="" loading="lazy"></p></blockquote><h4 id="Feature-extraction"><a href="#Feature-extraction" class="headerlink" title="Feature extraction"></a>Feature extraction</h4><p>在SRCNN中，feature extraction选的kernel size 为9，然而SRCNN是针对ILR（插值后的低分辨率图像）进行操作的。而FSRCNN则是对LR进行操作，因此在选取kernel的时候，可以选取小的一点。作者选取为5x5</p><h4 id="Shrinking"><a href="#Shrinking" class="headerlink" title="Shrinking"></a>Shrinking</h4><p>在mapping过程中，一般是将LR feature进行map到HR feature中，因此LR feature maps的维数一般非常高。作者通过选取1x1的卷积核进行降维，从而减少后面的计算量。</p><h4 id="Non-linear-mapping"><a href="#Non-linear-mapping" class="headerlink" title="Non-linear mapping"></a>Non-linear mapping</h4><p>在SRCNN中，作者选取了一个5x5的map layer，然而5x5会带来比较大的计算量，作者将选取3x3的kernel，通过m个3x3的卷积layer进行串联。</p><h4 id="Expanding"><a href="#Expanding" class="headerlink" title="Expanding"></a>Expanding</h4><p>作者发现低维度的HR dimension带来的重建效果并不是特别好，因此作者通过1x1的卷积layer，对HR dimension进行扩维，类似于Shrink的反操作。</p><h4 id="Deconvolution"><a href="#Deconvolution" class="headerlink" title="Deconvolution"></a>Deconvolution</h4><p>转置卷积则是实行上采样操作。这个操作可以看作是conv的逆操作，因为stride=k时的cov卷积操作会将feature map缩水k倍。所以stride为k的deconv则会将feature map进行放大k倍。</p><h4 id="PRelu"><a href="#PRelu" class="headerlink" title="PRelu"></a>PRelu</h4><p>作者定义了一个激活函数PRelu activation function: \( f(x_i)=\max(x_i; 0) + a_i \min(0; x_i)\)</p><h4 id="deconvolution-structure"><a href="#deconvolution-structure" class="headerlink" title="deconvolution structure"></a>deconvolution structure</h4><blockquote><p><img src="http://sotavision.cn/showdoc/server/../Public/Uploads/2020-01-22/5e280a794129e.png" alt="" loading="lazy"></p></blockquote><hr><h3 id="Experiment"><a href="#Experiment" class="headerlink" title="Experiment"></a>Experiment</h3><h4 id="Experiments-Details"><a href="#Experiments-Details" class="headerlink" title="Experiments Details"></a>Experiments Details</h4><p>论文详细代码：<a href="http://mmlab.ie.cuhk.edu.hk/projects/FSRCNN.html" target="_blank" rel="noopener">http://mmlab.ie.cuhk.edu.hk/projects/FSRCNN.html</a><br>详细参数如下：</p><blockquote><p><img src="http://sotavision.cn/showdoc/server/../Public/Uploads/2020-01-22/5e2809f39ec40.png" alt="" loading="lazy"></p></blockquote><h4 id="parameters-of-different-settings"><a href="#parameters-of-different-settings" class="headerlink" title="parameters of different settings"></a>parameters of different settings</h4><blockquote><p><img src="http://sotavision.cn/showdoc/server/../Public/Uploads/2020-01-22/5e280aa5a295a.png" alt="" loading="lazy"></p></blockquote><h4 id="Comparisons-with-state-of-the-arts"><a href="#Comparisons-with-state-of-the-arts" class="headerlink" title="Comparisons with state-of-the-arts"></a>Comparisons with state-of-the-arts</h4><blockquote><p><img src="http://sotavision.cn/showdoc/server/../Public/Uploads/2020-01-22/5e280ade5491b.png" alt="" loading="lazy"></p></blockquote><hr><h3 id="Conclusions"><a href="#Conclusions" class="headerlink" title="Conclusions"></a>Conclusions</h3><ul><li>在观察当前基于深度学习的SR模型的局限性的同时，我们探索了一种更有效的网络结构，以实现高运行速度而不损失恢复质量</li><li>重新设计SRCNN结构，设计了漏斗结构的卷积网络，不需要预处理操作，实现了这一目标，并实现了超过40倍的最终加速</li><li>在运行时间方面更优越。该模型可用于实时视频SR，并可为其他低层次视觉任务激励快速深度模型。</li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;https://cdn.jsdelivr.net/npm/aplayer@1.10/dist/APlayer.min.css&quot;&gt;&lt;script 
      
    
    </summary>
    
    
      <category term="论文笔记" scheme="http://alexzou14.github.io/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="深度学习" scheme="http://alexzou14.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="笔记" scheme="http://alexzou14.github.io/tags/%E7%AC%94%E8%AE%B0/"/>
    
      <category term="超分辨率" scheme="http://alexzou14.github.io/tags/%E8%B6%85%E5%88%86%E8%BE%A8%E7%8E%87/"/>
    
  </entry>
  
  <entry>
    <title>Second-order Attention Network for Single Image Super-Resolution论文阅读笔记</title>
    <link href="http://alexzou14.github.io/2020/04/23/SAN/"/>
    <id>http://alexzou14.github.io/2020/04/23/SAN/</id>
    <published>2020-04-23T11:35:13.000Z</published>
    <updated>2020-04-23T11:45:08.000Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="https://cdn.jsdelivr.net/npm/aplayer@1.10/dist/APlayer.min.css"><script src="https://cdn.jsdelivr.net/npm/aplayer@1.10/dist/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="https://cdn.jsdelivr.net/npm/meting@1.2/dist/Meting.min.js"></script><h3 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h3><p>   当时超分领域现有的基于深度卷积神经网络的方法主要专注于设计更深或者更宽的网络结构，却很少挖掘层间特征的相关性，从而降低了卷积神经网络的学习能力。这篇文章就提出了一个二阶注意力网络(SAN)来实现更强大的特征表达和特征相关学习。提出了一种新的可训练的二阶通道注意力(SOCA)模块，进行相关性学习。提出了一种非局部增强的残差组(NLRG)结构，捕获远距离空间上下文信息。</p><hr><h3 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h3><p>   本文在这部分介绍了单一超分问题存在不定性，并且很多的超分方法在近段时间提出。这篇论文说明了当时基于CNN的超分方法存在的局限性：</p><ul><li>大多数基于CNN的SR方法没有充分利用原始LR图像中的信息，从而导致相对较低的性能。</li><li>大多数现有的基于CNN的SR模型主要集中在设计一个更深或更宽的网络，以学习更具鉴别性的高级特征，而很少利用中间层的特征相关性，从而阻碍CNN的性能</li></ul><p>为了解决上面的局限性，作者提出了一种二阶信道注意机制，用于更好的特征相关学习。<br>本文主要贡献有：</p><ul><li>作者提出了一种用于精确图像SR的深度二阶注意网络（SAN）。在公共数据集上的广泛实验表明作者的方法效果很棒</li><li>作者提出了二阶信道注意机制，通过考虑高于一阶的特征统计量来自适应地重新标度特征。这种SOCA机制使我们的网络能够专注于更多的信息特征，并提高区分学习能力</li><li>作者提出了非局部增强剩余群结构来构建一个深度网络，该网络进一步结合了非局部操作来捕获空间上下文信息</li></ul><hr><h3 id="Related-work"><a href="#Related-work" class="headerlink" title="Related work"></a>Related work</h3><h4 id="CNN-based-SR-methods"><a href="#CNN-based-SR-methods" class="headerlink" title="CNN-based SR methods"></a>CNN-based SR methods</h4><p>   这部分作者主要介绍了当时大部分基于CNN方法的超分算法。</p><h4 id="Attention-mechanism"><a href="#Attention-mechanism" class="headerlink" title="Attention mechanism"></a>Attention mechanism</h4><p>   这部分作者主要介绍了当时在深度学习中应用的注意机制应用的都是一阶注意机制来进行统计特征，作者通过探索特征的二阶统计量，提出了一种深度二阶注意网络（SAN）。</p><hr><h3 id="Proposed-Method"><a href="#Proposed-Method" class="headerlink" title="Proposed Method"></a>Proposed Method</h3><h4 id="Network-structure"><a href="#Network-structure" class="headerlink" title="Network structure"></a>Network structure</h4><p><img src="http://sotavision.cn/showdoc/server/../Public/Uploads/2020-02-19/5e4cf99c19319.png" alt="" loading="lazy"><br>   如图所示，该网络包括四个部分：浅层特征提取，基于非局部增强残差组（NLRG）的深度特征提取，上采样模块，重建模块<br>首先通过一个卷积层提取浅层特征信息：\(F_0=H_{SF}(I_{LR})\)<br>F0作为NLRG输入，用于提取特征信息：\(F_{DF}=H_{NLRG}(F_0)\)<br>上采样特征：\(F_{\uparrow}=H_{\uparrow}(F_{DF})\)<br>重建模块：\(I_{SR}=H_R(F_{\uparrow})=H_{SAN}(I_{LR})\)<br>损失函数：\(L_1{\;}loss\)</p><h4 id="Non-locally-Enhanced-Residual-Group-NLRG"><a href="#Non-locally-Enhanced-Residual-Group-NLRG" class="headerlink" title="Non-locally Enhanced Residual Group (NLRG)"></a>Non-locally Enhanced Residual Group (NLRG)</h4><p>NLRG包括多个区域非局部模块RL-NL和一个同源残差组结构SSRG，SSRG又由G个局部原残差注意力模块（LSRAG）和一个卷积组成，同时使用了同源残差连接结构（SSC）。每一个LSRAG包括M个简化的带有本地源的跳跃连接的残差块。第g个LSRAG可以表示为：<br>\(F_g=W_{SSC}F_0+H_g(F_{g-1})\)<br>\(W_{SSC}\)表示卷积的权重，初始化为0，然后逐渐学会赋予浅的特征更多的权重，\(H_{g}()\)表示第g个LSRAG。<br>得到深度特征表示为：\(F_{DF}=W_{SSC}F_0+F_G\)</p><ul><li><p><strong>Region-level non-local module (RL-NL)</strong><br><img src="http://sotavision.cn/showdoc/server/../Public/Uploads/2020-02-19/5e4d0b1e64e92.png" alt="" loading="lazy"><br>将图像划分为k×k大小，每一个区域进行非局部操作。<br>对于全局非局部的好处：减少来计算量，对于低级任务区域非局部操作证明有效。</p></li><li><p><strong>Local-source residual attention group (LSRAG)</strong><br><img src="http://sotavision.cn/showdoc/server/../Public/Uploads/2020-02-19/5e4d0ae31cec3.png" alt="" loading="lazy"><br>使用了局部残差连接，在LSRAG末端有一个SOCA模块，即是二阶通道注意力机制。</p></li><li><p><strong>Second-order Channel Attention (SOCA)</strong><br><img src="http://sotavision.cn/showdoc/server/../Public/Uploads/2020-02-19/5e4d0c048d3d7.png" alt="" loading="lazy"><br>这个模块通过数学变换给所得到的特征进行二阶的通道注意。</p></li></ul><hr><h3 id="Experiment"><a href="#Experiment" class="headerlink" title="Experiment"></a>Experiment</h3><h4 id="Experiments-Details"><a href="#Experiments-Details" class="headerlink" title="Experiments Details"></a>Experiments Details</h4><p>具体实验细节可以参考<a href="https://github.com/daitao/SAN" target="_blank" rel="noopener">https://github.com/daitao/SAN</a><br>####Ablation Study<br><img src="http://sotavision.cn/showdoc/server/../Public/Uploads/2020-02-19/5e4d1bff63996.png" alt="" loading="lazy"><br>由上述结果可以看出二阶通道注意机制可以比较好的提升网络性能。</p><h4 id="Results-with-different-degradation"><a href="#Results-with-different-degradation" class="headerlink" title="Results with different degradation"></a>Results with different degradation</h4><ul><li><p><strong>Results with Bicubic Degradation (BI)</strong><br><img src="http://sotavision.cn/showdoc/server/../Public/Uploads/2020-02-19/5e4d1e818166e.png" alt="" loading="lazy"></p></li><li><p><strong>Results with Blur-downscale Degradation (BD)</strong><br><img src="http://sotavision.cn/showdoc/server/../Public/Uploads/2020-02-19/5e4d1ea3eb9c0.png" alt="" loading="lazy"></p></li></ul><hr><h3 id="Conclusions"><a href="#Conclusions" class="headerlink" title="Conclusions"></a>Conclusions</h3><ul><li>提出一个深的二阶注意力网络(Second-order Attention Network,SAN)</li><li>提出了一个非局部增强残差组(Non-locally Enhanced Residual Group,NLRG) ，提取深度特征，捕获长距离空间上下文信息(long-distance spatial contextual information)</li><li>提出了一个二阶通道注意力机制(Second-order Channel Attention,SOCA)以实现特征的相关性学习</li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;https://cdn.jsdelivr.net/npm/aplayer@1.10/dist/APlayer.min.css&quot;&gt;&lt;script 
      
    
    </summary>
    
    
      <category term="论文笔记" scheme="http://alexzou14.github.io/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="深度学习" scheme="http://alexzou14.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="笔记" scheme="http://alexzou14.github.io/tags/%E7%AC%94%E8%AE%B0/"/>
    
      <category term="超分辨率" scheme="http://alexzou14.github.io/tags/%E8%B6%85%E5%88%86%E8%BE%A8%E7%8E%87/"/>
    
  </entry>
  
  <entry>
    <title>Recurrent Back-Projection Network for Video Super-Resolution论文阅读笔记</title>
    <link href="http://alexzou14.github.io/2020/04/23/RBPN/"/>
    <id>http://alexzou14.github.io/2020/04/23/RBPN/</id>
    <published>2020-04-23T10:58:42.000Z</published>
    <updated>2020-04-23T11:15:18.000Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="https://cdn.jsdelivr.net/npm/aplayer@1.10/dist/APlayer.min.css"><script src="https://cdn.jsdelivr.net/npm/aplayer@1.10/dist/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="https://cdn.jsdelivr.net/npm/meting@1.2/dist/Meting.min.js"></script><h3 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h3><p>作者提出了一个新的网络结构来解决视频超分辨率问题。作者使用一个递归的编码器-解码器模块整合来自连续视频帧的时空上下文，该模块将多帧信息与目标帧的更传统的单帧超分辨率路径相融合。递归反投影网络（RBPN）将每个上下文帧视为单独的信息源。这些数据源被组合在一个迭代优化框架中，该框架的灵感来自于多重图像超分辨率中的反投影思想。这是通过显式地表示相对于目标的估计帧间运动而不是显式地对齐帧来辅助的。作者提出了一个新的视频超分辨率基准，允许在更大的尺度上进行评估，并考虑不同运动状态下的视频。实验结果表明，我们的RBPN方法在多个数据集上都优于现有方法。</p><hr><h3 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h3><p>流行的MISR或者VSR方法一般都是基于准确的运动估计和运动补偿（alignment），这个模式受到运动估计准确度的约束。RBPN希望改进这个模式。<br>另外，在绝大多数基于CNN的方法中（包括作者之前提出的DBPN），实际上都对图像做了concat以及1 * 1卷积的操作，或者直接把图片输入到RNN中。concat的方法导致多张图片被同步地处理，增加了训练网络的难度。RNN中，同时处理细微和明显的变化（比如同一图片中运动剧烈和缓慢的物体）是比较困难的。<br>DBPN中的不停向后传递的residual，实际上能比较好地应对RNN中对subtle 和 significant changes乏力的问题。<br>应用了循环Encoder-Decoder机制，用于通过反投影合并在SISR和MISR路径中提取的细节。RBPN中的这种机制扩大了RNN中的时间gap(t,t-1)，使得时间跨度更大的帧也能被姣好地利用。<br>本文主要贡献，有以下几点创新：</p><ul><li>用了一个统一的框架整合了SISR和MISR</li><li>作者提出了一个Back-projection模块，应用了循环Encoder-Decoder机制，用于通过反投影合并在SISR和MISR路径中提取的细节。</li><li>采用了更多的数据集来评估网络。</li></ul><hr><h3 id="Related-work"><a href="#Related-work" class="headerlink" title="Related work"></a>Related work</h3><p>与DBPN论文一样，都是先总结并分类当前主流的SR做法。<br>主流的Deep VSR（video super-resolution）分为以下四种(如下图)：</p><blockquote><p><img src="http://sotavision.cn/showdoc/server/../Public/Uploads/2020-04-23/5ea15dfbb43b9.png" alt="" loading="lazy"></p></blockquote><ol><li>Temporal Concatenation：送入网络前将frames直接concat起来</li><li>Temporal Aggregation：将不同数量的帧（有些分路包含更多的邻居frame）丢进网络分路，最后输出前concat起来</li><li>RNNs：frames迭代式地进入RNN，最后输出当前这一帧的输出</li><li>本文提出的RBPN</li></ol><hr><h3 id="Proposed-Method"><a href="#Proposed-Method" class="headerlink" title="Proposed Method"></a>Proposed Method</h3><p>首先RBPN是根据当前帧以及多个邻居帧生成当前帧的SR图像（multi frame-&gt;single frame），并不是multi frame -&gt; multi frame<br>RBPN与本文的兄弟paper图像超分辨率网络DBPN思想比较相似，核心都是残差学习。DBPN是根据浅层的特征来学习残差，RBPN是根据邻居帧以及两者的optical flow来学习残差</p><h4 id="Network-structure"><a href="#Network-structure" class="headerlink" title="Network structure"></a>Network structure</h4><p>网络结构如下图所示：</p><blockquote><p><img src="http://sotavision.cn/showdoc/server/../Public/Uploads/2020-04-23/5ea15e7a93861.png" alt="" loading="lazy"></p></blockquote><p>RBPN由三个部分构成：<strong>Initial feature extraction</strong>、<strong>Multiple Projections</strong>、<strong>Reconstruction</strong>.</p><ul><li><strong>Initial Feature Extraction</strong>：</li></ul><p>对当前帧采用卷积层\(I_t\)进行特征提取，得到LR特征\(L_t\)。并concat当前帧\(I_t\)、邻居帧\(I_{t-i}\)、两者之间的optial flow 图\(F_{t-i}\),然后对concat起来的输入使用卷积层进行特征提取，得到Multi-frame特征\(M_{t-i}\)。其中\(i=1,2,…,n\)。</p><ul><li><strong>Multiple Projections</strong>：</li></ul><p>Multiple Projections 是一个Encoder-Decoder结构的Module。Encoder负责上采样，Decoder负责下采样。</p><blockquote><p><img src="http://sotavision.cn/showdoc/server/../Public/Uploads/2020-04-23/5ea161d9efc1e.png" alt="" loading="lazy"></p></blockquote><p>Encoder：SISR和MISR两支路的特征输入到Encoder后，经由上采样得到高分的特征张量\(H_{t-n}^m\)和\(H_{t-n-1}^l\)，求得残差后，与SISR支路的高分特征张量\(H_{t-n-1}^l\)相加，得到Encoder的输出，一个高分的特征张量\(H_{t-n}\)<br>Decoder：Encoder的输出\(H_{t-n}\)通过残差块（即有shortcut的几个卷积层）下采样后，得到低分的张量\(L_{t-n}\)</p><blockquote><p>具体公式如下：<br>Encoder:\(H_{t-n}=Net_{E}(L_{t-n-1},M_{t-n}; \theta _ E)\)<br>Decoder:\(L_{t-n}=Net_{D}(H_{t-n};\theta_D)\)<br>其中Encoder里：<br>SISR upscale:\(H_{t-n-1}^l=Net_{sisr}(L_{t-n-1};\theta_{sisr})\)<br>MISR upscale:\(H_{t-n}^m=Net_{misr}(M_{t-n};\theta_{misr})\)<br>Residual:\(e_{t-n}=Net_{res}(H_{t-n-1}^l-H_{t-n}^m;\theta_{res})\)<br>Output: \(H_{t-n}=H_{t-n-1}^l+e_{t-n}\)</p></blockquote><ul><li><strong>Reconstruction</strong>:</li></ul><p>把Projection Module中Encoder每次迭代中输出的HR特征全部concat起来，经过卷积层得出最终的SR图像。公式为：\(SR_t=f_{rec}([H_{t-1},H_{t-2},…,H_{t-n}])\)</p><h4 id="Loss-function"><a href="#Loss-function" class="headerlink" title="Loss function"></a>Loss function</h4><p>作者在这里采用了L1的loss function。</p><hr><h3 id="Experiment"><a href="#Experiment" class="headerlink" title="Experiment"></a>Experiment</h3><h4 id="Experiments-Details"><a href="#Experiments-Details" class="headerlink" title="Experiments Details"></a>Experiments Details</h4><p>可以参考官方代码：<a href="https://github.com/alterzero/RBPN-PyTorch" target="_blank" rel="noopener">https://github.com/alterzero/RBPN-PyTorch</a></p><h4 id="Network-analysis"><a href="#Network-analysis" class="headerlink" title="Network analysis"></a>Network analysis</h4><ul><li>分析采用多少帧过去的帧进行SR重建<blockquote><p><img src="http://sotavision.cn/showdoc/server/../Public/Uploads/2020-04-23/5ea1691cbd39a.png" alt="" loading="lazy"></p></blockquote></li></ul><p>可以看出，显然用更多帧进行预测时效果更好。</p><h4 id="Comparisons-with-state-of-the-arts"><a href="#Comparisons-with-state-of-the-arts" class="headerlink" title="Comparisons with state-of-the-arts"></a>Comparisons with state-of-the-arts</h4><blockquote><p><img src="http://sotavision.cn/showdoc/server/../Public/Uploads/2020-04-23/5ea1699bbf60e.png" alt="" loading="lazy"><br><img src="http://sotavision.cn/showdoc/server/../Public/Uploads/2020-04-23/5ea169abdd151.png" alt="" loading="lazy"></p></blockquote><hr><h3 id="Conclusions"><a href="#Conclusions" class="headerlink" title="Conclusions"></a>Conclusions</h3><ul><li>作者提出一个新奇的视频超分方法名为RBPN的方法整合了SISR和MISR方法</li><li>作者提出了一个Back-projection模块，应用了循环Encoder-Decoder机制，用于通过反投影合并在SISR和MISR路径中提取的细节。</li><li>作者提出了一种新的视频SR评估协议，该协议允许根据输入视频中的运动量来区分视频SR的性能。</li><li>在广泛的实验中，作者评估了各种设计选择对我们的方法的最终性能所起的作用，并证明RBPN比现有的VSR方法获得了显著的更好的性能。</li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;https://cdn.jsdelivr.net/npm/aplayer@1.10/dist/APlayer.min.css&quot;&gt;&lt;script 
      
    
    </summary>
    
    
      <category term="论文笔记" scheme="http://alexzou14.github.io/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="深度学习" scheme="http://alexzou14.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="笔记" scheme="http://alexzou14.github.io/tags/%E7%AC%94%E8%AE%B0/"/>
    
      <category term="超分辨率" scheme="http://alexzou14.github.io/tags/%E8%B6%85%E5%88%86%E8%BE%A8%E7%8E%87/"/>
    
  </entry>
  
  <entry>
    <title>Toward Real-World Single Image Super-Resolution:A New Benchmark and A New Model论文阅读笔记</title>
    <link href="http://alexzou14.github.io/2020/04/11/RealSR/"/>
    <id>http://alexzou14.github.io/2020/04/11/RealSR/</id>
    <published>2020-04-11T10:34:11.000Z</published>
    <updated>2020-04-11T10:41:05.000Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="https://cdn.jsdelivr.net/npm/aplayer@1.10/dist/APlayer.min.css"><script src="https://cdn.jsdelivr.net/npm/aplayer@1.10/dist/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="https://cdn.jsdelivr.net/npm/meting@1.2/dist/Meting.min.js"></script><h3 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h3><p>目前大多数基于学习的单图像超分辨率算法（Single Image Super Resolution,SISR）大多是基于模拟数据集（例如：对高分辨率图像（HR）进行Bicubic 降采样，或者加入高斯白噪声）。然而，真实低分辨率图像（LR）的降质过程往往是非常复杂而且不可知，因此在模拟数据集上训练的 SISR 算法在真实场景下往往效果不佳。<br>本文介绍了自己的数据集来更真实的还原现实世界的场景，提出了一个基于拉普拉斯的核预测网络。</p><hr><h3 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h3><p>作者在这部分先说明了单一图像超分问题的不定性。现在的大多数方法都用模拟数据集来进行训练，所以现在大部分方法应用在现实场景中收到的效果不是特别好。作者通过调节焦距，在进行图像配对获得一组现实场景中的数据集。作者同时提出了一种Lap-KPN网络在现实场景中取得了比较好的结果。<br>本文主要贡献有：</p><ul><li><p>该论文提出了一个新的 Benchmark 数据集 RealSR dataset，它包含同场景下成对的 LR-HR 数据集，由变焦数码进行拍摄经过后期处理得到</p></li><li><p>由于真实图像降质核在数据集中并非完全一致，因此该论文提出了一个新的超分辨率算法LP-KPN：基于拉普拉斯金字塔的核预测网络。它能够有效地学习 per-pixel kernel（像素卷积核） 用于高分辨率图像的重建。</p></li></ul><hr><h3 id="Related-work"><a href="#Related-work" class="headerlink" title="Related work"></a>Related work</h3><h4 id="SISR-dataset"><a href="#SISR-dataset" class="headerlink" title="SISR dataset"></a>SISR dataset</h4><p>比较了当时使用比较广泛的数据集，如Set5、Set14、BSD300、Urban100、DIV2K等数据集。当下大多数网络训练过程中的LR图像都是通过对高分辨率图像（HR）进行Bicubic 降采样，或者加入高斯白噪声得到的。然后这些SISR模型在真实场景上恢复出来的HR图像比较差。</p><h4 id="Kernel-prediction-network"><a href="#Kernel-prediction-network" class="headerlink" title="Kernel prediction network"></a>Kernel prediction network</h4><p>作者介绍了KPN网络一开始是用来去噪的一个网络，由于现实场景下的低分辨率图像的模糊核是多样的，作者运用了相似的思想来恢复低分辨率图像。</p><hr><h3 id="Real-world-SISR-Dataset"><a href="#Real-world-SISR-Dataset" class="headerlink" title="Real-world SISR Dataset"></a>Real-world SISR Dataset</h3><h4 id="Image-formation-by-thin-lens"><a href="#Image-formation-by-thin-lens" class="headerlink" title="Image formation by thin lens"></a>Image formation by thin lens</h4><p>图像在相机生成的光学过程如下：<br><img src="http://sotavision.cn/showdoc/server/../Public/Uploads/2020-02-19/5e4d373365283.png" alt="" loading="lazy"><br>所以光学成像的表达式为：\({\dfrac{1}{f}}={\dfrac{1}{u}}+{\dfrac{1}{v}}\)<br>所以放大倍数可以表达为：\(M={\dfrac{h_2}{h_1}}={\dfrac{v}{u}}\)<br>所以当\(u \gg f\)时我们可以得到：\( h_2={\dfrac{f}{u-f}}\approx{\dfrac{f}{u}h} \)<br>所以可以通过调节焦距来获得到LR和HR图像</p><h4 id="Data-collection"><a href="#Data-collection" class="headerlink" title="Data collection"></a>Data collection</h4><p>使用了以上的相机来获得到图像：<br><img src="http://sotavision.cn/showdoc/server/../Public/Uploads/2020-02-19/5e4d3a520c86d.png" alt="" loading="lazy"></p><h4 id="Image-pair-registration"><a href="#Image-pair-registration" class="headerlink" title="Image pair registration"></a>Image pair registration</h4><p><img src="http://sotavision.cn/showdoc/server/../Public/Uploads/2020-02-19/5e4d3a84a9083.png" alt="" loading="lazy"><br>作者通过以上的对齐方式将获得到的图像整理成LR-HR的图像对制作成一个现实世界的训练集。</p><h3 id="Laplacian-Pyramid-based-Kernel-Prediction-Network"><a href="#Laplacian-Pyramid-based-Kernel-Prediction-Network" class="headerlink" title="Laplacian Pyramid based Kernel Prediction Network"></a>Laplacian Pyramid based Kernel Prediction Network</h3><p><img src="http://sotavision.cn/showdoc/server/../Public/Uploads/2020-02-19/5e4d3b89284c2.png" alt="" loading="lazy"><br>从图 1 看，网络一共分为2个部分：1.Backbone骨干网络，用于特征提取； 2.Per-pixel kernels at different level，处理不同拉普拉斯层级的解构图像，共有3层。对于每一个层，层级中的卷积对 Backbone 输出特征进行不同尺度缩放，然后提取特征输出 per-pixel kernel 特征矩阵，维度大小为\( H\times W\times (k\times k)\),其中\((k\times k)\)输出的卷积核尺寸。然后与对应的拉普拉斯金字塔解构图像进行如下的内积矩阵操作：<br>\[ I_H^P(i,j)=\left \langle {K(i,j),V(I_L^A(i,j))} \right \rangle \]<br>其中\( K(i,j) \)为per-pixel kernel 特征矩阵中\((i,j)\)位置的卷积核，\(V(I_L^A(i,j))\)为输入解构图像中，以\((i,j)\)位置的点位中心,\(k\times k\)大小的邻域。对于输入的每一个像素点，都有对应的卷积核单独对其邻域内的特征进行处理，所以 LP-KPN能够较好地应用于图像去噪，去模糊、超分辨率等任务，且效果较好。<br>LP-KPN 还利用 shuffle downsampling 和 shuffle upsampling，实现卷积特征空间分辨率上采样和下采样，其原理与 ESPCN 一致。在网络最前端进行4倍下采样，有效缓解了 输入图像尺寸与目标图像一致 而导致的计算消耗大的问题。 LP-KPN 输入图像为灰度图像，通过 shuffle downsampling 操作后，相当于将图像降采样成多张低分辨的灰度图像。</p><hr><h3 id="Experiment"><a href="#Experiment" class="headerlink" title="Experiment"></a>Experiment</h3><h4 id="Experiments-Details"><a href="#Experiments-Details" class="headerlink" title="Experiments Details"></a>Experiments Details</h4><p>具体实验细节可以参考：<a href="https://github.com/csjcai/RealSR" target="_blank" rel="noopener">https://github.com/csjcai/RealSR</a></p><h4 id="SISR-models-trained-on-RealSR-dataset"><a href="#SISR-models-trained-on-RealSR-dataset" class="headerlink" title="SISR models trained on RealSR dataset"></a>SISR models trained on RealSR dataset</h4><p>论文对比了不同的 SISR 模型在不同的数据集上进行训练，最终在真实图像测试集上进行测试的结果。可以看到在 RealSR 数据集上，相同模型能够生成视觉效果更好的图像，图像中的伪影和噪声更少，且边缘纹理更清晰。这说明了RealSR 数据集的有效性。<br><img src="http://sotavision.cn/showdoc/server/../Public/Uploads/2020-02-19/5e4d410bc9e0d.png" alt="" loading="lazy"></p><h4 id="Comparisons-with-state-of-the-arts"><a href="#Comparisons-with-state-of-the-arts" class="headerlink" title="Comparisons with state-of-the-arts"></a>Comparisons with state-of-the-arts</h4><p>论文对 LP-KPN方法的有效性进行了验证，LP-KPN 可以取得比 RCAN 相近的性能表现。<br><img src="http://sotavision.cn/showdoc/server/../Public/Uploads/2020-02-19/5e4d415614fdd.png" alt="" loading="lazy"></p><hr><h3 id="Conclusions"><a href="#Conclusions" class="headerlink" title="Conclusions"></a>Conclusions</h3><ul><li><p>该论文提出了一个新的 Benchmark 数据集 RealSR dataset，它包含同场景下成对的 LR-HR 数据集，由变焦数码进行拍摄经过后期处理得到</p></li><li><p>由于真实图像降质核在数据集中并非完全一致，因此该论文提出了一个新的超分辨率算法LP-KPN：基于拉普拉斯金字塔的核预测网络。它能够有效地学习 per-pixel kernel（像素卷积核） 用于高分辨率图像的重建。</p></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;https://cdn.jsdelivr.net/npm/aplayer@1.10/dist/APlayer.min.css&quot;&gt;&lt;script 
      
    
    </summary>
    
    
      <category term="论文笔记" scheme="http://alexzou14.github.io/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="深度学习" scheme="http://alexzou14.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="笔记" scheme="http://alexzou14.github.io/tags/%E7%AC%94%E8%AE%B0/"/>
    
      <category term="超分辨率" scheme="http://alexzou14.github.io/tags/%E8%B6%85%E5%88%86%E8%BE%A8%E7%8E%87/"/>
    
  </entry>
  
  <entry>
    <title>Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network论文阅读笔记</title>
    <link href="http://alexzou14.github.io/2020/04/11/SRGAN/"/>
    <id>http://alexzou14.github.io/2020/04/11/SRGAN/</id>
    <published>2020-04-11T10:04:44.000Z</published>
    <updated>2020-04-11T10:34:10.000Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="https://cdn.jsdelivr.net/npm/aplayer@1.10/dist/APlayer.min.css"><script src="https://cdn.jsdelivr.net/npm/aplayer@1.10/dist/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="https://cdn.jsdelivr.net/npm/meting@1.2/dist/Meting.min.js"></script><h3 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h3><p>  在超分辨率领域飞速发展的时候，仍然有一个核心问题没有解决：当我们在用大的上采样因子实现图像恢复时，我们如何恢复更精细的纹理细节？基于优化的超分辨率方法的行为主要是由目标函数的选择驱动的。最近的工作主要集中在最小化方均根误差（MSE）上。由此产生的估计具有高峰值信噪比（PSNR），但它们通常缺乏高频细节，并且在感觉上不能令人满意。<br>这篇文章提出了以下的方法来处理这一问题：</p><ul><li>更改损失函数，将传统的MSE换成：对抗损失（perceptual loss）和内容损失（content loss）</li><li>引入对抗生成网络，将传统的像素空间的Content Loss转换为对抗性质的相似性。</li><li>引入深度残差网络，来提取图片中的细节纹理。</li></ul><hr><h3 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h3><p>SR问题有一个现状：就是尤其在大的采样因子的条件下，重构的图像通常都缺少细节纹理。因为他们是给予像素上的差异来优化。<br>SRGAN实现思路：</p><ul><li>采用具有skip connection的深度残差网络（ResNet），MSE作为content loss</li><li>采用VGG网络提取特征，并与Discriminator结合，来定义新的perceptual loss</li></ul><hr><h3 id="Related-work"><a href="#Related-work" class="headerlink" title="Related work"></a>Related work</h3><h4 id="Image-super-resolution"><a href="#Image-super-resolution" class="headerlink" title="Image super-resolution"></a>Image super-resolution</h4><p>基于预测的恢复方法：（插值）比如说线性差值，双三次差值，Lanczos滤波（注：其实就是基于插值的上采样方法），速度非常快，但是会产生关于平滑的图像，导致纹理边缘信息丢失。更加强大的方法是想要建立低分辨率图像到高分辨率图像的复杂映射，并且依赖于训练数据：</p><ul><li>利用图像中不同尺度的补丁来驱动SR。</li><li>自我字典通过延伸来进行小变换和形状变化。</li><li>卷积稀疏编码方法，通过处理整个图像而不是重叠补丁来提高一致性。</li></ul><p>为了重建真实的纹理细节，同时避免过度伪影，Tai等人将基于先验的梯度曲线的边缘定向SR算法与基于学习的细节合成的益处相结合。提出了一种多尺度字典来捕获不同尺度的相似图像块的冗余。<br>邻域嵌入方法通过在低维流形中找到类似的LR训练补片并将它们相应的HR补片组合用于重建来上采样LRimage补片。</p><h4 id="Design-of-convolutional-neural-networks"><a href="#Design-of-convolutional-neural-networks" class="headerlink" title="Design of convolutional neural networks"></a>Design of convolutional neural networks</h4><p>卷积神经网络的引入给SR问题带来了新的方法：</p><ul><li>SRCNN将稀疏表示编码到神经网络架构中，使用双三次插值来放大输入图像并对端到端的三层深度卷积网络进行训练，以实现最先进的SR性能。</li><li>随后，表明使网络能够直接学习上采样滤波器可以进一步提高准确度和速度的性能。凭借其深度递归卷积网络（DRCN），Kim等人提出了一种高性能的架构，允许长距离像素依赖，同时保持模型参数的数量很少。</li><li>Johnson等人的着作，和布鲁纳等人。 依靠更接近感知相似性的损失函数来恢复视觉上更有说服力的HR图像。</li></ul><h4 id="Contribution"><a href="#Contribution" class="headerlink" title="Contribution"></a>Contribution</h4><ul><li>使用ResNet优化了过去的MSE损失。</li><li>提出SRGAN，这是一个基于GAN的网络，为新的感知损失进行了优化。在这里，我们用基于VGG网络的特征映射的损失计算替代基于MSE的内容丢失[49]，这对于像素空间的变化更加不变。</li><li>通过对来自三个公共基准数据集的图像进行广泛的平均意见得分（MOS）测试来证实，SRGAN是一种新的技术水平，通过大幅度的边缘，用于估计具有高放大因子的照片般逼真的重构图像。</li></ul><hr><h3 id="Proposed-Method"><a href="#Proposed-Method" class="headerlink" title="Proposed Method"></a>Proposed Method</h3><h4 id="Network-structure"><a href="#Network-structure" class="headerlink" title="Network structure"></a>Network structure</h4><p>网络结构如下图所示：</p><blockquote><p><img src="http://sotavision.cn/showdoc/server/../Public/Uploads/2020-02-24/5e5377a6492d0.png" alt="" loading="lazy"></p></blockquote><p>这里生成网络运用了SRResNet，对抗网络用了VGG网络。<br>生成网络优化的参数表达式为：</p><div>    \[ \hat{\theta}_G=\displaystyle\argmin_{\theta_G} \frac{1}{N} \sum_{n=1}^{N} l^{SR}(G_{\theta_G}(I_n^{LR}),I_n^{HR}) \]</div><p>其中\(\hat{\theta}_G\)是指生成网络的参数，\({I^{LR}}\)是指\({I^{HR}}\)通过高斯滤波器降置得到的。<br>我们进一步定义了一个鉴别器网络\(D_{\theta_D}\)，我们与\(G_{\theta_G}\)交替优化，以解决以下问题：</p><div>    \[ \min_{G_{\theta_G}}\max_{D_{\theta_D}}{E_{I^{HR}\sim {p_{train}(I^{HR})}}}[\log{D_{\theta_D}}(I^{HR})]+{E_{I^{LR}\sim {p_{G}(I^{LR})}}}[\log(1-{D_{\theta_D}}(G_{\theta_G}(I^{LR})))] \]</div><p>这里就相当于让LR图像经过生成网络得到一张SR图像，用SR图像进过鉴别器来判别这个图像是否和HR图像接近，如果否则仅需通过生成器生成图像，如果是则输出SR图像。所以这里最小化生成器的损失函数，最大化判别器的损失函数。</p><h4 id="Loss-function"><a href="#Loss-function" class="headerlink" title="Loss function"></a>Loss function</h4><ul><li>Perceptual loss function</li></ul><blockquote><p>\[ l^{SR}=l_X^{SR}+10^{-3}l_{Gen}^{SR} \]<br>其中\(l_X^{SR}\)是内容损失函数，\(l_{Gen}^{SR}\)是对抗损失函数。这里是用上面的损失函数来使得生成器的生成的图片接近原图</p></blockquote><ul><li>Content loss</li></ul><blockquote><p>这里可以使用两个内容损失函数：<br>MSE loss: </p></blockquote><div>    \[l_{MSE}^{SR}=\dfrac{1}{r^2WH}\displaystyle\sum_{x=1}^{rW}\sum_{y=1}^{rH}(I_{x,y}^{HR}-G_{\theta_G}(I^{LR})_{x,y})^2\]</div><p>VGG loss:</p><div>    \[l_{VGG/i,j}^{SR}=\dfrac{1}{W_{i,j}H_{i,j}}\displaystyle\sum_{x=1}^{W_{i,j}}\sum_{y=1}^{H_{i,j}}(\phi_{i,j}(I^{HR})_{x,y}-\phi_{i,j}(G_{\theta_G}(I^{LR}))_{x,y})^2\]</div><ul><li>Adversarial loss</li></ul><blockquote><p>\[l_{Gen}^{SR}=\displaystyle\sum_{n=1}^{N}-\log{D_{\theta_D}(G_{\theta_G}(I^{LR}))} \]<br>这里用优化上面的函数来替代之前提出的对抗损失函数来优化对抗网络，使鉴别器的鉴别的效果最大化</p></blockquote><hr><h3 id="Experiment"><a href="#Experiment" class="headerlink" title="Experiment"></a>Experiment</h3><h4 id="Network-analysis"><a href="#Network-analysis" class="headerlink" title="Network analysis"></a>Network analysis</h4><h5 id="Investigation-of-content-loss"><a href="#Investigation-of-content-loss" class="headerlink" title="Investigation of content loss"></a>Investigation of content loss</h5><blockquote><p><img src="http://sotavision.cn/showdoc/server/../Public/Uploads/2020-02-24/5e539ef6a1f4f.png" alt="" loading="lazy"></p></blockquote><p>SRGAN网络可以极大的提升生成图像的感知质量，都可以取得较高的MOS分数。</p><h4 id="Performance-PSNR-time-vs-network-depth"><a href="#Performance-PSNR-time-vs-network-depth" class="headerlink" title="Performance (PSNR/time) vs. network depth"></a>Performance (PSNR/time) vs. network depth</h4><p>文中研究了网络深度和PSNR结果值及网络预测时间的关系,同时还实验了skip-connection的影响,其中网络的深度是通过调整残差快的数量来实现的,注意网络最终选取的数量为16.结果图如下:</p><blockquote><p><img src="http://sotavision.cn/showdoc/server/../Public/Uploads/2020-02-24/5e53a181eea4f.png" alt="" loading="lazy"></p></blockquote><ul><li>注意实验中以PSNR值来衡量生成图像质量,从图中可以看出随着深度增加PSNR值逐渐增大,但是增加速度逐渐变慢,这也证实了增加网络深度可以提升生成图像质量.</li><li>左图中的蓝线与红线的差别就是有无skip-connection,这也证实了其必要性.</li><li>预测时间随深度增加线性增长,skip-connection并不影响预测时间.</li></ul><h4 id="Mean-opinion-score-MOS-testing"><a href="#Mean-opinion-score-MOS-testing" class="headerlink" title="Mean opinion score (MOS) testing"></a>Mean opinion score (MOS) testing</h4><p>文中在Set5,Set14和BSD100三种数据集上对各种方法找人类受试者进行了打分,其中分值越大表示效果越好.结果如下:</p><ul><li>MOS:平均主观得分,此标准更符合人类的感知</li><li>Set5数据集分辨率较低,在做放大之后效果区分并不明显,不过从分辨率最高的BSD100数据集上的结果可以明显看出SRGAN的效果最好.<blockquote><p><img src="http://sotavision.cn/showdoc/server/../Public/Uploads/2020-02-24/5e53a27883a8e.png" alt="" loading="lazy"></p></blockquote></li></ul><h4 id="Comparisons-with-state-of-the-arts"><a href="#Comparisons-with-state-of-the-arts" class="headerlink" title="Comparisons with state-of-the-arts"></a>Comparisons with state-of-the-arts</h4><blockquote><p><img src="http://sotavision.cn/showdoc/server/../Public/Uploads/2020-02-24/5e53a2b980f5c.png" alt="" loading="lazy"></p></blockquote><p>由上图可以得知，PSNR和SSIM数值虽然不是最高，但是效果也不差，平均意见分数在所有方法中获得的最高。</p><hr><h3 id="Conclusions"><a href="#Conclusions" class="headerlink" title="Conclusions"></a>Conclusions</h3><ul><li>使用ResNet优化了过去的MSE损失。</li><li>提出SRGAN，这是一个基于GAN的网络，为新的感知损失进行了优化。在这里，我们用基于VGG网络的特征映射的损失计算替代基于MSE的内容丢失[49]，这对于像素空间的变化更加不变。</li><li>通过对来自三个公共基准数据集的图像进行广泛的平均意见得分（MOS）测试来证实，SRGAN是一种新的技术水平，通过大幅度的边缘，用于估计具有高放大因子的照片般逼真的重构图像。</li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;https://cdn.jsdelivr.net/npm/aplayer@1.10/dist/APlayer.min.css&quot;&gt;&lt;script 
      
    
    </summary>
    
    
      <category term="论文笔记" scheme="http://alexzou14.github.io/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="深度学习" scheme="http://alexzou14.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="笔记" scheme="http://alexzou14.github.io/tags/%E7%AC%94%E8%AE%B0/"/>
    
      <category term="超分辨率" scheme="http://alexzou14.github.io/tags/%E8%B6%85%E5%88%86%E8%BE%A8%E7%8E%87/"/>
    
  </entry>
  
  <entry>
    <title>Accurate Image Super-Resolution Using Very Deep Convolutional Networks论文阅读笔记</title>
    <link href="http://alexzou14.github.io/2020/04/11/VDSR/"/>
    <id>http://alexzou14.github.io/2020/04/11/VDSR/</id>
    <published>2020-04-11T10:01:01.000Z</published>
    <updated>2020-04-11T10:04:04.000Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="https://cdn.jsdelivr.net/npm/aplayer@1.10/dist/APlayer.min.css"><script src="https://cdn.jsdelivr.net/npm/aplayer@1.10/dist/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="https://cdn.jsdelivr.net/npm/meting@1.2/dist/Meting.min.js"></script><h3 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h3><p>作者提出了一个高精度的单一图像超分方法，该方法是受到VGG网络的启发得到的。作者利用多次级联小尺寸滤波器得到的一个深层次网络（VDSR），虽然网络很深，但是收敛速度很快。这是因为作者采用了比较的学习率和残差学习的方法。作者提出的这个VDSR比现有的方法有很大提升。</p><hr><h3 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h3><p>作者在简介部分先是回顾了单一图像超分现有工作，指出SRCNN是首先使用CNN网络解决SISR问题的方法，并取得了卓越的性能效果。但是，SRCNN存在3点局限性。<br>1.依赖较小的图像区域的内容<br>2.网络训练收敛较慢<br>3.网络只能解决单一尺度的图像超分辨率</p><p>VDSR提出新的方法来解决这些局限性：<br>1.深度网络使用大的感受野来获取图像上下文信息<br>2.网络进行残差学习，并且使用大的学习率，提高收敛速度。采用大学习率，容易遇到梯度爆炸的问题，这里使用适度的梯度裁剪来抑制梯度问题的产生。<br>3.神经网络可以针对不同尺度进行图像超分辨率</p><p>最终的结果是，VDSR使用较SRCNN更深的网络，效果更好，收敛速度更快，并且可用于不同尺度的超分辨</p><hr><h3 id="Related-work"><a href="#Related-work" class="headerlink" title="Related work"></a>Related work</h3><h4 id="Convolutional-Network-for-Image-Super-Resolution"><a href="#Convolutional-Network-for-Image-Super-Resolution" class="headerlink" title="Convolutional Network for Image Super-Resolution"></a>Convolutional Network for Image Super-Resolution</h4><p>从三个方面详细分析性论文中提出的模型与SRCNN的不同之处，  model， train， scale<br><strong>model</strong>:<br>        SRCNN在实验中发现并认为越深的网络得不到更好的结果。<br>        VDSR在试验中成功使用20层网络用于重建信息并取得了优于SRCNN的结果。</p><p><strong>train</strong>:<br>分析SRCNN训练速度慢的原因，并提出VDSR的改进之处</p><p>论文中分析认为：SR在HR空间建模，HR 图片可以分解为高频信息和低频信息，输入和输出的图片享有相同的低频信息，SRCNN 把输入传递到末端，构建残差，这与自动编码的概念类似，在自动编码上会消耗训练时间，论文中提出直接对残差进行建模，加快收敛速度。（对SRCNN收敛速度的分析有点牵强） 主要是提出了论文的基于残差建模</p><p><strong>scale</strong>:<br>SRCNN是一个针对单一因子进行训练的，并且只能在指定的尺度下工作，本文提出了一个单一网络来有效的处理多个规模的SR问题</p><hr><h3 id="Proposed-Method"><a href="#Proposed-Method" class="headerlink" title="Proposed Method"></a>Proposed Method</h3><h4 id="Network-structure"><a href="#Network-structure" class="headerlink" title="Network structure"></a>Network structure</h4><p>除了输入输出层，每个中间层都有64个3*3*64的过滤器，每个过滤器卷积核大小3*3，通道数64，第一层是输入层，最后一层是输出层，仅使用一个3*3*64的过滤器用于图像重构，中间层使用padding技术，使每一层产生的feature map尺寸相同<br>结构如下图：</p><blockquote><p><img src="http://sotavision.cn/showdoc/server/../Public/Uploads/2020-04-11/5e918fb0c0ab5.png" alt="" loading="lazy"></p></blockquote><h4 id="Training"><a href="#Training" class="headerlink" title="Training"></a>Training</h4><ol><li><p>残差学习：训练过程中，网络学习到的是ground-truth与经过插值之后的低分辨率率图像ILR之间的残差。最后的输出则为输入图像与网络预测的残差图像之和。</p></li><li><p>更大的学习率：训练过程中，设置大学习率，可以提高收敛速度，但是容易产生梯度消失或梯度爆炸问题。</p></li><li><p>可调节的梯度裁剪：梯度裁剪方法通常会用在训练RNN网络过程中，但直接用于CNN训练会有一定限制，通常的梯度裁剪策略是把梯度预先限制在\([-\theta,\theta]\)范围内，为了最大化提高收敛速度，本文使用的梯度裁剪策略为：\([-\dfrac{\theta}{\gamma},\dfrac{\theta}{\gamma}]\)，其中\(\gamma\)为虚席率</p></li><li><p>多尺度训练：不同尺度的网络，有很多参数是可以公用的，这里训练了单一网络可以解决不同尺度的图像超分辨问题。</p></li></ol><hr><h3 id="Experiment"><a href="#Experiment" class="headerlink" title="Experiment"></a>Experiment</h3><h4 id="Understanding-Properties"><a href="#Understanding-Properties" class="headerlink" title="Understanding Properties"></a>Understanding Properties</h4><p>从三个方面，结合训练数据分析模型的优越性</p><h5 id="The-Deeper-the-Better"><a href="#The-Deeper-the-Better" class="headerlink" title="The Deeper, the Better"></a>The Deeper, the Better</h5><blockquote><p><img src="http://sotavision.cn/showdoc/server/../Public/Uploads/2020-04-11/5e9192edcb6d4.png" alt="" loading="lazy"></p></blockquote><h5 id="Residual-Learning"><a href="#Residual-Learning" class="headerlink" title="Residual-Learning"></a>Residual-Learning</h5><p>对残差网络，非残差网络，插值  的收敛速度进行对比（用学习epoch和PSNR的关系来看收敛速度）</p><blockquote><p><img src="http://sotavision.cn/showdoc/server/../Public/Uploads/2020-04-11/5e9193125d228.png" alt="" loading="lazy"></p></blockquote><h5 id="Single-Model-for-Multiple-Scales"><a href="#Single-Model-for-Multiple-Scales" class="headerlink" title="Single Model for Multiple Scales"></a>Single Model for Multiple Scales</h5><blockquote><p><img src="http://sotavision.cn/showdoc/server/../Public/Uploads/2020-04-11/5e9193501bac2.png" alt="" loading="lazy"></p></blockquote><h4 id="Comparisons-with-state-of-the-arts"><a href="#Comparisons-with-state-of-the-arts" class="headerlink" title="Comparisons with state-of-the-arts"></a>Comparisons with state-of-the-arts</h4><blockquote><p><img src="http://sotavision.cn/showdoc/server/../Public/Uploads/2020-04-11/5e91937366775.png" alt="" loading="lazy"></p></blockquote><hr><h3 id="Conclusions"><a href="#Conclusions" class="headerlink" title="Conclusions"></a>Conclusions</h3><ol><li>提出了一个更深的网络，并且证明了网络越深效果越好。由于更深的网络结构使得后面的网络层拥有更大的感受野，该文章采取3X3的卷积核，从而使得深度为D的网络，拥有(2D+1)X(2D+1)的感受野，从而可以根据更多的像素点去推断结果像素点。</li><li>加深的网络结构为梯度传输带来了困难，采用残差学习，提高学习率，加快了收敛速度，同时采用调整梯度裁剪</li><li>数据混合:将不同大小倍数的图像混合在一起训练，从而支持不同倍数的高清化</li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;https://cdn.jsdelivr.net/npm/aplayer@1.10/dist/APlayer.min.css&quot;&gt;&lt;script 
      
    
    </summary>
    
    
      <category term="论文笔记" scheme="http://alexzou14.github.io/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="深度学习" scheme="http://alexzou14.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="笔记" scheme="http://alexzou14.github.io/tags/%E7%AC%94%E8%AE%B0/"/>
    
      <category term="超分辨率" scheme="http://alexzou14.github.io/tags/%E8%B6%85%E5%88%86%E8%BE%A8%E7%8E%87/"/>
    
  </entry>
  
  <entry>
    <title>Embedded Block Residual Network:A Recursive Restoration Model for Single-Image Super-Resolution 论文阅读笔记</title>
    <link href="http://alexzou14.github.io/2020/04/10/EBRN/"/>
    <id>http://alexzou14.github.io/2020/04/10/EBRN/</id>
    <published>2020-04-10T13:27:05.000Z</published>
    <updated>2020-04-10T13:31:45.000Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="https://cdn.jsdelivr.net/npm/aplayer@1.10/dist/APlayer.min.css"><script src="https://cdn.jsdelivr.net/npm/aplayer@1.10/dist/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="https://cdn.jsdelivr.net/npm/meting@1.2/dist/Meting.min.js"></script><h3 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h3><p>当时的单一图像超分领域中，比较好的方法都是通过比较复杂的卷积网络或者递归神经网络来得到。但是，这些方法都试图用一个模型去恢复图像中的所有结构和纹理，这样就会在处理高频细节的时忽略低频纹理。所以，图片的低频和高频信息的复杂程度是不一样的，因此在对这两部分信息进行恢复的时候，应该使用具有不同复杂度的模型或者是不同深度的网络结构。本文就提出了一种EBRN，不同模块储存不同的频率信息，浅层网络处理低频信息，深层网络处理高频信息。</p><hr><h3 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h3><p>由于单一图像超分辨的问题存在不定性，所以单一图像超分辨率领域在近些年飞速发展，有很多的超分方法被提出，尤其是近几年学习方法在超分领域的应用，大多数方法都是在致力于训练更深更大的网络来恢复图像，这样的网络在设计中参数数量增大连接过程更加巧妙，需要更多的trick来处理这个网络。可是图片的低频和高频信息的复杂程度是不一样的，因此在对这两部分信息进行恢复的时候，应该使用具有不同复杂度的模型或者是不同深度的网络结构。如果对这两部分信息使用同样的网络结构或者模型进行恢复的话，会出现如下情形：</p><blockquote><p><img src="http://sotavision.cn/showdoc/server/../Public/Uploads/2020-03-04/5e5f169065599.png" alt="" loading="lazy"></p></blockquote><p>即用浅层的网络能够很好地拟合对低频信息进行恢复所需的函数，但是高频信息由于函数更为复杂，低层的网络结构的学习能力有限，对于高频信息是欠学习的(underfit)；而用深层的网络能够很好地拟合对高频信息进行恢复所需的函数，但是对于低频信息而言，属于过拟合(overfit)。所以不同深度网络适应不同频率的信息。<br>本文主要贡献有:</p><ul><li>图片的低频和高频信息的复杂程度是不一样的，因此在对这两部分信息进行恢复的时候，应该使用具有不同复杂度的模型或者是不同深度的网络结构</li><li>提出了一种块残差模块（BRM），它试图恢复图像结构和纹理，同时将难以恢复的信息传递给更深层次的模块</li><li>提出了一种新的嵌入多个BRM的技术，它可以有效地提高基于每个模块输出的最终重建质量.</li></ul><hr><h3 id="Related-work"><a href="#Related-work" class="headerlink" title="Related work"></a>Related work</h3><p>SISR问题在现存的文献中存在的方法可以分为3大类：基于插值方法，基于重建的方法，基于学习的方法。当前超分辨问题的研究方向有以下几个方面：</p><ul><li>通过优化CNN模型框架，介绍了基于深度学习的超分辨率经典模型，包括SRCNN、VDSR、DRCN、EDSR等，这些模型都是基于优化PSNR/SSIM的；</li><li>基于优化损失函数：较为常见的有L1、L2，以及后来的perceptual loss 和adversarial loss.介绍了基于优化感知损失的SRGAN 、SFTGAN模型。</li><li>基于扩大放大倍数上，x2,x3,x4的研究工作已经快接近瓶颈，x8放大成为研究热点。</li></ul><p>通过文献阅读，当前没有人研究模型复杂度和图像频率信息，本文就是基于此开展研究的。</p><hr><h3 id="Proposed-Method"><a href="#Proposed-Method" class="headerlink" title="Proposed Method"></a>Proposed Method</h3><p>基于上述的出发点，作者的策略是对于低频的信息，使用一个大网络结构中的浅层网络进行恢复，对于高频信息，使用一个大网络结构中的深层网络进行恢复。因为对于一个网络结构而言，其中深层的层相比于浅层而言，具有更强的函数拟合能力。因此，作者只需要将图片中的不同频率的信息交由不同深度的网络进行恢复即可。作者的整体网络结构图如下所示：</p><blockquote><p><img src="http://sotavision.cn/showdoc/server/../Public/Uploads/2020-03-04/5e5f1f4a52d7c.png" alt="" loading="lazy"></p></blockquote><h4 id="Block-Residual-Module"><a href="#Block-Residual-Module" class="headerlink" title="Block Residual Module"></a>Block Residual Module</h4><p>从该图可以看出，浅层的网络恢复低频信息，深层的网络恢复高频信息，最后再将这些信息进行concat便得到了最终的超分图片。那么现在的问题是如何在同一个网络结构下，使用不同深度的网络对不同频率的信息进行恢复。针对此，作者设计了一个网络模块BRM，如下图所示：</p><blockquote><p><img src="http://sotavision.cn/showdoc/server/../Public/Uploads/2020-03-04/5e5f28c62f5eb.png" alt="" loading="lazy"></p></blockquote><p>这个模块的作用是使用当前深度的网络恢复部分HR信息，同时将没有恢复的信息(相对来说属于高频的信息)传至更深的网络中进行恢复。这个模块有两个分支：super-resolution flow和back-projection flow。</p><ul><li>super-resolution flow: 对输入的信息(低分辨率特征层)进行超分。</li><li>back-projection flow: 对super-resolution flow的超分图片进行一个下采样，得到一个低分辨率特征层，之后用输出的低分辨率特征层减去这个低分辨率，就能得到此时还没有恢复的信息。之后再将这部分信息输入到一个残差模块，之后将输出的信息输入到下一个RBM模块。</li></ul><p>为什么相减能得到没有恢复的信息：因为原始的LR图片是由HR图片进行bicubic降采样得到的，而用LR超分得到的当前的SR图片的信息肯定是小于HR的，因此对SR进行下采样得到的\(LR’\)的信息肯定是小于\(LR\)的，所以相减就能得到当前深度的网络还没有恢复的信息。<br>之后便是如何对这些恢复的不同频率的信息进行结合了，其中作者发现较深的网络恢复的信息能够提高浅层网络的信息恢复效果，因此提出了一个recursive fusion的方法，这个应该是通过尝试多种不同的信息融合方式所得到的结果,fusion process是：\(O’_x = f(O_x+O’_{x+1})\)</p><h4 id="Loss-Function"><a href="#Loss-Function" class="headerlink" title="Loss Function"></a>Loss Function</h4><p>这里作者先用L1loss快速收敛，让后用L2loss微调模型参数。</p><hr><h3 id="Experiment"><a href="#Experiment" class="headerlink" title="Experiment"></a>Experiment</h3><h4 id="Experiments-Details"><a href="#Experiments-Details" class="headerlink" title="Experiments Details"></a>Experiments Details</h4><p>作者暂未公开代码，可以依照原文自己复现。</p><h4 id="EBRN-VS-other-Network"><a href="#EBRN-VS-other-Network" class="headerlink" title="EBRN VS other Network"></a>EBRN VS other Network</h4><h5 id="EBRN-VS-Residual-Network"><a href="#EBRN-VS-Residual-Network" class="headerlink" title="EBRN VS Residual Network"></a>EBRN VS Residual Network</h5><p>与传统CNN模型相比，残差网络的优势在于残差学习促进了网络中特征的传输，缓解了梯度消失问题，使得网络更容易训练。<br>在这项工作中，我们利用了不同于传统的残差网络的残差学习思想。例如：</p><ol><li>由于BN层限制了特征归一化过程中中间特征的范围灵活性，所以本文模型没有使用batch normalization (BN)层。</li><li>另一个重要的区别是残差是如何计算的以及残差所传达的信息。在残差网络中，剩余信号是输入和输出的差值。在该模型中，一种残差信号是某一频率范围内的信息;残差信号的另一种类型是原始LR特征与反投影LR特征之间的差异。在每一个BRM中，第二个残差信号对于SR是很重要的，因为它明确地传达了哪些信息需要后续BRM恢复。</li></ol><h5 id="EBRN-VS-Deep-Back-Projection-Network"><a href="#EBRN-VS-Deep-Back-Projection-Network" class="headerlink" title="EBRN VS Deep Back-Projection Network"></a>EBRN VS Deep Back-Projection Network</h5><p>DBPN：该方法利用迭代的上下采样层，为每个阶段的投影误差提供了误差反馈机制。误差可以有效地提高模型中深层的恢复效果。<br>两种方法的不同之处在于:</p><ol><li>在上投影单元和下投影单元中，DBPN将LR残差直接映射到HR空间，而我们的模型中LR残差包含更高频率的信息，这些信息被反馈到更深的子网络中进行恢复;</li><li>DBPN利用LR残差和HR残差，目标是每个上、下投影单元尽量减小残差，而我们的方法将残差信号与不同频率的信息联系起来，每个BRM负责恢复相应的信息。动机的不同导致模型的参数比DBPN少，但性能比DBPN有提高。</li></ol><h4 id="Model-Analyses"><a href="#Model-Analyses" class="headerlink" title="Model Analyses"></a>Model Analyses</h4><p>为了验证这一点，我们在图中演示了不同频段上不同BRM输出的能量分布。利用小波变换的不同能级系数，计算了能量在不同频段的分布。结果表明，浅层BRMs的输出包含更多的低频信息，而深层BRMs的输出则倾向于恢复更多的高频信息。</p><blockquote><p><img src="http://sotavision.cn/showdoc/server/../Public/Uploads/2020-03-04/5e5f356e406e3.png" alt="" loading="lazy"></p></blockquote><h4 id="运行速度、最优BRM数验证"><a href="#运行速度、最优BRM数验证" class="headerlink" title="运行速度、最优BRM数验证"></a>运行速度、最优BRM数验证</h4><p><img src="http://sotavision.cn/showdoc/server/../Public/Uploads/2020-03-04/5e5f35dc49d26.png" alt="" loading="lazy"></p><h4 id="Comparisons-with-state-of-the-arts"><a href="#Comparisons-with-state-of-the-arts" class="headerlink" title="Comparisons with state-of-the-arts"></a>Comparisons with state-of-the-arts</h4><blockquote><p><img src="http://sotavision.cn/showdoc/server/../Public/Uploads/2020-03-04/5e5f363b00d9b.png" alt="" loading="lazy"><br><img src="http://sotavision.cn/showdoc/server/../Public/Uploads/2020-03-04/5e5f362b6df91.png" alt="" loading="lazy"></p></blockquote><hr><h3 id="Conclusions"><a href="#Conclusions" class="headerlink" title="Conclusions"></a>Conclusions</h3><ul><li>作者提出了图片的低频和高频信息的复杂程度是不一样的，因此在对这两部分信息进行恢复的时候，应该使用具有不同复杂度的模型或者是不同深度的网络结构</li><li>提出了一种块残差模块（BRM），它试图恢复图像结构和纹理，同时将难以恢复的信息传递给更深层次的模块</li><li>提出了一种新的嵌入多个BRM的技术，它可以有效地提高基于每个模块输出的最终重建质量.</li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;https://cdn.jsdelivr.net/npm/aplayer@1.10/dist/APlayer.min.css&quot;&gt;&lt;script 
      
    
    </summary>
    
    
      <category term="论文笔记" scheme="http://alexzou14.github.io/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="深度学习" scheme="http://alexzou14.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="笔记" scheme="http://alexzou14.github.io/tags/%E7%AC%94%E8%AE%B0/"/>
    
      <category term="超分辨率" scheme="http://alexzou14.github.io/tags/%E8%B6%85%E5%88%86%E8%BE%A8%E7%8E%87/"/>
    
  </entry>
  
  <entry>
    <title>Image Super-Resolution by Neural Texture Transfer论文阅读笔记</title>
    <link href="http://alexzou14.github.io/2020/04/10/SRNTT/"/>
    <id>http://alexzou14.github.io/2020/04/10/SRNTT/</id>
    <published>2020-04-10T13:08:38.000Z</published>
    <updated>2020-04-10T13:27:29.000Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="https://cdn.jsdelivr.net/npm/aplayer@1.10/dist/APlayer.min.css"><script src="https://cdn.jsdelivr.net/npm/aplayer@1.10/dist/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="https://cdn.jsdelivr.net/npm/meting@1.2/dist/Meting.min.js"></script><h3 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h3><p>当参考（Ref）图像的内容与给定出输入LR相似时，基于参考的超分辨率方法（RefSR）在恢复高分辨率（HR）细节方面已被证明是有希望的。由于RefSR方法不稳定，生成的图像质量不高，所以本文的目的是通过参考图像利用更多的纹理细节，甚至更强的鲁棒性，释放参考图像的潜力。作者受到风格迁移的思想激发，可以将RefSR方法可以应用神经风格迁移方法。<br>本文提出了一个端到端的深层模型，它使HR细节通过自适应转移相似的语义信息，本文方法主要包括两步：</p><ol><li>特征空间的纹理匹配，</li><li>移匹配的纹理。</li></ol><p>另外，本文提出一个CUFFED5数据集，这个数据集包含不同相似级别的参考图像。</p><hr><h3 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h3><p>在神经网络的引入下，解决SISR卓越de的方法得到了很大的提升。由于SISR问题的不定性，这导致现有的大部分方法还是在大放大倍数下会产生模糊的结果。在感知损失和对抗损失的引入下提升了图像的感知细节，但是同时又会在一定程度上产生虚假纹理和伪影。<br>基于参考（reference-based）的方法, 即RefSR，通过利用高分辨率（HR）参考图像的丰富纹理来补偿低分辨率（LR）图像丢失的细节。但是之前的方法需要参考图像与LR包含相似的内容，并且需要图像对齐，否则的话，这些方法效果会很差。RefSR理想上应该可以有效利用参考图像，如果参考图像与LR不同，也不应该损害LR的复原效果，即RefSR应不差于SISR。<br>现有的RefSP模型对参考图像有很高的要求，要求参考图像与模糊图像的内容相仿且具有良好的对齐，这是比较难做到的，后来有人提出使用optical flow(一种图像对齐算法)先对参考图像和模糊图像进行对齐，然后送入RefSR模型。但是optical flow在两张图象的错位极其严重时表现欠佳，因此Adobe团队提出了基于纹理迁移的图像超分辨率模型(Image Super-Resolution by Neural Texture Transfer)，简称为SRNTT。<br>SRNTT主要有以下几个贡献：</p><ul><li>解决了现有SISR方法会出现虚假纹理的问题</li><li>放松了现有的RefSR方法的约束问题，不要求参考图像与模糊图像严格对齐</li><li>提高了现有RefSR方法的鲁棒性，即使使用相似性不是很高的参考图像也可以得到较好的结果构建了一个基准数据集CUFED5</li></ul><hr><h3 id="Related-work"><a href="#Related-work" class="headerlink" title="Related work"></a>Related work</h3><h4 id="Deep-Learning-based-SISR"><a href="#Deep-Learning-based-SISR" class="headerlink" title="Deep Learning based SISR"></a>Deep Learning based SISR</h4><p>单图超分辨率模型(SISR)的输入只有一张图象，模型会从这一张图像提取一些高频信息并使用特殊的方法合成到原图上去，以完成超分辨率的过程。这种方法有一个缺点，模糊图像毕竟不含有我们想要的高频信息，所以即便我们使用特殊的方法去提取，最后得到的结果也不可能与实际情况完全相同，也就是说，最后模型得到的图像存在一些虚假的纹理，虽然在视觉效果上图像是清晰的，但是图像的细节信息却是假的。</p><h4 id="Reference-based-Super-Resolution"><a href="#Reference-based-Super-Resolution" class="headerlink" title="Reference-based Super-Resolution"></a>Reference-based Super-Resolution</h4><p>基于参考图像的超分辨率(RefSR)。这种模型的输入图像有两个，一个是模糊图像，一个是清晰图像。模型会从清晰图像中提取真实的高频信息，然后将其合成到模糊图像中去。也许你会有一个疑问，既然已经有了清晰的图像，为什么我们去做超分辨率？这是因为清晰图像的角度、拍摄内容、光线等不一定乐意是我们满意，但它的高频信息却是我们需要的。</p><hr><h3 id="Proposed-Method"><a href="#Proposed-Method" class="headerlink" title="Proposed Method"></a>Proposed Method</h3><p>网络结构如下图。</p><blockquote><p><img src="http://sotavision.cn/showdoc/server/../Public/Uploads/2020-03-04/5e5f701e31110.png" alt="" loading="lazy"></p></blockquote><p>SRNTT主要由两个部分组成，一是上图中蓝色方框之外的部分，称之为特征交换；另一部分为蓝色方框内部的纹理迁移部分。</p><h4 id="Feature-Swapping"><a href="#Feature-Swapping" class="headerlink" title="Feature Swapping"></a>Feature Swapping</h4><p>特征交换这一部的主要工作就是在Ref中找到与LR最接近的块，然后替换掉LR中的块。这就涉及两个步骤: 如何计算相似度、如何交换对应块。</p><ul><li>如何计算相似度</li></ul><p>论文提出的方法不是计算整个图的相似度，而是分块计算。这里因为LR和Ref的大小不同，先对LR用bicubic进行上采样使LR和Ref具有相同的大小，而同时考虑到二者的分辨率不同，对Ref用bicubic进行下采样再上采样，使得\(I^{Ref\downarrow\uparrow}\)的模糊程度跟\(I^{LR\uparrow}\)接近。<br>考虑到\(I^{LR\uparrow}\)和\(I^{Ref\downarrow\uparrow}\)的块在颜色和光照上面可能有些不同，于是论文不直接对Patch的像素进行计算相似度，而在高层次的特征图上计算，即\(\phi(I)\)，来突出结构和纹理信息。<br>论文采用内积方法来计算相似度：<br>\[s_{i,j}=\langle{P_i(\phi(I^{LR\uparrow})),\dfrac{P_i(\phi(I^{LR\uparrow}))}{\left |P_i(\phi(I^{LR\uparrow})\right |}}\rangle\]<br>上式，计算了LR的 i-th patch和模糊化参考图像 的 j-th patch的相似性。注意，对参考patch进行了归一化。可以通过卷积或者互相关来加速以上计算过程：<br>\[S_j=\phi(I^{LR\uparrow}*{\dfrac{P_j(\phi(I^{LR\uparrow}))}{\left |P_j(\phi(I^{LR\uparrow})\right |}})\]<br>\(S_j\)即表示 s-th patch 相对 LR的相似性。相当于一个卷积核对LR进行卷积，计算结果即为Similarity map。</p><ul><li>交换操作</li></ul><p>因为在LR和Ref的特征空间中密集采样，所以每个LR位置都对应多个不同的卷积核的卷积结果，对应多个不同相似性的纹理特征。基于Similarity map，选择LR每个位置的相似性最高的Ref patch，构成交换特征图M (swapped feature map)：<br>\[P_{\omega(x,y)}(M)=P_{j^*}(\phi(I^{Ref})),j^*=\arg \max_jS_j(x,y)\]<br>即M在（x,y）位置对应的Ref patch是similairty score最大的Ref patch。由于每一个位置对应一个Ref patch，所以这些patch是重叠的，在重叠位置，取平均。另外，另外，注意计算相似性是使用\(I^{Ref\uparrow\downarrow}\)，纹理迁移则是利用 \(I^{Ref}\)。<br>在实现上，利用VGG-19提取特征，relu1_1, relu2_1,relu3_1用于多个尺度上纹理编码，但是为了加快匹配，只在relu3_1层进行匹配，将匹配结果对应到relu1_1和relu2_1。</p><h4 id="Neural-Texture-Transfer"><a href="#Neural-Texture-Transfer" class="headerlink" title="Neural Texture Transfer"></a>Neural Texture Transfer</h4><p>有了多尺度的swapped feature map，如何进行纹理迁移呢？采用residual blocks和跳跃连接方式，融合LR的特征和swap特征，并通过sub-pixel conv上采样。如下图：</p><blockquote><p><img src="http://sotavision.cn/showdoc/server/../Public/Uploads/2020-03-04/5e5f7c3c83fb7.png" alt="" loading="lazy"></p></blockquote><p>论文采用残差块和跳跃连接来建立基本的生成网络。记\(\psi_l\)是第l层的输出，可以定义为：<br>\[\psi_l=[\text{Res}(\psi_{l-1}||M_{l-1})+\psi_{l-1}]\uparrow_{2\times}\]<br>假设有L层，那最终SR:<br>\[I^{SR}=\text{Res}(\psi_{L-1}||M_{L-1})+\psi_{L-1}\]</p><h4 id="Loss-function"><a href="#Loss-function" class="headerlink" title="Loss function"></a>Loss function</h4><p>目标：1）保留LR的空间结构，2）提升SR的视觉质量，3）充分利用Ref的丰富纹理。提出四个loss function：</p><ul><li>Reconstruction loss: SR与HR的L1距离</li></ul><p>\[L_{rec}=\left |I^{HR}-I^{SR} \right |_1\]</p><ul><li>Perceptual loss： 采用VGG-19的relu5_1层</li></ul><p>\[{L_{per}=\dfrac{1}{V}\sum_{i=1}^C\left |{\phi_i(I^{HR})-\phi_i(I^{SR})}\right |_F}\]</p><ul><li>Adversarial loss：利用 WGAN-GP<div>  \[L_{adv}=-\text{E}_{\tilde{x}\sim{P_g}}[D(\tilde{x})],\min_g \max_{D\in \text{D}}\text{E}_{x \sim{P_r}}[D(x)]-\text{E}_{\tilde{x}\sim{P_g}}[D(\tilde{x})]\]</div></li><li>Texture loss：纹理损失是为了让SR的纹理与Ref纹理更接近，另外，通过使用similarity map作为权重，抑制不相似纹理的惩罚，放大相似纹理的惩罚，这样可以自适应地进行纹理迁移<div>  \[L_{tex}=\sum_l{\lambda_l \left \|Gr(\phi_l(I^{SR})\cdot S_l^*)-Gr(M_l\cdot S_l^*) \right \|}_F\]</div></li></ul><p>其中，\(G_r\)计算 Gram matrix。</p><hr><h3 id="Experiment"><a href="#Experiment" class="headerlink" title="Experiment"></a>Experiment</h3><h4 id="Experiments-Details"><a href="#Experiments-Details" class="headerlink" title="Experiments Details"></a>Experiments Details</h4><p>具体实验细节可以查看官方代码：<a href="https://github.com/ZZUTK/SRNTT" target="_blank" rel="noopener">https://github.com/ZZUTK/SRNTT</a></p><h4 id="DataSet"><a href="#DataSet" class="headerlink" title="DataSet"></a>DataSet</h4><p>本文基于CUFED构建了一个数据集，数据集包含不同相似度的LR-HR-Ref 图像对。共有4个相似度级别，不同的相似度是基于SIFT特征匹配计算的。测试数据集包含Sun80和Urban100。</p><h4 id="Experiment-Result"><a href="#Experiment-Result" class="headerlink" title="Experiment Result"></a>Experiment Result</h4><p>论文对比了不同的模型在不同数据集上的表现，最后结果是，定量观测PSNR值来看，在单图超分辨率领域，SRNTT取得第二名；在基于参考的超分辨领域，SRNTT优于现有的所有模型，位列第一。</p><blockquote><p><img src="http://sotavision.cn/showdoc/server/../Public/Uploads/2020-03-04/5e5f9e4edb182.png" alt="" loading="lazy"></p></blockquote><hr><h3 id="Conclusions"><a href="#Conclusions" class="headerlink" title="Conclusions"></a>Conclusions</h3><ul><li>论文学会用RefSR的思想，打破了SISR的束缚。（其实就是不再单纯地学习HR和LR的映射，而是引入RefSR）</li><li>论文提出了SRNTT，可以得到更好的超分辨率效果。</li><li>论文建立了一个新的数据集，CUFED5，对LR有不同相似程度的RefSR，用来进行进一步探索。</li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;https://cdn.jsdelivr.net/npm/aplayer@1.10/dist/APlayer.min.css&quot;&gt;&lt;script 
      
    
    </summary>
    
    
      <category term="论文笔记" scheme="http://alexzou14.github.io/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="深度学习" scheme="http://alexzou14.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="笔记" scheme="http://alexzou14.github.io/tags/%E7%AC%94%E8%AE%B0/"/>
    
      <category term="超分辨率" scheme="http://alexzou14.github.io/tags/%E8%B6%85%E5%88%86%E8%BE%A8%E7%8E%87/"/>
    
  </entry>
  
  <entry>
    <title>Real-Time Single Image and Video Super-Resolution Using an EfficientSub-Pixel Convolutional Neural Network论文阅读笔记</title>
    <link href="http://alexzou14.github.io/2020/04/10/ESPCN/"/>
    <id>http://alexzou14.github.io/2020/04/10/ESPCN/</id>
    <published>2020-04-10T12:55:35.000Z</published>
    <updated>2020-04-10T13:06:04.000Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="https://cdn.jsdelivr.net/npm/aplayer@1.10/dist/APlayer.min.css"><script src="https://cdn.jsdelivr.net/npm/aplayer@1.10/dist/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="https://cdn.jsdelivr.net/npm/meting@1.2/dist/Meting.min.js"></script><h3 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h3><p>近来，基于深层神经网络的几种模型在单像超分辨率的重构精度和计算性能方面取得了巨大的成功。在这些方法中，低分辨率（LR）输入图像在重建之前使用单个滤波器（通常是双三次插值）被放大到高分辨率（HR）空间再输入到网络中重建，这样会增加了计算复杂度。在本文中，作者提出了第一个能够实现1080p视频实时SR的卷积神经网络。为了实现这一点，我们提出了一种新颖的CNN架构，其中在LR空间中提取特征图。另外，我们引入了一个有效的亚像素卷积层，该层学习了一个升序滤波器阵列，将最终的LR特征图升级到HR输出。通过这样做，我们有效地更换SR管道中的手工双三次插值滤波器，并为每个特征图进行了专门训练的更复杂的升频滤波器，同时还降低了整个SR操作的计算复杂度。</p><hr><h3 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h3><p>全局SR问题假设LR数据是HR数据的低通滤波（模糊），下采样和噪声版本。 由于在不可逆低通滤波和子采样期间发生的高频信息的丢失，这是一个高度ill-posed的问题。 此外，SR操作实际上是从LR到HR空间的一对多映射，其可以具有多个解决方案，其中确定正确的解决方案是不容易的。 基于许多SR技术的一个关键假设是大部分高频数据是冗余的，因此可以从低频分量精确地重构。因此，SR是一个推理问题，因此依赖于图像的统计信息。</p><hr><h3 id="Related-work"><a href="#Related-work" class="headerlink" title="Related work"></a>Related work</h3><p>许多方法假定多个图像可看作具有不同视角的相同场景的LR实例，即具有独特的先前仿射变换。这些可以被分类为多图像SR方法，并且通过基于附加信息来限制ill-posed问题并尝试反转下采样过程的方式来利用显式冗余信息。然而，这些方法通常需要计算复杂的图像配准和融合阶段，其准确性直接影响结果的质量。另一个方法是单图像超分辨率（SISR）技术。这些技术寻求学习自然数据中存在的隐性冗余，以从单个LR实例中恢复丢失的HR信息。这通常以图像的局部空间相关性和视频中的附加时间相关性的形式出现。在这种情况下，需要以重建约束的形式的先验信息来限制重构的解空间。<br>本文主要贡献为：<br>1.upscaling由network最后一层处理，也就表示每各LR图像可以从network中之间得到反馈并在LR空间里中进行特种提取。（减少计算和存储器的复杂度）</p><p>2.对于具有L层的网络，我们学习nL-1特征映射的nL-1升级滤波器，而不是输入图像的一个升频滤波器。此外，不使用显式插值滤波器意味着网络隐式地学习SR所需的处理。因此，与在第一层的单个固定滤波器放大相比，网络能够学习更好和更复杂的LR到HR映射。这导致模型的重建精度的额外增益</p><hr><h3 id="Proposed-Method"><a href="#Proposed-Method" class="headerlink" title="Proposed Method"></a>Proposed Method</h3><h4 id="Network-structure"><a href="#Network-structure" class="headerlink" title="Network structure"></a>Network structure</h4><blockquote><p><img src="http://sotavision.cn/showdoc/server/../Public/Uploads/2020-04-10/5e90616e8620f.png" alt="" loading="lazy"></p></blockquote><p>ESPCN的核心概念是亚像素卷积层(sub-pixel convolutional layer)。如上图所示，网络的输入是原始低分辨率图像，通过两个卷积层以后，得到的特征图像大小与输入图像一样，但是特征通道为\(r^2\)（r是图像的目标放大倍数)。将每个像素的个通道\(r^2\)重新排列成一个r x r的区域，对应于高分辨率图像中的一个r x r大小的子块，从而大小为\( r^2 \times H \times W \) 的特征图像被重新排列成\( 1 \times rH \times rW \)大小的高分辨率图像。这个变换虽然被称作sub-pixel convolution, 但实际上并没有卷积操作。通过使用sub-pixel convolution, 图像从低分辨率到高分辨率放大的过程，插值函数被隐含地包含在前面的卷积层中，可以自动学习到。只在最后一层对图像大小做变换，前面的卷积运算由于在低分辨率图像上进行，因此效率会较高。</p><p>该网络可以用数学公式表达为：<br>\[ f^1(I^{LR};W_1,b_1)=\phi(W_1 \ast I^{LR}+b_1) \]<br>\[ f^l(I^{LR};W_{1:l},b_{1:l})=\phi(W_l \ast f^{l-1}(I^{LR})+b_l) \]</p><h4 id="Efficient-sub-pixel-convolution-layer"><a href="#Efficient-sub-pixel-convolution-layer" class="headerlink" title="Efficient sub-pixel convolution layer"></a>Efficient sub-pixel convolution layer</h4><p>deconvolution, FSRCNN在最后的上采样层，通过学习最后的deconvolution layer。但deconvolution本质上是可以看做一种特殊的卷积，理论上后面要通过stack filters才能使得性能有更大的提升。本文提到的亚像素卷积sub-pixel Layer，其实跟常规的卷积层没有什么不同，不同的是其输出的特征通道数为r^2，其中r为缩放倍数。表达式为：<br>\[ I^{SR}=f^L(I^{LR})=\mathcal{PS}(W_L \ast f^{L-1}(I^{LR})+b_L) \]</p><p>可从上面的公式看到，所得的高分辨率图像是通过PS操作，将r^2维度的低分辨率特征重新排列成高分辨率图像。其中PS操作称之为： periodic shuffling<br>本质上就是将低分辨率特征，按照特定位置，周期性的插入到高分辨率图像中，可以通过颜色观测到上图的插入方式。可以表达为：</p><div>    \[ \mathcal{PS}(T)_{x,y,c}=T_{\left \lfloor x/r \right\rfloor,\left \lfloor y/r \right\rfloor, C \cdot r \cdot mod(y,r)+C \cdot mod(x,r)+c} \]</div><h4 id="Loss-function"><a href="#Loss-function" class="headerlink" title="Loss function"></a>Loss function</h4><p>文章用了MSE loss优化网络：<br>\[ l(W_{1:l},b_{1:L})=\dfrac{1}{r^2HW}\displaystyle\sum_{x=1}^{rH}\sum_{x=1}^{rW}(I_{x,y}^{HR}-f_{x,y}^L(I^{LR}))^2 \]</p><hr><h3 id="Experiment"><a href="#Experiment" class="headerlink" title="Experiment"></a>Experiment</h3><h4 id="Comparison-to-the-state-of-the-art"><a href="#Comparison-to-the-state-of-the-art" class="headerlink" title="Comparison to the state-of-the-art"></a>Comparison to the state-of-the-art</h4><p>图像质量：</p><blockquote><p><img src="http://sotavision.cn/showdoc/server/../Public/Uploads/2020-04-10/5e906a516211f.png" alt="" loading="lazy"></p></blockquote><p>重建质量也是能够达到当时最优的。</p><h4 id="Run-time-evaluations"><a href="#Run-time-evaluations" class="headerlink" title="Run time evaluations"></a>Run time evaluations</h4><p>运行速度:</p><blockquote><p><img src="http://sotavision.cn/showdoc/server/../Public/Uploads/2020-04-10/5e906ad840ea9.png" alt="" loading="lazy"></p></blockquote><p>速度上是相当快的，基本上能在视频上进行实时处理</p><hr><h3 id="Conclusions"><a href="#Conclusions" class="headerlink" title="Conclusions"></a>Conclusions</h3><ol><li>作者提出了ESPCN模型，并达到了当时最先进的性能。</li><li>证明了第一层的固定滤波器升级不能为SISR提供任何额外信息，但需要更多的计算复杂性。</li><li>提出了一种新颖的亚像素卷积层，与去卷积层相比，它能够以非常小的额外计算成本将LR数据超解析为HR空间。 在升级因子为4的扩展基准标记数据集上进行的评估表明，与之前的CNN方法相比，我们具有显着的速度（&gt; 10倍）和性能（图像上的+ 0.15dB和视频上的+ 0.39dB）增强。 这使我们的模型成为第一个能够在单个GPU上实时生成SR高清视频的CNN模型。</li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;https://cdn.jsdelivr.net/npm/aplayer@1.10/dist/APlayer.min.css&quot;&gt;&lt;script 
      
    
    </summary>
    
    
      <category term="论文笔记" scheme="http://alexzou14.github.io/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="深度学习" scheme="http://alexzou14.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="笔记" scheme="http://alexzou14.github.io/tags/%E7%AC%94%E8%AE%B0/"/>
    
      <category term="超分辨率" scheme="http://alexzou14.github.io/tags/%E8%B6%85%E5%88%86%E8%BE%A8%E7%8E%87/"/>
    
  </entry>
  
  <entry>
    <title>Feedback Network for Image Super-Resolution论文阅读笔记</title>
    <link href="http://alexzou14.github.io/2020/04/08/SRFBN/"/>
    <id>http://alexzou14.github.io/2020/04/08/SRFBN/</id>
    <published>2020-04-08T03:19:33.000Z</published>
    <updated>2020-04-08T03:34:21.000Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="https://cdn.jsdelivr.net/npm/aplayer@1.10/dist/APlayer.min.css"><script src="https://cdn.jsdelivr.net/npm/aplayer@1.10/dist/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="https://cdn.jsdelivr.net/npm/meting@1.2/dist/Meting.min.js"></script><h3 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h3><p>由于现有的基于深度学习的图像SR方法尚未充分利用人类视觉系统中常见的反馈机制，所以本文基于这一想法提出了一个图像超分辨率反馈网络(SRFBN)通过高层信息来细化低层信息。具体这种反馈机制，是用具有约束的RNN中的隐状态来实现这种反馈机制。这种反馈机制能够产生非常强的高层表征能力。。此外，作者还引入了curriculum learning 策略，使网络非常适合于更复杂的任务。</p><hr><h3 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h3><p>随着网络的深度增加，参数的数量也会增加。大容量网络将占用大量存储资源并遭受过度拟合问题。为了减少网络参数，通常采用循环结构。因为具有重复结构的这些网络可以以前馈方式共享信息。但是，前馈方式使得先前的层不可能从以下层访问有用信息，即使采用跳过连接也是如此。<br>在本文中，作者提出了一种新的图像SR网络，即超分辨率反馈网络（SRFBN），以便通过反馈连接使用高级信息来改进低级信息。 所提出的SRFBN本质上是具有反馈块（FB）的RNN，其专门用于图像SR任务。<br>总之，我们的主要贡献如下：</p><ul><li>提出采用反馈机制的图像超分辨率反馈网络（SRFBN）。 通过反馈连接在自上而下的反馈流中提供高级信息。 同时，这种具有反馈连接的循环结构提供了强大的早期重建能力，并且仅需要很少的参数。</li><li>提出反馈块（FB），它不仅可以有效地处理反馈信息流，还可以通过上采样层和下采样层以及密集跳过连接来丰富高级表示。</li><li>为SRFBN提出curriculum -based训练策略，其中将具有增加的重建难度的HR图像作为连续迭代的目标馈入网络。 该策略使网络能够逐步学习复杂的退化模型，而对于那些只有一步预测的方法，同样的策略是不可能的。</li></ul><hr><h3 id="Related-work"><a href="#Related-work" class="headerlink" title="Related work"></a>Related work</h3><h4 id="Deep-learning-based-image-super-resolution"><a href="#Deep-learning-based-image-super-resolution" class="headerlink" title="Deep learning based image super-resolution"></a>Deep learning based image super-resolution</h4><p>这一部分作者主要回顾了深度学习在超分变率领域的应用和方法回顾。从2015年董超提出SRCN开始将深度学习引入超分辨领域，近几年的超分领域的不断发展，提出了很多方法，如：VDSR，EDSR等</p><h4 id="Feedback-mechanism"><a href="#Feedback-mechanism" class="headerlink" title="Feedback mechanism"></a>Feedback mechanism</h4><p>反馈机制能够允许该网络携带当前的输出去纠正之前的一些状态。<br>针对图片超分辨率中的feedback mechanism，有两个必要条件：<br>1）迭代 ；见下图(b)<br>2）重新路由（rerouting）系统的输出，以纠正（correct）每个循环中的输入， 在本文提出的网络结构中，实施feedback mechanism，有三个必不可少的部分:<br>1）在每一次迭代过程中都绑定loss；<br>2）使用循环结构（实现迭代的过程）；<br>3）在每一次迭代过程中提供低分辨率图片的输入。<br>上面三个部分缺一不可。</p><blockquote><p><img src="http://sotavision.cn/showdoc/server/../Public/Uploads/2020-03-11/5e68efd37e4e0.png" alt="" loading="lazy"></p></blockquote><h4 id="Curriculum-learning"><a href="#Curriculum-learning" class="headerlink" title="Curriculum learning"></a>Curriculum learning</h4><p>Curriculum learning逐渐增加了学习目标的难度，众所周知，这是改进训练程序的有效策略。早期的课程学习工作主要集中在一项任务上。 Pentina等以连续的方式将课程学习扩展到多个任务。虽然之前的工作主要集中在单个degradation过程，但我们对案例强制执行curriculum ，其中LR图像被多种类型的劣化所破坏。包含易于做出决策的curriculum可以针对一个问题进行解决，以逐步恢复损坏的LR图像。</p><hr><h3 id="Proposed-Method"><a href="#Proposed-Method" class="headerlink" title="Proposed Method"></a>Proposed Method</h3><blockquote><p><img src="http://sotavision.cn/showdoc/server/../Public/Uploads/2020-03-11/5e68f4db2d8fc.png" alt="" loading="lazy"></p></blockquote><p>见图我们提出的SRFBN可以被展开为T个迭代，迭代的顺序是从1到t，为了使得中间的状态能够携带一些输出信息，我们在每一次的迭代后面都会绑定一个loss值。<br>网络结构被分为三个模块：LR特征提取模块，反馈模块，重建模块</p><h4 id="Network-structure"><a href="#Network-structure" class="headerlink" title="Network structure"></a>Network structure</h4><ul><li><p>LR特征提取模块:这个LR特征提取模块由一个conv(3, 4m)和一个conv(3, m)组成，浅层特征提取可以由下列表达式表达：\(F_{in}^t = f_{LRFB}(I_{LR})\)</p></li><li><p>反馈模块:第t个权重共享模块的输出可以表示为：\(F_{out}^t=f_{FB}(F_{out}^{t-1},F_{in}^t)\)</p></li><li><p>第t个权重共享模块的中间监督输出：\(I_{SR}^t=I_{Res}^t+f_{UP}(I_{LR})\)其中\(I_{Res}^t=f_{RB}(F_{out}^t)\)</p></li></ul><h4 id="Feedback-block"><a href="#Feedback-block" class="headerlink" title="Feedback block"></a>Feedback block</h4><p><img src="http://sotavision.cn/showdoc/server/../Public/Uploads/2020-03-11/5e68feb50db97.png" alt="" loading="lazy"><br>简而言之，反馈模块就是反复上采样再下采样操作，同时，对所有上采样后的特征用dense connection，也对下采样后的特征用dense connection，中间用1*1卷积来降低计算量。<br>输入特征：\(L_0^t = C_0([F_{out}^{t-1},F_{in}^t])\)<br>上采样后的特征：\(H_g^t = C_g^{\uparrow}([L_0^t,L_1^t,…,L_{g-1}^t])\)<br>下采样后的特征：\(L_g^t = C_g^{\downarrow}([H_1^t,H_2^t,…,H_{g-1}^t])\)<br>输出特征：\(f_{out}^t = C_{FF}([L_1^t,L_2^t,…,L_{G}^t])\)</p><h4 id="Loss-function"><a href="#Loss-function" class="headerlink" title="Loss function"></a>Loss function</h4><p>loss function 为：\(L({\Theta}) = \dfrac{1}{T} \displaystyle\sum_{t=1}^{T}W^t||I_{HR}^t - I_{SR}^t||\)。控制数值\(W^t\)实验中都为1。</p><h4 id="Curriculum-learning-strategy"><a href="#Curriculum-learning-strategy" class="headerlink" title="Curriculum learning strategy"></a>Curriculum learning strategy</h4><p> 简而言之就是，中间监督的真值会根据任务难度进行选择，比如单一的bicubic降采样退化，所有的真值都是一样的；而对于BD（bicubic+blur）退化，头两个中间监督输出用带高斯模糊的真值，之后的中间监督用不带高斯模糊的真值。</p><hr><h3 id="Experiment"><a href="#Experiment" class="headerlink" title="Experiment"></a>Experiment</h3><h4 id="Experiments-Details"><a href="#Experiments-Details" class="headerlink" title="Experiments Details"></a>Experiments Details</h4><p>实验细节可以参考官方代码：<a href="https://github.com/Paper99/SRFBN_CVPR19" target="_blank" rel="noopener">https://github.com/Paper99/SRFBN_CVPR19</a></p><h4 id="Network-analysis"><a href="#Network-analysis" class="headerlink" title="Network analysis"></a>Network analysis</h4><p>在这一小节中，我们探讨了迭代次数(表示为T)和反馈块中投影组的数目(表示为G)的影响。</p><blockquote><p><img src="http://sotavision.cn/showdoc/server/../Public/Uploads/2020-03-11/5e6906c0a2c1c.png" alt="" loading="lazy"></p></blockquote><p>选择更大的T或G都有助于取得更好的结果。值得注意的是，小T和G仍然优于VDSR</p><h4 id="Comparisons-with-state-of-the-arts"><a href="#Comparisons-with-state-of-the-arts" class="headerlink" title="Comparisons with state-of-the-arts"></a>Comparisons with state-of-the-arts</h4><blockquote><p><img src="http://sotavision.cn/showdoc/server/../Public/Uploads/2020-03-11/5e69077a67a19.png" alt="" loading="lazy"></p></blockquote><h4 id="Results-with-BI-degradation-model"><a href="#Results-with-BI-degradation-model" class="headerlink" title="Results with BI degradation model"></a>Results with BI degradation model</h4><blockquote><p><img src="http://sotavision.cn/showdoc/server/../Public/Uploads/2020-03-11/5e6909a18b73d.png" alt="" loading="lazy"><br>这里没用放RCAN和RDN的数据，实际上SRFBN没达到SOTA，现在最好的还是RCAN</p></blockquote><h4 id="Results-with-BD-and-DN-degradation-models"><a href="#Results-with-BD-and-DN-degradation-models" class="headerlink" title="Results with BD and DN degradation models"></a>Results with BD and DN degradation models</h4><blockquote><p><img src="http://sotavision.cn/showdoc/server/../Public/Uploads/2020-03-11/5e6909ac81cd8.png" alt="" loading="lazy"></p></blockquote><hr><h3 id="Conclusions"><a href="#Conclusions" class="headerlink" title="Conclusions"></a>Conclusions</h3><p>本文提出了一种新的图像SR网络-超分辨率反馈网络(SRFBN)，通过增强高层次的图像表示来忠实地重建SR图像。网络中的反馈块(FB)可以有效地处理反馈信息流和特征重用。此外，还提出了一种curriculum学习策略，使网络能够很好地适应复杂退化模型破坏低分辨率图像的复杂任务。综合实验结果表明，所提出的SRFBN能以极小的参数提供与现有方法相比的比较或更好的性能。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;https://cdn.jsdelivr.net/npm/aplayer@1.10/dist/APlayer.min.css&quot;&gt;&lt;script 
      
    
    </summary>
    
    
      <category term="论文笔记" scheme="http://alexzou14.github.io/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="深度学习" scheme="http://alexzou14.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="笔记" scheme="http://alexzou14.github.io/tags/%E7%AC%94%E8%AE%B0/"/>
    
      <category term="超分辨率" scheme="http://alexzou14.github.io/tags/%E8%B6%85%E5%88%86%E8%BE%A8%E7%8E%87/"/>
    
  </entry>
  
  <entry>
    <title>Guided Super-Resolution as Pixel-to-Pixel Transformation论文阅读笔记</title>
    <link href="http://alexzou14.github.io/2020/04/08/GuideSR/"/>
    <id>http://alexzou14.github.io/2020/04/08/GuideSR/</id>
    <published>2020-04-08T03:11:46.000Z</published>
    <updated>2020-04-08T03:17:26.000Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="https://cdn.jsdelivr.net/npm/aplayer@1.10/dist/APlayer.min.css"><script src="https://cdn.jsdelivr.net/npm/aplayer@1.10/dist/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="https://cdn.jsdelivr.net/npm/meting@1.2/dist/Meting.min.js"></script><h3 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h3><p>本文提出了引导超分的概念。输入是某种模态的低分辨率图像数据，如用ToF相机获取的深度图像。目标输出是输入图像的高分辨率版本。其中还有一个概念叫做Guided Image——引导图像，是输入图像在另一种模态下的高分辨率图像，如使用传统相机获取的深度图像。传统建模的思想是：将低分辨率图像进行上采样，从引导图像中获取高频信息补充到上采样后的输入图像。本文作者利用了机器学习的方法将这个问题转化成了引导图像到输入图像模态的pixel-to-pixel的映射，和风格迁移有着很大的相似之处。Pixel-to-pixel mapping是通过多层感知机进行的实现，通过最小化源图像和下采样目标图像之间的差异来学习其权重，即为我们所要求解的映射。本文提出的算法只需要对映射函数进行正则化，避免了对输出结果的直接处理，而且是无监督学习的算法。</p><hr><h3 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h3><p>很多计算机视觉任务都可以看作引导超分辨率的实例，例如，环境测绘，机器人视觉等等。引导超分辨率可以看成输入一张低分辨率图，再出入一张相似的高清图来还原目标图像得到高分辨率图。如下图所示：</p><blockquote><p><img src="http://sotavision.cn/showdoc/server/../Public/Uploads/2020-03-23/5e7815665ee78.png" alt="" loading="lazy"></p></blockquote><p>换一个角度看，引导超分辨又可以看作是引导滤波器的推广形式，引导滤波器将源图像映射到目标图像  通过计算每个像素处的相同大小，依赖于源和引导图像中的局部邻域，从而得到源图像的高清版本。作者提出了另一种关于超分辨率的解释，源图像和引导图像的作用被交换，即从一个图像的像素级映射 到另一个而不改变分辨率，并通过要求其下采样版本与源映像匹配来约束输出。<br>文中认为，这种导超分辨率的观点有两个非常实际的优点：</p><ul><li>从所需分辨率开始，只使用1×1个核，不同的输入位置不会混合，这避免了模糊。</li><li>通过对所有像素使用相同的映射函数，并在其参数之前放置收缩，这样就会得到不需要正则化输出图像的适定问题</li></ul><p>本文的贡献是</p><ul><li>一种新的引导超分辨率公式，即无监督学习从像素到像素的转换受低分辨率光源限制。</li><li>我们对两个任务进行了实验：深度图的超分辨率和树高图的超分辨率。它们表明我们的方法在高上采样因子上明显优于其他的超分辨率方法</li></ul><hr><h3 id="Related-work"><a href="#Related-work" class="headerlink" title="Related work"></a>Related work</h3><h4 id="Guided-filtering"><a href="#Guided-filtering" class="headerlink" title="Guided filtering"></a>Guided filtering</h4><p>关于引导滤波，一般的原理是通过应用一个滤波器来增强源图像，该滤波器的输出不仅取决于源图像的局部邻域，而且还取决于从引导图像中的相同邻域导出的权重。引导滤波已经被用于各种各样的图像处理应用中，从去噪或着色等低级任务一直到立体匹配等。</p><h4 id="Guided-super-resolution"><a href="#Guided-super-resolution" class="headerlink" title="Guided super-resolution"></a>Guided super-resolution</h4><p>对于超分辨深度以及诸如色调映射和图像着色等低层操作，已经探索了将引导滤波扩展到超分辨问题。这些方法中有基于上述局部滤波原理的局部方法和将上采样任务描述为全局能量最小化的全局方法。</p><h4 id="Learned-guided-super-resolution"><a href="#Learned-guided-super-resolution" class="headerlink" title="Learned guided super-resolution"></a>Learned guided super-resolution</h4><p>这些方法迄今为止都是非监督的。还有一系列的工作，从例子中学习如何向上采样源图像同时将高频细节从引导图像传输到目标输出上。<br>这中数据处理方法的优点有：从真实图像数据中学习如何最佳融合源图像和引导图像可能比手工启发式方法更好。<br>与所有监督学习的缺点一样：</p><ol><li>必须获取足够数量的训练数据</li><li>通过这样设计的算法可能在训练数据上过拟合，但是在真实场景下表现不好</li></ol><p>高分辨率“引导”图像通过标准语义分割网络生成“目标”分割映射，使用一个损失函数鼓励目标具有相同的目标。 标签分布作为低分辨率的源图。</p><hr><h3 id="Proposed-Method"><a href="#Proposed-Method" class="headerlink" title="Proposed Method"></a>Proposed Method</h3><blockquote><p><img src="http://sotavision.cn/showdoc/server/../Public/Uploads/2020-03-23/5e7825da3d75f.png" alt="" loading="lazy"></p></blockquote><h4 id="Notation-and-Preliminaries"><a href="#Notation-and-Preliminaries" class="headerlink" title="Notation and Preliminaries"></a>Notation and Preliminaries</h4><p>首先对符号做出如下的定义：Source Image记作S（S.size=M×M），Guided Image记作G（G.size=N×N×C），目标图像记作T（T.size=N×N）。N和M之间的数量关系是通过尺度为D的上采样算子决定的，即N=D·M。S中的第m个像素记作\(S_m\),\(S_m\)是由T中的D×D范围内的block即b(m)经过某种非线性均值方式获得到的，即<br>\[ s_m=\dfrac{1}{D^2} \sum_{n\in b(m)}t_n = {\left\langle t_n \right\rangle}_{b(m)} \]<br>目标是Given S and G，esitimate\(\hat{T}\)</p><h4 id="Proposed-Solution"><a href="#Proposed-Solution" class="headerlink" title="Proposed Solution"></a>Proposed Solution</h4><p>设Guided Image和Target Image之间的映射为参数记作\(\theta\)的函数\(f_{\theta}\), 二者满足如下的关系\(\hat{t}_ n =f_{\theta}(g_n)\)，s和t之间的距离度量采用1-Norm，目标函数建立如下:<br>\[ \hat{\theta}=\arg\min_{\theta}\sum_{m}\left|{s_m - \left\langle{f_{\theta}(g_n)}\right\rangle_{b(m)}}\right| \]<br>这里是一个很典型的参数估计问题：\(f_{\theta}\)的形式为多层卷积感知器，参数记作\(\theta\) 。为了避免ill-posed problem求解出过拟合的参数\(\theta\)，作者引入了对\(\theta\)的 2-Norm 正则项，如下所示。参数\(\lambda\)控制了正则化的强度。<br>\[ \hat{\theta}=\arg\min_{\theta}\sum_{m}\left|{s_m - \left\langle{f_{\theta}(g_n)}\right\rangle_{b(m)}}\right|+\lambda\left|{\theta}\right|^2 \]<br>最后为了避免由于色彩、角点等其它特征造成的映射歧义，\(f_{\theta}\)函数引入了坐标变量 \(x_{n}\)。坐标和图像数据分别进行训练，将最终的结果merge到一起来进行参数优化。形式如下式所示:<br>\[ \hat{\theta}=\arg\min_{\theta}\sum_{m}\left|{s_m - \left\langle{f_{\theta}(g_n,x_n)}\right\rangle_{b(m)}}\right|+\lambda\left|{\theta}\right|^2 \]<br>通过这种架构，还可以通过设置各个超参数λg(=0.001)，λx(=0.0001)，λhead(=0.0001)来不同地对每个分支进行处理。目标函数采用了梯度下降法进行求解。</p><blockquote><p><img src="http://sotavision.cn/showdoc/server/../Public/Uploads/2020-03-23/5e782eec854c0.png" alt="" loading="lazy"></p></blockquote><hr><h3 id="Experimental-Results"><a href="#Experimental-Results" class="headerlink" title="Experimental Results"></a>Experimental Results</h3><p>论文代码：<a href="https://github.com/riccardodelutio/PixTransform" target="_blank" rel="noopener">https://github.com/riccardodelutio/PixTransform</a></p><h4 id="Evaluation-Settings"><a href="#Evaluation-Settings" class="headerlink" title="Evaluation Settings"></a>Evaluation Settings</h4><p>在所有实验中，作者将目标分辨率设置为256×256像素。 在不同的上采样因子（即×4，×8，×16和×32）下评估算法，分别对应于64×64、32×32、16×16和8×8的源分辨率。 所对比的方法：BiCubic、Guided Filter、Fast Bilateral Solver、Static-Dynamic Solver、MSG-Net。对比指标：Percentage of Bad Pixels（PBP），均方误差，平均绝对误差。<br>\[PBP_{\delta} = \dfrac{1}{N^2}\sum_n[|\hat{t_n}-t_n|&gt;\delta]\]<br>这里没有使用SSIM和PSNR的原因，个人分析认为：Guided SR不同于重建，因此不存在质量评价的这一标准，只需要考量和Ground Truth之间的差异即可。</p><h4 id="Analysis"><a href="#Analysis" class="headerlink" title="Analysis"></a>Analysis</h4><blockquote><p><img src="http://sotavision.cn/showdoc/server/../Public/Uploads/2020-03-23/5e7830c4e71b9.png" alt="" loading="lazy"></p></blockquote><p>这里展示了不同的映射函数的结果</p><h4 id="Comparisons-with-state-of-the-arts"><a href="#Comparisons-with-state-of-the-arts" class="headerlink" title="Comparisons with state-of-the-arts"></a>Comparisons with state-of-the-arts</h4><p><img src="http://sotavision.cn/showdoc/server/../Public/Uploads/2020-03-23/5e78312f76337.png" alt="" loading="lazy"></p><p>由上面显示的结果可看出，这个方法在处理分割图像超分上是目前最优的</p><hr><p>###Conclusions</p><ol><li>提出了一种新的、无监督的引导超分辨率方法。关键思想是将问题看作是高分辨率引导图像向低分辨率源图像域的像素级变换。</li><li>即使在高上采样因子上该方法也能够恢复非常精细的结构和非常锋利的边缘</li><li>利用了机器学习的方法将这个问题转化成了引导图像到输入图像模态的pixel-to-pixel的映射</li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;https://cdn.jsdelivr.net/npm/aplayer@1.10/dist/APlayer.min.css&quot;&gt;&lt;script 
      
    
    </summary>
    
    
      <category term="论文笔记" scheme="http://alexzou14.github.io/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="深度学习" scheme="http://alexzou14.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="笔记" scheme="http://alexzou14.github.io/tags/%E7%AC%94%E8%AE%B0/"/>
    
      <category term="超分辨率" scheme="http://alexzou14.github.io/tags/%E8%B6%85%E5%88%86%E8%BE%A8%E7%8E%87/"/>
    
  </entry>
  
  <entry>
    <title>Deep Laplacian Pyramid Networks for Fast and Accurate Super-Resolution论文阅读笔记</title>
    <link href="http://alexzou14.github.io/2020/04/08/LapSRN/"/>
    <id>http://alexzou14.github.io/2020/04/08/LapSRN/</id>
    <published>2020-04-08T03:04:02.000Z</published>
    <updated>2020-04-08T03:10:11.000Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="https://cdn.jsdelivr.net/npm/aplayer@1.10/dist/APlayer.min.css"><script src="https://cdn.jsdelivr.net/npm/aplayer@1.10/dist/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="https://cdn.jsdelivr.net/npm/meting@1.2/dist/Meting.min.js"></script><h3 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h3><p>当时卷积神经网络经常被用到对单图像超分辨率的高质量重建中。在这篇论文中，作者提出了Laplacian金字塔超分辨率网络（LapSRN）来逐步重构高分辨率图像的子频带残差（sub-band residuals）。在每个金字塔的层，我们将粗分辨率特征图作为输入，预测高频残差（high-frequency residuals），并使用反卷积（transposed convolutions）来进行向上采样到finer level。LapSRN方法并没有将双三次差值（bicubic interpolation）作为预处理步骤，从而减小了计算的复杂性。作者使用了一个强大的Charbonnier损失函数对所提出的LapSRN进行了深入的监督，实现了高质量的重建。我们的网络通过渐进的重构，在一个前馈的过程（feed-forward）中产生了多尺度的预测，从而促进了资源感知(resorce-aware)的应用。对基准数据集进行了大量的定量和定性评估，结果表明，该算法在速度和精度方面优于最先进的方法。</p><hr><h3 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h3><p>当时CNN在单一图像超分问题上取得了巨大成功，并不断的发展。有很多方法被提出：</p><blockquote><p><img src="http://sotavision.cn/showdoc/server/../Public/Uploads/2020-04-08/5e8d2cb679ad6.png" alt="" loading="lazy"></p></blockquote><p>作者在这部分说明了当时CNN方法在SR问题上存在三个主要问题：</p><ol><li>当时的方法使用了预先定义的上采样操作，将输入图像提升到所需分辨率大小再输入进网络，这样增加了很多不必要的计算成本。</li><li>当时的方法在优化网络的时候，采用L2 loss function，这样会导致图像过于平滑</li><li>大多数方法都是一个上采样的步骤重建图像，这增加了训练大尺度因子的难度。</li></ol><p>本文中的LapSRN与其他方法的区别：</p><ol><li>直接用LR提取特征，并用Charbonnier的损失函数</li><li>速度上，作者的方法比一般的SRCNN要快，和FSRCNN相近但是质量要更高</li><li>渐进重构：在一条前馈通道中可以生成多个SR预测，更好的应用在不同的场景中。</li></ol><hr><h3 id="Related-work"><a href="#Related-work" class="headerlink" title="Related work"></a>Related work</h3><h4 id="SR-based-on-internal-databases"><a href="#SR-based-on-internal-databases" class="headerlink" title="SR based on internal databases"></a>SR based on internal databases</h4><p>一些方法利用自然图像中的自相似性属性，基于低分辨率输入图像的尺度空间金字塔构造LR-HR补丁对。然而内部数据库包含的相关训练补丁比外部数据库要多，LR-HR补丁对的数量可能不足以覆盖图像中较大的纹理变化。Singh等人将补丁分解成定向频率子带（directional frequency sub-bands ），并独立地在每个子带金字塔（sub-band pyramid）中确定更好的匹配。</p><h4 id="SR-based-on-external-databases"><a href="#SR-based-on-external-databases" class="headerlink" title="SR based on external databases"></a>SR based on external databases</h4><p>大量的SR方法通过从外部数据库中收集的图像对学习LR-HR映射，使用监督学习算法。不是直接在整个数据库上对复杂的补丁空间进行建模，而是通过k均值、稀疏字典或随机森林来划分图像数据库，并学习每个集群的局部线性回归。</p><h4 id="Convolutional-neural-networks-based-SR"><a href="#Convolutional-neural-networks-based-SR" class="headerlink" title="Convolutional neural networks based SR"></a>Convolutional neural networks based SR</h4><p>与在补丁空间中对LR-HR映射建模不同的是，SRCNN联合优化了所有的步骤，并在图像空间中学习了非线性映射。VDSR网络通过将网络深度从3个层增加到20个卷积层，显示了对SRCNN的显著改进。为了促使训练出一个更快收敛速度的更深的模型，VDSR训练网络来预测剩余的值，而不是实际的像素值。Wang等人将稀疏编码的领域知识与深度CNN结合起来，并训练一个级联网络（SCN），逐步将图像提升到所需的比例因子。Kim等人提出了一个具有深度递归层（DRCN）的浅层网络，以减少参数的数量。<br>LapSRN与上述方法不同：</p><ol><li>加入了残差和反卷积组成的上采样滤波器，有效的抑制了双三插值导致的重构伪影，降低计算复杂度。</li><li>利用Charbonnier损失函数提高重建精度</li><li>渐进重构的方法可以适用于不同的比例因子</li></ol><hr><h3 id="Proposed-Method"><a href="#Proposed-Method" class="headerlink" title="Proposed Method"></a>Proposed Method</h3><h4 id="Network-structure"><a href="#Network-structure" class="headerlink" title="Network structure"></a>Network structure</h4><p>网络结构如下图所示：</p><blockquote><p><img src="http://sotavision.cn/showdoc/server/../Public/Uploads/2020-04-08/5e8d3168dec26.png" alt="" loading="lazy"></p></blockquote><p><strong>Feature extraction branch</strong>：<br>通过stack convolution来获取非线性特征映射<br><strong>Image reconstruction branch</strong>：<br>在每一个pyramid level，最后加上deconv来提升图像的2x分辨率<br><strong>参数共享</strong><br>本文网络在两个地方进行参数共享，减少了参数量<br>1.在各个pyramid level之间参数共享， 称之为Recursive block</p><p>因为laplacian pyramid是在x2的基础上得到x4，由于各个level中的结构相似性，因此在各个level，参数得以共享<br>形式如下：</p><blockquote><p><img src="http://sotavision.cn/showdoc/server/../Public/Uploads/2020-04-08/5e8d319ef0847.png" alt="" loading="lazy"></p></blockquote><p>2.每个pyramid level之中参数共享</p><p>受DRCN和DRRN启发，作者在每个pyramid level中进行参数共享，如下图:</p><blockquote><p><img src="http://sotavision.cn/showdoc/server/../Public/Uploads/2020-04-08/5e8d326d42b45.png" alt="" loading="lazy"></p></blockquote><h4 id="Loss-function"><a href="#Loss-function" class="headerlink" title="Loss function"></a>Loss function</h4><p>本文认为相同的LR patch 可能有多种corresponding HR patches，而L2范数并不能capture the underlying multi-modal distributions of HR patches. 因此L2范数重建出的图像往往过平滑。<br>本文提出了一种抗噪性强的loss functions：</p><div>\[ L(\hat{y},y;\theta)=\dfrac{1}{N}\displaystyle\sum_{i=1}^N \sum_{s=1}^L \rho(\hat{y}_s^{(i)}-y_s^{(i)})=\dfrac{1}{N}\displaystyle\sum_{i=1}^N \sum_{s=1}^L \rho((\hat{y}_s^{(i)}-x_s^{(i)})-r_s^{(i)}) \]</div><p>其中\(\rho(x)=\sqrt{x^2+\varepsilon ^2}\)</p><hr><h3 id="Experiment"><a href="#Experiment" class="headerlink" title="Experiment"></a>Experiment</h3><h4 id="Experiments-Details"><a href="#Experiments-Details" class="headerlink" title="Experiments Details"></a>Experiments Details</h4><p>论文具体细节待复现。</p><h4 id="Model-analysis"><a href="#Model-analysis" class="headerlink" title="Model analysis"></a>Model analysis</h4><p><strong>Residual learning</strong>：</p><blockquote><p><img src="http://sotavision.cn/showdoc/server/../Public/Uploads/2020-04-08/5e8d3d9e29c77.png" alt="" loading="lazy"></p></blockquote><p>残差学习可以提高网络的收敛速度，是PSNR更稳定。<br><strong>Loss function</strong>：</p><blockquote><p><img src="http://sotavision.cn/showdoc/server/../Public/Uploads/2020-04-08/5e8d3dea66dc8.png" alt="" loading="lazy"></p></blockquote><p>L1 loss比L2 loss 产生的图像更加的清晰。<br><strong>Network depth</strong>：</p><blockquote><p><img src="http://sotavision.cn/showdoc/server/../Public/Uploads/2020-04-08/5e8d3e3107068.png" alt="" loading="lazy"></p></blockquote><p>当网络深度d=10的时候速度最快，效果最好。</p><h4 id="Comparisons-with-state-of-the-arts"><a href="#Comparisons-with-state-of-the-arts" class="headerlink" title="Comparisons with state-of-the-arts"></a>Comparisons with state-of-the-arts</h4><p>各个超分辨率算法实验结果：</p><blockquote><p><img src="http://sotavision.cn/showdoc/server/../Public/Uploads/2020-04-08/5e8d3e8ab7b3c.png" alt="" loading="lazy"><br><img src="http://sotavision.cn/showdoc/server/../Public/Uploads/2020-04-08/5e8d3ec48f9bc.png" alt="" loading="lazy"></p></blockquote><hr><h3 id="Conclusions"><a href="#Conclusions" class="headerlink" title="Conclusions"></a>Conclusions</h3><ol><li>本文是通过将低分辨率图像直接作为输入到网络中，通过逐级放大，在减少计算量的同时，也有效的提高了精度</li><li>提出了一种鲁棒的loss function, robust Charbonnier loss function.</li><li>对各个金字塔的level之间和每个level之内，通过recursive进行参数共享</li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;https://cdn.jsdelivr.net/npm/aplayer@1.10/dist/APlayer.min.css&quot;&gt;&lt;script 
      
    
    </summary>
    
    
      <category term="论文笔记" scheme="http://alexzou14.github.io/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="深度学习" scheme="http://alexzou14.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="笔记" scheme="http://alexzou14.github.io/tags/%E7%AC%94%E8%AE%B0/"/>
    
      <category term="超分辨率" scheme="http://alexzou14.github.io/tags/%E8%B6%85%E5%88%86%E8%BE%A8%E7%8E%87/"/>
    
  </entry>
  
</feed>
