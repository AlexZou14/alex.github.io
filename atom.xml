<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>秩同道合的小站</title>
  
  <subtitle>寻找志趣相投的伙伴！</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://alexzou14.github.io/"/>
  <updated>2020-04-11T10:41:05.303Z</updated>
  <id>http://alexzou14.github.io/</id>
  
  <author>
    <name>秩同道合</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Toward Real-World Single Image Super-Resolution:A New Benchmark and A New Model论文阅读笔记</title>
    <link href="http://alexzou14.github.io/2020/04/11/RealSR/"/>
    <id>http://alexzou14.github.io/2020/04/11/RealSR/</id>
    <published>2020-04-11T10:34:11.000Z</published>
    <updated>2020-04-11T10:41:05.303Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="https://cdn.jsdelivr.net/npm/aplayer@1.10/dist/APlayer.min.css"><script src="https://cdn.jsdelivr.net/npm/aplayer@1.10/dist/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="https://cdn.jsdelivr.net/npm/meting@1.2/dist/Meting.min.js"></script><h3 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h3><p>目前大多数基于学习的单图像超分辨率算法（Single Image Super Resolution,SISR）大多是基于模拟数据集（例如：对高分辨率图像（HR）进行Bicubic 降采样，或者加入高斯白噪声）。然而，真实低分辨率图像（LR）的降质过程往往是非常复杂而且不可知，因此在模拟数据集上训练的 SISR 算法在真实场景下往往效果不佳。<br>本文介绍了自己的数据集来更真实的还原现实世界的场景，提出了一个基于拉普拉斯的核预测网络。</p><hr><h3 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h3><p>作者在这部分先说明了单一图像超分问题的不定性。现在的大多数方法都用模拟数据集来进行训练，所以现在大部分方法应用在现实场景中收到的效果不是特别好。作者通过调节焦距，在进行图像配对获得一组现实场景中的数据集。作者同时提出了一种Lap-KPN网络在现实场景中取得了比较好的结果。<br>本文主要贡献有：</p><ul><li><p>该论文提出了一个新的 Benchmark 数据集 RealSR dataset，它包含同场景下成对的 LR-HR 数据集，由变焦数码进行拍摄经过后期处理得到</p></li><li><p>由于真实图像降质核在数据集中并非完全一致，因此该论文提出了一个新的超分辨率算法LP-KPN：基于拉普拉斯金字塔的核预测网络。它能够有效地学习 per-pixel kernel（像素卷积核） 用于高分辨率图像的重建。</p></li></ul><hr><h3 id="Related-work"><a href="#Related-work" class="headerlink" title="Related work"></a>Related work</h3><h4 id="SISR-dataset"><a href="#SISR-dataset" class="headerlink" title="SISR dataset"></a>SISR dataset</h4><p>比较了当时使用比较广泛的数据集，如Set5、Set14、BSD300、Urban100、DIV2K等数据集。当下大多数网络训练过程中的LR图像都是通过对高分辨率图像（HR）进行Bicubic 降采样，或者加入高斯白噪声得到的。然后这些SISR模型在真实场景上恢复出来的HR图像比较差。</p><h4 id="Kernel-prediction-network"><a href="#Kernel-prediction-network" class="headerlink" title="Kernel prediction network"></a>Kernel prediction network</h4><p>作者介绍了KPN网络一开始是用来去噪的一个网络，由于现实场景下的低分辨率图像的模糊核是多样的，作者运用了相似的思想来恢复低分辨率图像。</p><hr><h3 id="Real-world-SISR-Dataset"><a href="#Real-world-SISR-Dataset" class="headerlink" title="Real-world SISR Dataset"></a>Real-world SISR Dataset</h3><h4 id="Image-formation-by-thin-lens"><a href="#Image-formation-by-thin-lens" class="headerlink" title="Image formation by thin lens"></a>Image formation by thin lens</h4><p>图像在相机生成的光学过程如下：<br><img src="http://sotavision.cn/showdoc/server/../Public/Uploads/2020-02-19/5e4d373365283.png" alt="" loading="lazy"><br>所以光学成像的表达式为：\({\dfrac{1}{f}}={\dfrac{1}{u}}+{\dfrac{1}{v}}\)<br>所以放大倍数可以表达为：\(M={\dfrac{h_2}{h_1}}={\dfrac{v}{u}}\)<br>所以当\(u \gg f\)时我们可以得到：\( h_2={\dfrac{f}{u-f}}\approx{\dfrac{f}{u}h} \)<br>所以可以通过调节焦距来获得到LR和HR图像</p><h4 id="Data-collection"><a href="#Data-collection" class="headerlink" title="Data collection"></a>Data collection</h4><p>使用了以上的相机来获得到图像：<br><img src="http://sotavision.cn/showdoc/server/../Public/Uploads/2020-02-19/5e4d3a520c86d.png" alt="" loading="lazy"></p><h4 id="Image-pair-registration"><a href="#Image-pair-registration" class="headerlink" title="Image pair registration"></a>Image pair registration</h4><p><img src="http://sotavision.cn/showdoc/server/../Public/Uploads/2020-02-19/5e4d3a84a9083.png" alt="" loading="lazy"><br>作者通过以上的对齐方式将获得到的图像整理成LR-HR的图像对制作成一个现实世界的训练集。</p><h3 id="Laplacian-Pyramid-based-Kernel-Prediction-Network"><a href="#Laplacian-Pyramid-based-Kernel-Prediction-Network" class="headerlink" title="Laplacian Pyramid based Kernel Prediction Network"></a>Laplacian Pyramid based Kernel Prediction Network</h3><p><img src="http://sotavision.cn/showdoc/server/../Public/Uploads/2020-02-19/5e4d3b89284c2.png" alt="" loading="lazy"><br>从图 1 看，网络一共分为2个部分：1.Backbone骨干网络，用于特征提取； 2.Per-pixel kernels at different level，处理不同拉普拉斯层级的解构图像，共有3层。对于每一个层，层级中的卷积对 Backbone 输出特征进行不同尺度缩放，然后提取特征输出 per-pixel kernel 特征矩阵，维度大小为\( H\times W\times (k\times k)\),其中\((k\times k)\)输出的卷积核尺寸。然后与对应的拉普拉斯金字塔解构图像进行如下的内积矩阵操作：<br>\[ I_H^P(i,j)=\left \langle {K(i,j),V(I_L^A(i,j))} \right \rangle \]<br>其中\( K(i,j) \)为per-pixel kernel 特征矩阵中\((i,j)\)位置的卷积核，\(V(I_L^A(i,j))\)为输入解构图像中，以\((i,j)\)位置的点位中心,\(k\times k\)大小的邻域。对于输入的每一个像素点，都有对应的卷积核单独对其邻域内的特征进行处理，所以 LP-KPN能够较好地应用于图像去噪，去模糊、超分辨率等任务，且效果较好。<br>LP-KPN 还利用 shuffle downsampling 和 shuffle upsampling，实现卷积特征空间分辨率上采样和下采样，其原理与 ESPCN 一致。在网络最前端进行4倍下采样，有效缓解了 输入图像尺寸与目标图像一致 而导致的计算消耗大的问题。 LP-KPN 输入图像为灰度图像，通过 shuffle downsampling 操作后，相当于将图像降采样成多张低分辨的灰度图像。</p><hr><h3 id="Experiment"><a href="#Experiment" class="headerlink" title="Experiment"></a>Experiment</h3><h4 id="Experiments-Details"><a href="#Experiments-Details" class="headerlink" title="Experiments Details"></a>Experiments Details</h4><p>具体实验细节可以参考：<a href="https://github.com/csjcai/RealSR" target="_blank" rel="noopener">https://github.com/csjcai/RealSR</a></p><h4 id="SISR-models-trained-on-RealSR-dataset"><a href="#SISR-models-trained-on-RealSR-dataset" class="headerlink" title="SISR models trained on RealSR dataset"></a>SISR models trained on RealSR dataset</h4><p>论文对比了不同的 SISR 模型在不同的数据集上进行训练，最终在真实图像测试集上进行测试的结果。可以看到在 RealSR 数据集上，相同模型能够生成视觉效果更好的图像，图像中的伪影和噪声更少，且边缘纹理更清晰。这说明了RealSR 数据集的有效性。<br><img src="http://sotavision.cn/showdoc/server/../Public/Uploads/2020-02-19/5e4d410bc9e0d.png" alt="" loading="lazy"></p><h4 id="Comparisons-with-state-of-the-arts"><a href="#Comparisons-with-state-of-the-arts" class="headerlink" title="Comparisons with state-of-the-arts"></a>Comparisons with state-of-the-arts</h4><p>论文对 LP-KPN方法的有效性进行了验证，LP-KPN 可以取得比 RCAN 相近的性能表现。<br><img src="http://sotavision.cn/showdoc/server/../Public/Uploads/2020-02-19/5e4d415614fdd.png" alt="" loading="lazy"></p><hr><h3 id="Conclusions"><a href="#Conclusions" class="headerlink" title="Conclusions"></a>Conclusions</h3><ul><li><p>该论文提出了一个新的 Benchmark 数据集 RealSR dataset，它包含同场景下成对的 LR-HR 数据集，由变焦数码进行拍摄经过后期处理得到</p></li><li><p>由于真实图像降质核在数据集中并非完全一致，因此该论文提出了一个新的超分辨率算法LP-KPN：基于拉普拉斯金字塔的核预测网络。它能够有效地学习 per-pixel kernel（像素卷积核） 用于高分辨率图像的重建。</p></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;https://cdn.jsdelivr.net/npm/aplayer@1.10/dist/APlayer.min.css&quot;&gt;&lt;script 
      
    
    </summary>
    
    
      <category term="论文笔记" scheme="http://alexzou14.github.io/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="深度学习" scheme="http://alexzou14.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="笔记" scheme="http://alexzou14.github.io/tags/%E7%AC%94%E8%AE%B0/"/>
    
      <category term="超分辨率" scheme="http://alexzou14.github.io/tags/%E8%B6%85%E5%88%86%E8%BE%A8%E7%8E%87/"/>
    
  </entry>
  
  <entry>
    <title>Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network论文阅读笔记</title>
    <link href="http://alexzou14.github.io/2020/04/11/SRGAN/"/>
    <id>http://alexzou14.github.io/2020/04/11/SRGAN/</id>
    <published>2020-04-11T10:04:44.000Z</published>
    <updated>2020-04-11T10:34:10.160Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="https://cdn.jsdelivr.net/npm/aplayer@1.10/dist/APlayer.min.css"><script src="https://cdn.jsdelivr.net/npm/aplayer@1.10/dist/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="https://cdn.jsdelivr.net/npm/meting@1.2/dist/Meting.min.js"></script><h3 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h3><p>  在超分辨率领域飞速发展的时候，仍然有一个核心问题没有解决：当我们在用大的上采样因子实现图像恢复时，我们如何恢复更精细的纹理细节？基于优化的超分辨率方法的行为主要是由目标函数的选择驱动的。最近的工作主要集中在最小化方均根误差（MSE）上。由此产生的估计具有高峰值信噪比（PSNR），但它们通常缺乏高频细节，并且在感觉上不能令人满意。<br>这篇文章提出了以下的方法来处理这一问题：</p><ul><li>更改损失函数，将传统的MSE换成：对抗损失（perceptual loss）和内容损失（content loss）</li><li>引入对抗生成网络，将传统的像素空间的Content Loss转换为对抗性质的相似性。</li><li>引入深度残差网络，来提取图片中的细节纹理。</li></ul><hr><h3 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h3><p>SR问题有一个现状：就是尤其在大的采样因子的条件下，重构的图像通常都缺少细节纹理。因为他们是给予像素上的差异来优化。<br>SRGAN实现思路：</p><ul><li>采用具有skip connection的深度残差网络（ResNet），MSE作为content loss</li><li>采用VGG网络提取特征，并与Discriminator结合，来定义新的perceptual loss</li></ul><hr><h3 id="Related-work"><a href="#Related-work" class="headerlink" title="Related work"></a>Related work</h3><h4 id="Image-super-resolution"><a href="#Image-super-resolution" class="headerlink" title="Image super-resolution"></a>Image super-resolution</h4><p>基于预测的恢复方法：（插值）比如说线性差值，双三次差值，Lanczos滤波（注：其实就是基于插值的上采样方法），速度非常快，但是会产生关于平滑的图像，导致纹理边缘信息丢失。更加强大的方法是想要建立低分辨率图像到高分辨率图像的复杂映射，并且依赖于训练数据：</p><ul><li>利用图像中不同尺度的补丁来驱动SR。</li><li>自我字典通过延伸来进行小变换和形状变化。</li><li>卷积稀疏编码方法，通过处理整个图像而不是重叠补丁来提高一致性。</li></ul><p>为了重建真实的纹理细节，同时避免过度伪影，Tai等人将基于先验的梯度曲线的边缘定向SR算法与基于学习的细节合成的益处相结合。提出了一种多尺度字典来捕获不同尺度的相似图像块的冗余。<br>邻域嵌入方法通过在低维流形中找到类似的LR训练补片并将它们相应的HR补片组合用于重建来上采样LRimage补片。</p><h4 id="Design-of-convolutional-neural-networks"><a href="#Design-of-convolutional-neural-networks" class="headerlink" title="Design of convolutional neural networks"></a>Design of convolutional neural networks</h4><p>卷积神经网络的引入给SR问题带来了新的方法：</p><ul><li>SRCNN将稀疏表示编码到神经网络架构中，使用双三次插值来放大输入图像并对端到端的三层深度卷积网络进行训练，以实现最先进的SR性能。</li><li>随后，表明使网络能够直接学习上采样滤波器可以进一步提高准确度和速度的性能。凭借其深度递归卷积网络（DRCN），Kim等人提出了一种高性能的架构，允许长距离像素依赖，同时保持模型参数的数量很少。</li><li>Johnson等人的着作，和布鲁纳等人。 依靠更接近感知相似性的损失函数来恢复视觉上更有说服力的HR图像。</li></ul><h4 id="Contribution"><a href="#Contribution" class="headerlink" title="Contribution"></a>Contribution</h4><ul><li>使用ResNet优化了过去的MSE损失。</li><li>提出SRGAN，这是一个基于GAN的网络，为新的感知损失进行了优化。在这里，我们用基于VGG网络的特征映射的损失计算替代基于MSE的内容丢失[49]，这对于像素空间的变化更加不变。</li><li>通过对来自三个公共基准数据集的图像进行广泛的平均意见得分（MOS）测试来证实，SRGAN是一种新的技术水平，通过大幅度的边缘，用于估计具有高放大因子的照片般逼真的重构图像。</li></ul><hr><h3 id="Proposed-Method"><a href="#Proposed-Method" class="headerlink" title="Proposed Method"></a>Proposed Method</h3><h4 id="Network-structure"><a href="#Network-structure" class="headerlink" title="Network structure"></a>Network structure</h4><p>网络结构如下图所示：</p><blockquote><p><img src="http://sotavision.cn/showdoc/server/../Public/Uploads/2020-02-24/5e5377a6492d0.png" alt="" loading="lazy"></p></blockquote><p>这里生成网络运用了SRResNet，对抗网络用了VGG网络。<br>生成网络优化的参数表达式为：</p><div>    \[ \hat{\theta}_G=\displaystyle\argmin_{\theta_G} \frac{1}{N} \sum_{n=1}^{N} l^{SR}(G_{\theta_G}(I_n^{LR}),I_n^{HR}) \]</div><p>其中\(\hat{\theta}_G\)是指生成网络的参数，\({I^{LR}}\)是指\({I^{HR}}\)通过高斯滤波器降置得到的。<br>我们进一步定义了一个鉴别器网络\(D_{\theta_D}\)，我们与\(G_{\theta_G}\)交替优化，以解决以下问题：</p><div>    \[ \min_{G_{\theta_G}}\max_{D_{\theta_D}}{E_{I^{HR}\sim {p_{train}(I^{HR})}}}[\log{D_{\theta_D}}(I^{HR})]+{E_{I^{LR}\sim {p_{G}(I^{LR})}}}[\log(1-{D_{\theta_D}}(G_{\theta_G}(I^{LR})))] \]</div><p>这里就相当于让LR图像经过生成网络得到一张SR图像，用SR图像进过鉴别器来判别这个图像是否和HR图像接近，如果否则仅需通过生成器生成图像，如果是则输出SR图像。所以这里最小化生成器的损失函数，最大化判别器的损失函数。</p><h4 id="Loss-function"><a href="#Loss-function" class="headerlink" title="Loss function"></a>Loss function</h4><ul><li>Perceptual loss function</li></ul><blockquote><p>\[ l^{SR}=l_X^{SR}+10^{-3}l_{Gen}^{SR} \]<br>其中\(l_X^{SR}\)是内容损失函数，\(l_{Gen}^{SR}\)是对抗损失函数。这里是用上面的损失函数来使得生成器的生成的图片接近原图</p></blockquote><ul><li>Content loss</li></ul><blockquote><p>这里可以使用两个内容损失函数：<br>MSE loss: </p></blockquote><div>    \[l_{MSE}^{SR}=\dfrac{1}{r^2WH}\displaystyle\sum_{x=1}^{rW}\sum_{y=1}^{rH}(I_{x,y}^{HR}-G_{\theta_G}(I^{LR})_{x,y})^2\]</div><p>VGG loss:</p><div>    \[l_{VGG/i,j}^{SR}=\dfrac{1}{W_{i,j}H_{i,j}}\displaystyle\sum_{x=1}^{W_{i,j}}\sum_{y=1}^{H_{i,j}}(\phi_{i,j}(I^{HR})_{x,y}-\phi_{i,j}(G_{\theta_G}(I^{LR}))_{x,y})^2\]</div><ul><li>Adversarial loss</li></ul><blockquote><p>\[l_{Gen}^{SR}=\displaystyle\sum_{n=1}^{N}-\log{D_{\theta_D}(G_{\theta_G}(I^{LR}))} \]<br>这里用优化上面的函数来替代之前提出的对抗损失函数来优化对抗网络，使鉴别器的鉴别的效果最大化</p></blockquote><hr><h3 id="Experiment"><a href="#Experiment" class="headerlink" title="Experiment"></a>Experiment</h3><h4 id="Network-analysis"><a href="#Network-analysis" class="headerlink" title="Network analysis"></a>Network analysis</h4><h5 id="Investigation-of-content-loss"><a href="#Investigation-of-content-loss" class="headerlink" title="Investigation of content loss"></a>Investigation of content loss</h5><blockquote><p><img src="http://sotavision.cn/showdoc/server/../Public/Uploads/2020-02-24/5e539ef6a1f4f.png" alt="" loading="lazy"></p></blockquote><p>SRGAN网络可以极大的提升生成图像的感知质量，都可以取得较高的MOS分数。</p><h4 id="Performance-PSNR-time-vs-network-depth"><a href="#Performance-PSNR-time-vs-network-depth" class="headerlink" title="Performance (PSNR/time) vs. network depth"></a>Performance (PSNR/time) vs. network depth</h4><p>文中研究了网络深度和PSNR结果值及网络预测时间的关系,同时还实验了skip-connection的影响,其中网络的深度是通过调整残差快的数量来实现的,注意网络最终选取的数量为16.结果图如下:</p><blockquote><p><img src="http://sotavision.cn/showdoc/server/../Public/Uploads/2020-02-24/5e53a181eea4f.png" alt="" loading="lazy"></p></blockquote><ul><li>注意实验中以PSNR值来衡量生成图像质量,从图中可以看出随着深度增加PSNR值逐渐增大,但是增加速度逐渐变慢,这也证实了增加网络深度可以提升生成图像质量.</li><li>左图中的蓝线与红线的差别就是有无skip-connection,这也证实了其必要性.</li><li>预测时间随深度增加线性增长,skip-connection并不影响预测时间.</li></ul><h4 id="Mean-opinion-score-MOS-testing"><a href="#Mean-opinion-score-MOS-testing" class="headerlink" title="Mean opinion score (MOS) testing"></a>Mean opinion score (MOS) testing</h4><p>文中在Set5,Set14和BSD100三种数据集上对各种方法找人类受试者进行了打分,其中分值越大表示效果越好.结果如下:</p><ul><li>MOS:平均主观得分,此标准更符合人类的感知</li><li>Set5数据集分辨率较低,在做放大之后效果区分并不明显,不过从分辨率最高的BSD100数据集上的结果可以明显看出SRGAN的效果最好.<blockquote><p><img src="http://sotavision.cn/showdoc/server/../Public/Uploads/2020-02-24/5e53a27883a8e.png" alt="" loading="lazy"></p></blockquote></li></ul><h4 id="Comparisons-with-state-of-the-arts"><a href="#Comparisons-with-state-of-the-arts" class="headerlink" title="Comparisons with state-of-the-arts"></a>Comparisons with state-of-the-arts</h4><blockquote><p><img src="http://sotavision.cn/showdoc/server/../Public/Uploads/2020-02-24/5e53a2b980f5c.png" alt="" loading="lazy"></p></blockquote><p>由上图可以得知，PSNR和SSIM数值虽然不是最高，但是效果也不差，平均意见分数在所有方法中获得的最高。</p><hr><h3 id="Conclusions"><a href="#Conclusions" class="headerlink" title="Conclusions"></a>Conclusions</h3><ul><li>使用ResNet优化了过去的MSE损失。</li><li>提出SRGAN，这是一个基于GAN的网络，为新的感知损失进行了优化。在这里，我们用基于VGG网络的特征映射的损失计算替代基于MSE的内容丢失[49]，这对于像素空间的变化更加不变。</li><li>通过对来自三个公共基准数据集的图像进行广泛的平均意见得分（MOS）测试来证实，SRGAN是一种新的技术水平，通过大幅度的边缘，用于估计具有高放大因子的照片般逼真的重构图像。</li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;https://cdn.jsdelivr.net/npm/aplayer@1.10/dist/APlayer.min.css&quot;&gt;&lt;script 
      
    
    </summary>
    
    
      <category term="论文笔记" scheme="http://alexzou14.github.io/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="深度学习" scheme="http://alexzou14.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="笔记" scheme="http://alexzou14.github.io/tags/%E7%AC%94%E8%AE%B0/"/>
    
      <category term="超分辨率" scheme="http://alexzou14.github.io/tags/%E8%B6%85%E5%88%86%E8%BE%A8%E7%8E%87/"/>
    
  </entry>
  
  <entry>
    <title>Accurate Image Super-Resolution Using Very Deep Convolutional Networks论文阅读笔记</title>
    <link href="http://alexzou14.github.io/2020/04/11/VDSR/"/>
    <id>http://alexzou14.github.io/2020/04/11/VDSR/</id>
    <published>2020-04-11T10:01:01.000Z</published>
    <updated>2020-04-11T10:04:04.286Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="https://cdn.jsdelivr.net/npm/aplayer@1.10/dist/APlayer.min.css"><script src="https://cdn.jsdelivr.net/npm/aplayer@1.10/dist/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="https://cdn.jsdelivr.net/npm/meting@1.2/dist/Meting.min.js"></script><h3 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h3><p>作者提出了一个高精度的单一图像超分方法，该方法是受到VGG网络的启发得到的。作者利用多次级联小尺寸滤波器得到的一个深层次网络（VDSR），虽然网络很深，但是收敛速度很快。这是因为作者采用了比较的学习率和残差学习的方法。作者提出的这个VDSR比现有的方法有很大提升。</p><hr><h3 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h3><p>作者在简介部分先是回顾了单一图像超分现有工作，指出SRCNN是首先使用CNN网络解决SISR问题的方法，并取得了卓越的性能效果。但是，SRCNN存在3点局限性。<br>1.依赖较小的图像区域的内容<br>2.网络训练收敛较慢<br>3.网络只能解决单一尺度的图像超分辨率</p><p>VDSR提出新的方法来解决这些局限性：<br>1.深度网络使用大的感受野来获取图像上下文信息<br>2.网络进行残差学习，并且使用大的学习率，提高收敛速度。采用大学习率，容易遇到梯度爆炸的问题，这里使用适度的梯度裁剪来抑制梯度问题的产生。<br>3.神经网络可以针对不同尺度进行图像超分辨率</p><p>最终的结果是，VDSR使用较SRCNN更深的网络，效果更好，收敛速度更快，并且可用于不同尺度的超分辨</p><hr><h3 id="Related-work"><a href="#Related-work" class="headerlink" title="Related work"></a>Related work</h3><h4 id="Convolutional-Network-for-Image-Super-Resolution"><a href="#Convolutional-Network-for-Image-Super-Resolution" class="headerlink" title="Convolutional Network for Image Super-Resolution"></a>Convolutional Network for Image Super-Resolution</h4><p>从三个方面详细分析性论文中提出的模型与SRCNN的不同之处，  model， train， scale<br><strong>model</strong>:<br>        SRCNN在实验中发现并认为越深的网络得不到更好的结果。<br>        VDSR在试验中成功使用20层网络用于重建信息并取得了优于SRCNN的结果。</p><p><strong>train</strong>:<br>分析SRCNN训练速度慢的原因，并提出VDSR的改进之处</p><p>论文中分析认为：SR在HR空间建模，HR 图片可以分解为高频信息和低频信息，输入和输出的图片享有相同的低频信息，SRCNN 把输入传递到末端，构建残差，这与自动编码的概念类似，在自动编码上会消耗训练时间，论文中提出直接对残差进行建模，加快收敛速度。（对SRCNN收敛速度的分析有点牵强） 主要是提出了论文的基于残差建模</p><p><strong>scale</strong>:<br>SRCNN是一个针对单一因子进行训练的，并且只能在指定的尺度下工作，本文提出了一个单一网络来有效的处理多个规模的SR问题</p><hr><h3 id="Proposed-Method"><a href="#Proposed-Method" class="headerlink" title="Proposed Method"></a>Proposed Method</h3><h4 id="Network-structure"><a href="#Network-structure" class="headerlink" title="Network structure"></a>Network structure</h4><p>除了输入输出层，每个中间层都有64个3*3*64的过滤器，每个过滤器卷积核大小3*3，通道数64，第一层是输入层，最后一层是输出层，仅使用一个3*3*64的过滤器用于图像重构，中间层使用padding技术，使每一层产生的feature map尺寸相同<br>结构如下图：</p><blockquote><p><img src="http://sotavision.cn/showdoc/server/../Public/Uploads/2020-04-11/5e918fb0c0ab5.png" alt="" loading="lazy"></p></blockquote><h4 id="Training"><a href="#Training" class="headerlink" title="Training"></a>Training</h4><ol><li><p>残差学习：训练过程中，网络学习到的是ground-truth与经过插值之后的低分辨率率图像ILR之间的残差。最后的输出则为输入图像与网络预测的残差图像之和。</p></li><li><p>更大的学习率：训练过程中，设置大学习率，可以提高收敛速度，但是容易产生梯度消失或梯度爆炸问题。</p></li><li><p>可调节的梯度裁剪：梯度裁剪方法通常会用在训练RNN网络过程中，但直接用于CNN训练会有一定限制，通常的梯度裁剪策略是把梯度预先限制在\([-\theta,\theta]\)范围内，为了最大化提高收敛速度，本文使用的梯度裁剪策略为：\([-\dfrac{\theta}{\gamma},\dfrac{\theta}{\gamma}]\)，其中\(\gamma\)为虚席率</p></li><li><p>多尺度训练：不同尺度的网络，有很多参数是可以公用的，这里训练了单一网络可以解决不同尺度的图像超分辨问题。</p></li></ol><hr><h3 id="Experiment"><a href="#Experiment" class="headerlink" title="Experiment"></a>Experiment</h3><h4 id="Understanding-Properties"><a href="#Understanding-Properties" class="headerlink" title="Understanding Properties"></a>Understanding Properties</h4><p>从三个方面，结合训练数据分析模型的优越性</p><h5 id="The-Deeper-the-Better"><a href="#The-Deeper-the-Better" class="headerlink" title="The Deeper, the Better"></a>The Deeper, the Better</h5><blockquote><p><img src="http://sotavision.cn/showdoc/server/../Public/Uploads/2020-04-11/5e9192edcb6d4.png" alt="" loading="lazy"></p></blockquote><h5 id="Residual-Learning"><a href="#Residual-Learning" class="headerlink" title="Residual-Learning"></a>Residual-Learning</h5><p>对残差网络，非残差网络，插值  的收敛速度进行对比（用学习epoch和PSNR的关系来看收敛速度）</p><blockquote><p><img src="http://sotavision.cn/showdoc/server/../Public/Uploads/2020-04-11/5e9193125d228.png" alt="" loading="lazy"></p></blockquote><h5 id="Single-Model-for-Multiple-Scales"><a href="#Single-Model-for-Multiple-Scales" class="headerlink" title="Single Model for Multiple Scales"></a>Single Model for Multiple Scales</h5><blockquote><p><img src="http://sotavision.cn/showdoc/server/../Public/Uploads/2020-04-11/5e9193501bac2.png" alt="" loading="lazy"></p></blockquote><h4 id="Comparisons-with-state-of-the-arts"><a href="#Comparisons-with-state-of-the-arts" class="headerlink" title="Comparisons with state-of-the-arts"></a>Comparisons with state-of-the-arts</h4><blockquote><p><img src="http://sotavision.cn/showdoc/server/../Public/Uploads/2020-04-11/5e91937366775.png" alt="" loading="lazy"></p></blockquote><hr><h3 id="Conclusions"><a href="#Conclusions" class="headerlink" title="Conclusions"></a>Conclusions</h3><ol><li>提出了一个更深的网络，并且证明了网络越深效果越好。由于更深的网络结构使得后面的网络层拥有更大的感受野，该文章采取3X3的卷积核，从而使得深度为D的网络，拥有(2D+1)X(2D+1)的感受野，从而可以根据更多的像素点去推断结果像素点。</li><li>加深的网络结构为梯度传输带来了困难，采用残差学习，提高学习率，加快了收敛速度，同时采用调整梯度裁剪</li><li>数据混合:将不同大小倍数的图像混合在一起训练，从而支持不同倍数的高清化</li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;https://cdn.jsdelivr.net/npm/aplayer@1.10/dist/APlayer.min.css&quot;&gt;&lt;script 
      
    
    </summary>
    
    
      <category term="论文笔记" scheme="http://alexzou14.github.io/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="深度学习" scheme="http://alexzou14.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="笔记" scheme="http://alexzou14.github.io/tags/%E7%AC%94%E8%AE%B0/"/>
    
      <category term="超分辨率" scheme="http://alexzou14.github.io/tags/%E8%B6%85%E5%88%86%E8%BE%A8%E7%8E%87/"/>
    
  </entry>
  
  <entry>
    <title>Embedded Block Residual Network:A Recursive Restoration Model for Single-Image Super-Resolution 论文阅读笔记</title>
    <link href="http://alexzou14.github.io/2020/04/10/EBRN/"/>
    <id>http://alexzou14.github.io/2020/04/10/EBRN/</id>
    <published>2020-04-10T13:27:05.000Z</published>
    <updated>2020-04-10T13:31:45.067Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="https://cdn.jsdelivr.net/npm/aplayer@1.10/dist/APlayer.min.css"><script src="https://cdn.jsdelivr.net/npm/aplayer@1.10/dist/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="https://cdn.jsdelivr.net/npm/meting@1.2/dist/Meting.min.js"></script><h3 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h3><p>当时的单一图像超分领域中，比较好的方法都是通过比较复杂的卷积网络或者递归神经网络来得到。但是，这些方法都试图用一个模型去恢复图像中的所有结构和纹理，这样就会在处理高频细节的时忽略低频纹理。所以，图片的低频和高频信息的复杂程度是不一样的，因此在对这两部分信息进行恢复的时候，应该使用具有不同复杂度的模型或者是不同深度的网络结构。本文就提出了一种EBRN，不同模块储存不同的频率信息，浅层网络处理低频信息，深层网络处理高频信息。</p><hr><h3 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h3><p>由于单一图像超分辨的问题存在不定性，所以单一图像超分辨率领域在近些年飞速发展，有很多的超分方法被提出，尤其是近几年学习方法在超分领域的应用，大多数方法都是在致力于训练更深更大的网络来恢复图像，这样的网络在设计中参数数量增大连接过程更加巧妙，需要更多的trick来处理这个网络。可是图片的低频和高频信息的复杂程度是不一样的，因此在对这两部分信息进行恢复的时候，应该使用具有不同复杂度的模型或者是不同深度的网络结构。如果对这两部分信息使用同样的网络结构或者模型进行恢复的话，会出现如下情形：</p><blockquote><p><img src="http://sotavision.cn/showdoc/server/../Public/Uploads/2020-03-04/5e5f169065599.png" alt="" loading="lazy"></p></blockquote><p>即用浅层的网络能够很好地拟合对低频信息进行恢复所需的函数，但是高频信息由于函数更为复杂，低层的网络结构的学习能力有限，对于高频信息是欠学习的(underfit)；而用深层的网络能够很好地拟合对高频信息进行恢复所需的函数，但是对于低频信息而言，属于过拟合(overfit)。所以不同深度网络适应不同频率的信息。<br>本文主要贡献有:</p><ul><li>图片的低频和高频信息的复杂程度是不一样的，因此在对这两部分信息进行恢复的时候，应该使用具有不同复杂度的模型或者是不同深度的网络结构</li><li>提出了一种块残差模块（BRM），它试图恢复图像结构和纹理，同时将难以恢复的信息传递给更深层次的模块</li><li>提出了一种新的嵌入多个BRM的技术，它可以有效地提高基于每个模块输出的最终重建质量.</li></ul><hr><h3 id="Related-work"><a href="#Related-work" class="headerlink" title="Related work"></a>Related work</h3><p>SISR问题在现存的文献中存在的方法可以分为3大类：基于插值方法，基于重建的方法，基于学习的方法。当前超分辨问题的研究方向有以下几个方面：</p><ul><li>通过优化CNN模型框架，介绍了基于深度学习的超分辨率经典模型，包括SRCNN、VDSR、DRCN、EDSR等，这些模型都是基于优化PSNR/SSIM的；</li><li>基于优化损失函数：较为常见的有L1、L2，以及后来的perceptual loss 和adversarial loss.介绍了基于优化感知损失的SRGAN 、SFTGAN模型。</li><li>基于扩大放大倍数上，x2,x3,x4的研究工作已经快接近瓶颈，x8放大成为研究热点。</li></ul><p>通过文献阅读，当前没有人研究模型复杂度和图像频率信息，本文就是基于此开展研究的。</p><hr><h3 id="Proposed-Method"><a href="#Proposed-Method" class="headerlink" title="Proposed Method"></a>Proposed Method</h3><p>基于上述的出发点，作者的策略是对于低频的信息，使用一个大网络结构中的浅层网络进行恢复，对于高频信息，使用一个大网络结构中的深层网络进行恢复。因为对于一个网络结构而言，其中深层的层相比于浅层而言，具有更强的函数拟合能力。因此，作者只需要将图片中的不同频率的信息交由不同深度的网络进行恢复即可。作者的整体网络结构图如下所示：</p><blockquote><p><img src="http://sotavision.cn/showdoc/server/../Public/Uploads/2020-03-04/5e5f1f4a52d7c.png" alt="" loading="lazy"></p></blockquote><h4 id="Block-Residual-Module"><a href="#Block-Residual-Module" class="headerlink" title="Block Residual Module"></a>Block Residual Module</h4><p>从该图可以看出，浅层的网络恢复低频信息，深层的网络恢复高频信息，最后再将这些信息进行concat便得到了最终的超分图片。那么现在的问题是如何在同一个网络结构下，使用不同深度的网络对不同频率的信息进行恢复。针对此，作者设计了一个网络模块BRM，如下图所示：</p><blockquote><p><img src="http://sotavision.cn/showdoc/server/../Public/Uploads/2020-03-04/5e5f28c62f5eb.png" alt="" loading="lazy"></p></blockquote><p>这个模块的作用是使用当前深度的网络恢复部分HR信息，同时将没有恢复的信息(相对来说属于高频的信息)传至更深的网络中进行恢复。这个模块有两个分支：super-resolution flow和back-projection flow。</p><ul><li>super-resolution flow: 对输入的信息(低分辨率特征层)进行超分。</li><li>back-projection flow: 对super-resolution flow的超分图片进行一个下采样，得到一个低分辨率特征层，之后用输出的低分辨率特征层减去这个低分辨率，就能得到此时还没有恢复的信息。之后再将这部分信息输入到一个残差模块，之后将输出的信息输入到下一个RBM模块。</li></ul><p>为什么相减能得到没有恢复的信息：因为原始的LR图片是由HR图片进行bicubic降采样得到的，而用LR超分得到的当前的SR图片的信息肯定是小于HR的，因此对SR进行下采样得到的\(LR’\)的信息肯定是小于\(LR\)的，所以相减就能得到当前深度的网络还没有恢复的信息。<br>之后便是如何对这些恢复的不同频率的信息进行结合了，其中作者发现较深的网络恢复的信息能够提高浅层网络的信息恢复效果，因此提出了一个recursive fusion的方法，这个应该是通过尝试多种不同的信息融合方式所得到的结果,fusion process是：\(O’_x = f(O_x+O’_{x+1})\)</p><h4 id="Loss-Function"><a href="#Loss-Function" class="headerlink" title="Loss Function"></a>Loss Function</h4><p>这里作者先用L1loss快速收敛，让后用L2loss微调模型参数。</p><hr><h3 id="Experiment"><a href="#Experiment" class="headerlink" title="Experiment"></a>Experiment</h3><h4 id="Experiments-Details"><a href="#Experiments-Details" class="headerlink" title="Experiments Details"></a>Experiments Details</h4><p>作者暂未公开代码，可以依照原文自己复现。</p><h4 id="EBRN-VS-other-Network"><a href="#EBRN-VS-other-Network" class="headerlink" title="EBRN VS other Network"></a>EBRN VS other Network</h4><h5 id="EBRN-VS-Residual-Network"><a href="#EBRN-VS-Residual-Network" class="headerlink" title="EBRN VS Residual Network"></a>EBRN VS Residual Network</h5><p>与传统CNN模型相比，残差网络的优势在于残差学习促进了网络中特征的传输，缓解了梯度消失问题，使得网络更容易训练。<br>在这项工作中，我们利用了不同于传统的残差网络的残差学习思想。例如：</p><ol><li>由于BN层限制了特征归一化过程中中间特征的范围灵活性，所以本文模型没有使用batch normalization (BN)层。</li><li>另一个重要的区别是残差是如何计算的以及残差所传达的信息。在残差网络中，剩余信号是输入和输出的差值。在该模型中，一种残差信号是某一频率范围内的信息;残差信号的另一种类型是原始LR特征与反投影LR特征之间的差异。在每一个BRM中，第二个残差信号对于SR是很重要的，因为它明确地传达了哪些信息需要后续BRM恢复。</li></ol><h5 id="EBRN-VS-Deep-Back-Projection-Network"><a href="#EBRN-VS-Deep-Back-Projection-Network" class="headerlink" title="EBRN VS Deep Back-Projection Network"></a>EBRN VS Deep Back-Projection Network</h5><p>DBPN：该方法利用迭代的上下采样层，为每个阶段的投影误差提供了误差反馈机制。误差可以有效地提高模型中深层的恢复效果。<br>两种方法的不同之处在于:</p><ol><li>在上投影单元和下投影单元中，DBPN将LR残差直接映射到HR空间，而我们的模型中LR残差包含更高频率的信息，这些信息被反馈到更深的子网络中进行恢复;</li><li>DBPN利用LR残差和HR残差，目标是每个上、下投影单元尽量减小残差，而我们的方法将残差信号与不同频率的信息联系起来，每个BRM负责恢复相应的信息。动机的不同导致模型的参数比DBPN少，但性能比DBPN有提高。</li></ol><h4 id="Model-Analyses"><a href="#Model-Analyses" class="headerlink" title="Model Analyses"></a>Model Analyses</h4><p>为了验证这一点，我们在图中演示了不同频段上不同BRM输出的能量分布。利用小波变换的不同能级系数，计算了能量在不同频段的分布。结果表明，浅层BRMs的输出包含更多的低频信息，而深层BRMs的输出则倾向于恢复更多的高频信息。</p><blockquote><p><img src="http://sotavision.cn/showdoc/server/../Public/Uploads/2020-03-04/5e5f356e406e3.png" alt="" loading="lazy"></p></blockquote><h4 id="运行速度、最优BRM数验证"><a href="#运行速度、最优BRM数验证" class="headerlink" title="运行速度、最优BRM数验证"></a>运行速度、最优BRM数验证</h4><p><img src="http://sotavision.cn/showdoc/server/../Public/Uploads/2020-03-04/5e5f35dc49d26.png" alt="" loading="lazy"></p><h4 id="Comparisons-with-state-of-the-arts"><a href="#Comparisons-with-state-of-the-arts" class="headerlink" title="Comparisons with state-of-the-arts"></a>Comparisons with state-of-the-arts</h4><blockquote><p><img src="http://sotavision.cn/showdoc/server/../Public/Uploads/2020-03-04/5e5f363b00d9b.png" alt="" loading="lazy"><br><img src="http://sotavision.cn/showdoc/server/../Public/Uploads/2020-03-04/5e5f362b6df91.png" alt="" loading="lazy"></p></blockquote><hr><h3 id="Conclusions"><a href="#Conclusions" class="headerlink" title="Conclusions"></a>Conclusions</h3><ul><li>作者提出了图片的低频和高频信息的复杂程度是不一样的，因此在对这两部分信息进行恢复的时候，应该使用具有不同复杂度的模型或者是不同深度的网络结构</li><li>提出了一种块残差模块（BRM），它试图恢复图像结构和纹理，同时将难以恢复的信息传递给更深层次的模块</li><li>提出了一种新的嵌入多个BRM的技术，它可以有效地提高基于每个模块输出的最终重建质量.</li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;https://cdn.jsdelivr.net/npm/aplayer@1.10/dist/APlayer.min.css&quot;&gt;&lt;script 
      
    
    </summary>
    
    
      <category term="论文笔记" scheme="http://alexzou14.github.io/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="深度学习" scheme="http://alexzou14.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="笔记" scheme="http://alexzou14.github.io/tags/%E7%AC%94%E8%AE%B0/"/>
    
      <category term="超分辨率" scheme="http://alexzou14.github.io/tags/%E8%B6%85%E5%88%86%E8%BE%A8%E7%8E%87/"/>
    
  </entry>
  
  <entry>
    <title>Image Super-Resolution by Neural Texture Transfer论文阅读笔记</title>
    <link href="http://alexzou14.github.io/2020/04/10/SRNTT/"/>
    <id>http://alexzou14.github.io/2020/04/10/SRNTT/</id>
    <published>2020-04-10T13:08:38.000Z</published>
    <updated>2020-04-10T13:27:29.336Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="https://cdn.jsdelivr.net/npm/aplayer@1.10/dist/APlayer.min.css"><script src="https://cdn.jsdelivr.net/npm/aplayer@1.10/dist/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="https://cdn.jsdelivr.net/npm/meting@1.2/dist/Meting.min.js"></script><h3 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h3><p>当参考（Ref）图像的内容与给定出输入LR相似时，基于参考的超分辨率方法（RefSR）在恢复高分辨率（HR）细节方面已被证明是有希望的。由于RefSR方法不稳定，生成的图像质量不高，所以本文的目的是通过参考图像利用更多的纹理细节，甚至更强的鲁棒性，释放参考图像的潜力。作者受到风格迁移的思想激发，可以将RefSR方法可以应用神经风格迁移方法。<br>本文提出了一个端到端的深层模型，它使HR细节通过自适应转移相似的语义信息，本文方法主要包括两步：</p><ol><li>特征空间的纹理匹配，</li><li>移匹配的纹理。</li></ol><p>另外，本文提出一个CUFFED5数据集，这个数据集包含不同相似级别的参考图像。</p><hr><h3 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h3><p>在神经网络的引入下，解决SISR卓越de的方法得到了很大的提升。由于SISR问题的不定性，这导致现有的大部分方法还是在大放大倍数下会产生模糊的结果。在感知损失和对抗损失的引入下提升了图像的感知细节，但是同时又会在一定程度上产生虚假纹理和伪影。<br>基于参考（reference-based）的方法, 即RefSR，通过利用高分辨率（HR）参考图像的丰富纹理来补偿低分辨率（LR）图像丢失的细节。但是之前的方法需要参考图像与LR包含相似的内容，并且需要图像对齐，否则的话，这些方法效果会很差。RefSR理想上应该可以有效利用参考图像，如果参考图像与LR不同，也不应该损害LR的复原效果，即RefSR应不差于SISR。<br>现有的RefSP模型对参考图像有很高的要求，要求参考图像与模糊图像的内容相仿且具有良好的对齐，这是比较难做到的，后来有人提出使用optical flow(一种图像对齐算法)先对参考图像和模糊图像进行对齐，然后送入RefSR模型。但是optical flow在两张图象的错位极其严重时表现欠佳，因此Adobe团队提出了基于纹理迁移的图像超分辨率模型(Image Super-Resolution by Neural Texture Transfer)，简称为SRNTT。<br>SRNTT主要有以下几个贡献：</p><ul><li>解决了现有SISR方法会出现虚假纹理的问题</li><li>放松了现有的RefSR方法的约束问题，不要求参考图像与模糊图像严格对齐</li><li>提高了现有RefSR方法的鲁棒性，即使使用相似性不是很高的参考图像也可以得到较好的结果构建了一个基准数据集CUFED5</li></ul><hr><h3 id="Related-work"><a href="#Related-work" class="headerlink" title="Related work"></a>Related work</h3><h4 id="Deep-Learning-based-SISR"><a href="#Deep-Learning-based-SISR" class="headerlink" title="Deep Learning based SISR"></a>Deep Learning based SISR</h4><p>单图超分辨率模型(SISR)的输入只有一张图象，模型会从这一张图像提取一些高频信息并使用特殊的方法合成到原图上去，以完成超分辨率的过程。这种方法有一个缺点，模糊图像毕竟不含有我们想要的高频信息，所以即便我们使用特殊的方法去提取，最后得到的结果也不可能与实际情况完全相同，也就是说，最后模型得到的图像存在一些虚假的纹理，虽然在视觉效果上图像是清晰的，但是图像的细节信息却是假的。</p><h4 id="Reference-based-Super-Resolution"><a href="#Reference-based-Super-Resolution" class="headerlink" title="Reference-based Super-Resolution"></a>Reference-based Super-Resolution</h4><p>基于参考图像的超分辨率(RefSR)。这种模型的输入图像有两个，一个是模糊图像，一个是清晰图像。模型会从清晰图像中提取真实的高频信息，然后将其合成到模糊图像中去。也许你会有一个疑问，既然已经有了清晰的图像，为什么我们去做超分辨率？这是因为清晰图像的角度、拍摄内容、光线等不一定乐意是我们满意，但它的高频信息却是我们需要的。</p><hr><h3 id="Proposed-Method"><a href="#Proposed-Method" class="headerlink" title="Proposed Method"></a>Proposed Method</h3><p>网络结构如下图。</p><blockquote><p><img src="http://sotavision.cn/showdoc/server/../Public/Uploads/2020-03-04/5e5f701e31110.png" alt="" loading="lazy"></p></blockquote><p>SRNTT主要由两个部分组成，一是上图中蓝色方框之外的部分，称之为特征交换；另一部分为蓝色方框内部的纹理迁移部分。</p><h4 id="Feature-Swapping"><a href="#Feature-Swapping" class="headerlink" title="Feature Swapping"></a>Feature Swapping</h4><p>特征交换这一部的主要工作就是在Ref中找到与LR最接近的块，然后替换掉LR中的块。这就涉及两个步骤: 如何计算相似度、如何交换对应块。</p><ul><li>如何计算相似度</li></ul><p>论文提出的方法不是计算整个图的相似度，而是分块计算。这里因为LR和Ref的大小不同，先对LR用bicubic进行上采样使LR和Ref具有相同的大小，而同时考虑到二者的分辨率不同，对Ref用bicubic进行下采样再上采样，使得\(I^{Ref\downarrow\uparrow}\)的模糊程度跟\(I^{LR\uparrow}\)接近。<br>考虑到\(I^{LR\uparrow}\)和\(I^{Ref\downarrow\uparrow}\)的块在颜色和光照上面可能有些不同，于是论文不直接对Patch的像素进行计算相似度，而在高层次的特征图上计算，即\(\phi(I)\)，来突出结构和纹理信息。<br>论文采用内积方法来计算相似度：<br>\[s_{i,j}=\langle{P_i(\phi(I^{LR\uparrow})),\dfrac{P_i(\phi(I^{LR\uparrow}))}{\left |P_i(\phi(I^{LR\uparrow})\right |}}\rangle\]<br>上式，计算了LR的 i-th patch和模糊化参考图像 的 j-th patch的相似性。注意，对参考patch进行了归一化。可以通过卷积或者互相关来加速以上计算过程：<br>\[S_j=\phi(I^{LR\uparrow}*{\dfrac{P_j(\phi(I^{LR\uparrow}))}{\left |P_j(\phi(I^{LR\uparrow})\right |}})\]<br>\(S_j\)即表示 s-th patch 相对 LR的相似性。相当于一个卷积核对LR进行卷积，计算结果即为Similarity map。</p><ul><li>交换操作</li></ul><p>因为在LR和Ref的特征空间中密集采样，所以每个LR位置都对应多个不同的卷积核的卷积结果，对应多个不同相似性的纹理特征。基于Similarity map，选择LR每个位置的相似性最高的Ref patch，构成交换特征图M (swapped feature map)：<br>\[P_{\omega(x,y)}(M)=P_{j^*}(\phi(I^{Ref})),j^*=\arg \max_jS_j(x,y)\]<br>即M在（x,y）位置对应的Ref patch是similairty score最大的Ref patch。由于每一个位置对应一个Ref patch，所以这些patch是重叠的，在重叠位置，取平均。另外，另外，注意计算相似性是使用\(I^{Ref\uparrow\downarrow}\)，纹理迁移则是利用 \(I^{Ref}\)。<br>在实现上，利用VGG-19提取特征，relu1_1, relu2_1,relu3_1用于多个尺度上纹理编码，但是为了加快匹配，只在relu3_1层进行匹配，将匹配结果对应到relu1_1和relu2_1。</p><h4 id="Neural-Texture-Transfer"><a href="#Neural-Texture-Transfer" class="headerlink" title="Neural Texture Transfer"></a>Neural Texture Transfer</h4><p>有了多尺度的swapped feature map，如何进行纹理迁移呢？采用residual blocks和跳跃连接方式，融合LR的特征和swap特征，并通过sub-pixel conv上采样。如下图：</p><blockquote><p><img src="http://sotavision.cn/showdoc/server/../Public/Uploads/2020-03-04/5e5f7c3c83fb7.png" alt="" loading="lazy"></p></blockquote><p>论文采用残差块和跳跃连接来建立基本的生成网络。记\(\psi_l\)是第l层的输出，可以定义为：<br>\[\psi_l=[\text{Res}(\psi_{l-1}||M_{l-1})+\psi_{l-1}]\uparrow_{2\times}\]<br>假设有L层，那最终SR:<br>\[I^{SR}=\text{Res}(\psi_{L-1}||M_{L-1})+\psi_{L-1}\]</p><h4 id="Loss-function"><a href="#Loss-function" class="headerlink" title="Loss function"></a>Loss function</h4><p>目标：1）保留LR的空间结构，2）提升SR的视觉质量，3）充分利用Ref的丰富纹理。提出四个loss function：</p><ul><li>Reconstruction loss: SR与HR的L1距离</li></ul><p>\[L_{rec}=\left |I^{HR}-I^{SR} \right |_1\]</p><ul><li>Perceptual loss： 采用VGG-19的relu5_1层</li></ul><p>\[{L_{per}=\dfrac{1}{V}\sum_{i=1}^C\left |{\phi_i(I^{HR})-\phi_i(I^{SR})}\right |_F}\]</p><ul><li>Adversarial loss：利用 WGAN-GP<div>  \[L_{adv}=-\text{E}_{\tilde{x}\sim{P_g}}[D(\tilde{x})],\min_g \max_{D\in \text{D}}\text{E}_{x \sim{P_r}}[D(x)]-\text{E}_{\tilde{x}\sim{P_g}}[D(\tilde{x})]\]</div></li><li>Texture loss：纹理损失是为了让SR的纹理与Ref纹理更接近，另外，通过使用similarity map作为权重，抑制不相似纹理的惩罚，放大相似纹理的惩罚，这样可以自适应地进行纹理迁移<div>  \[L_{tex}=\sum_l{\lambda_l \left \|Gr(\phi_l(I^{SR})\cdot S_l^*)-Gr(M_l\cdot S_l^*) \right \|}_F\]</div></li></ul><p>其中，\(G_r\)计算 Gram matrix。</p><hr><h3 id="Experiment"><a href="#Experiment" class="headerlink" title="Experiment"></a>Experiment</h3><h4 id="Experiments-Details"><a href="#Experiments-Details" class="headerlink" title="Experiments Details"></a>Experiments Details</h4><p>具体实验细节可以查看官方代码：<a href="https://github.com/ZZUTK/SRNTT" target="_blank" rel="noopener">https://github.com/ZZUTK/SRNTT</a></p><h4 id="DataSet"><a href="#DataSet" class="headerlink" title="DataSet"></a>DataSet</h4><p>本文基于CUFED构建了一个数据集，数据集包含不同相似度的LR-HR-Ref 图像对。共有4个相似度级别，不同的相似度是基于SIFT特征匹配计算的。测试数据集包含Sun80和Urban100。</p><h4 id="Experiment-Result"><a href="#Experiment-Result" class="headerlink" title="Experiment Result"></a>Experiment Result</h4><p>论文对比了不同的模型在不同数据集上的表现，最后结果是，定量观测PSNR值来看，在单图超分辨率领域，SRNTT取得第二名；在基于参考的超分辨领域，SRNTT优于现有的所有模型，位列第一。</p><blockquote><p><img src="http://sotavision.cn/showdoc/server/../Public/Uploads/2020-03-04/5e5f9e4edb182.png" alt="" loading="lazy"></p></blockquote><hr><h3 id="Conclusions"><a href="#Conclusions" class="headerlink" title="Conclusions"></a>Conclusions</h3><ul><li>论文学会用RefSR的思想，打破了SISR的束缚。（其实就是不再单纯地学习HR和LR的映射，而是引入RefSR）</li><li>论文提出了SRNTT，可以得到更好的超分辨率效果。</li><li>论文建立了一个新的数据集，CUFED5，对LR有不同相似程度的RefSR，用来进行进一步探索。</li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;https://cdn.jsdelivr.net/npm/aplayer@1.10/dist/APlayer.min.css&quot;&gt;&lt;script 
      
    
    </summary>
    
    
      <category term="论文笔记" scheme="http://alexzou14.github.io/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="深度学习" scheme="http://alexzou14.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="笔记" scheme="http://alexzou14.github.io/tags/%E7%AC%94%E8%AE%B0/"/>
    
      <category term="超分辨率" scheme="http://alexzou14.github.io/tags/%E8%B6%85%E5%88%86%E8%BE%A8%E7%8E%87/"/>
    
  </entry>
  
  <entry>
    <title>Real-Time Single Image and Video Super-Resolution Using an EfficientSub-Pixel Convolutional Neural Network论文阅读笔记</title>
    <link href="http://alexzou14.github.io/2020/04/10/ESPCN/"/>
    <id>http://alexzou14.github.io/2020/04/10/ESPCN/</id>
    <published>2020-04-10T12:55:35.000Z</published>
    <updated>2020-04-10T13:06:04.617Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="https://cdn.jsdelivr.net/npm/aplayer@1.10/dist/APlayer.min.css"><script src="https://cdn.jsdelivr.net/npm/aplayer@1.10/dist/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="https://cdn.jsdelivr.net/npm/meting@1.2/dist/Meting.min.js"></script><h3 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h3><p>近来，基于深层神经网络的几种模型在单像超分辨率的重构精度和计算性能方面取得了巨大的成功。在这些方法中，低分辨率（LR）输入图像在重建之前使用单个滤波器（通常是双三次插值）被放大到高分辨率（HR）空间再输入到网络中重建，这样会增加了计算复杂度。在本文中，作者提出了第一个能够实现1080p视频实时SR的卷积神经网络。为了实现这一点，我们提出了一种新颖的CNN架构，其中在LR空间中提取特征图。另外，我们引入了一个有效的亚像素卷积层，该层学习了一个升序滤波器阵列，将最终的LR特征图升级到HR输出。通过这样做，我们有效地更换SR管道中的手工双三次插值滤波器，并为每个特征图进行了专门训练的更复杂的升频滤波器，同时还降低了整个SR操作的计算复杂度。</p><hr><h3 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h3><p>全局SR问题假设LR数据是HR数据的低通滤波（模糊），下采样和噪声版本。 由于在不可逆低通滤波和子采样期间发生的高频信息的丢失，这是一个高度ill-posed的问题。 此外，SR操作实际上是从LR到HR空间的一对多映射，其可以具有多个解决方案，其中确定正确的解决方案是不容易的。 基于许多SR技术的一个关键假设是大部分高频数据是冗余的，因此可以从低频分量精确地重构。因此，SR是一个推理问题，因此依赖于图像的统计信息。</p><hr><h3 id="Related-work"><a href="#Related-work" class="headerlink" title="Related work"></a>Related work</h3><p>许多方法假定多个图像可看作具有不同视角的相同场景的LR实例，即具有独特的先前仿射变换。这些可以被分类为多图像SR方法，并且通过基于附加信息来限制ill-posed问题并尝试反转下采样过程的方式来利用显式冗余信息。然而，这些方法通常需要计算复杂的图像配准和融合阶段，其准确性直接影响结果的质量。另一个方法是单图像超分辨率（SISR）技术。这些技术寻求学习自然数据中存在的隐性冗余，以从单个LR实例中恢复丢失的HR信息。这通常以图像的局部空间相关性和视频中的附加时间相关性的形式出现。在这种情况下，需要以重建约束的形式的先验信息来限制重构的解空间。<br>本文主要贡献为：<br>1.upscaling由network最后一层处理，也就表示每各LR图像可以从network中之间得到反馈并在LR空间里中进行特种提取。（减少计算和存储器的复杂度）</p><p>2.对于具有L层的网络，我们学习nL-1特征映射的nL-1升级滤波器，而不是输入图像的一个升频滤波器。此外，不使用显式插值滤波器意味着网络隐式地学习SR所需的处理。因此，与在第一层的单个固定滤波器放大相比，网络能够学习更好和更复杂的LR到HR映射。这导致模型的重建精度的额外增益</p><hr><h3 id="Proposed-Method"><a href="#Proposed-Method" class="headerlink" title="Proposed Method"></a>Proposed Method</h3><h4 id="Network-structure"><a href="#Network-structure" class="headerlink" title="Network structure"></a>Network structure</h4><blockquote><p><img src="http://sotavision.cn/showdoc/server/../Public/Uploads/2020-04-10/5e90616e8620f.png" alt="" loading="lazy"></p></blockquote><p>ESPCN的核心概念是亚像素卷积层(sub-pixel convolutional layer)。如上图所示，网络的输入是原始低分辨率图像，通过两个卷积层以后，得到的特征图像大小与输入图像一样，但是特征通道为\(r^2\)（r是图像的目标放大倍数)。将每个像素的个通道\(r^2\)重新排列成一个r x r的区域，对应于高分辨率图像中的一个r x r大小的子块，从而大小为\( r^2 \times H \times W \) 的特征图像被重新排列成\( 1 \times rH \times rW \)大小的高分辨率图像。这个变换虽然被称作sub-pixel convolution, 但实际上并没有卷积操作。通过使用sub-pixel convolution, 图像从低分辨率到高分辨率放大的过程，插值函数被隐含地包含在前面的卷积层中，可以自动学习到。只在最后一层对图像大小做变换，前面的卷积运算由于在低分辨率图像上进行，因此效率会较高。</p><p>该网络可以用数学公式表达为：<br>\[ f^1(I^{LR};W_1,b_1)=\phi(W_1 \ast I^{LR}+b_1) \]<br>\[ f^l(I^{LR};W_{1:l},b_{1:l})=\phi(W_l \ast f^{l-1}(I^{LR})+b_l) \]</p><h4 id="Efficient-sub-pixel-convolution-layer"><a href="#Efficient-sub-pixel-convolution-layer" class="headerlink" title="Efficient sub-pixel convolution layer"></a>Efficient sub-pixel convolution layer</h4><p>deconvolution, FSRCNN在最后的上采样层，通过学习最后的deconvolution layer。但deconvolution本质上是可以看做一种特殊的卷积，理论上后面要通过stack filters才能使得性能有更大的提升。本文提到的亚像素卷积sub-pixel Layer，其实跟常规的卷积层没有什么不同，不同的是其输出的特征通道数为r^2，其中r为缩放倍数。表达式为：<br>\[ I^{SR}=f^L(I^{LR})=\mathcal{PS}(W_L \ast f^{L-1}(I^{LR})+b_L) \]</p><p>可从上面的公式看到，所得的高分辨率图像是通过PS操作，将r^2维度的低分辨率特征重新排列成高分辨率图像。其中PS操作称之为： periodic shuffling<br>本质上就是将低分辨率特征，按照特定位置，周期性的插入到高分辨率图像中，可以通过颜色观测到上图的插入方式。可以表达为：</p><div>    \[ \mathcal{PS}(T)_{x,y,c}=T_{\left \lfloor x/r \right\rfloor,\left \lfloor y/r \right\rfloor, C \cdot r \cdot mod(y,r)+C \cdot mod(x,r)+c} \]</div><h4 id="Loss-function"><a href="#Loss-function" class="headerlink" title="Loss function"></a>Loss function</h4><p>文章用了MSE loss优化网络：<br>\[ l(W_{1:l},b_{1:L})=\dfrac{1}{r^2HW}\displaystyle\sum_{x=1}^{rH}\sum_{x=1}^{rW}(I_{x,y}^{HR}-f_{x,y}^L(I^{LR}))^2 \]</p><hr><h3 id="Experiment"><a href="#Experiment" class="headerlink" title="Experiment"></a>Experiment</h3><h4 id="Comparison-to-the-state-of-the-art"><a href="#Comparison-to-the-state-of-the-art" class="headerlink" title="Comparison to the state-of-the-art"></a>Comparison to the state-of-the-art</h4><p>图像质量：</p><blockquote><p><img src="http://sotavision.cn/showdoc/server/../Public/Uploads/2020-04-10/5e906a516211f.png" alt="" loading="lazy"></p></blockquote><p>重建质量也是能够达到当时最优的。</p><h4 id="Run-time-evaluations"><a href="#Run-time-evaluations" class="headerlink" title="Run time evaluations"></a>Run time evaluations</h4><p>运行速度:</p><blockquote><p><img src="http://sotavision.cn/showdoc/server/../Public/Uploads/2020-04-10/5e906ad840ea9.png" alt="" loading="lazy"></p></blockquote><p>速度上是相当快的，基本上能在视频上进行实时处理</p><hr><h3 id="Conclusions"><a href="#Conclusions" class="headerlink" title="Conclusions"></a>Conclusions</h3><ol><li>作者提出了ESPCN模型，并达到了当时最先进的性能。</li><li>证明了第一层的固定滤波器升级不能为SISR提供任何额外信息，但需要更多的计算复杂性。</li><li>提出了一种新颖的亚像素卷积层，与去卷积层相比，它能够以非常小的额外计算成本将LR数据超解析为HR空间。 在升级因子为4的扩展基准标记数据集上进行的评估表明，与之前的CNN方法相比，我们具有显着的速度（&gt; 10倍）和性能（图像上的+ 0.15dB和视频上的+ 0.39dB）增强。 这使我们的模型成为第一个能够在单个GPU上实时生成SR高清视频的CNN模型。</li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;https://cdn.jsdelivr.net/npm/aplayer@1.10/dist/APlayer.min.css&quot;&gt;&lt;script 
      
    
    </summary>
    
    
      <category term="论文笔记" scheme="http://alexzou14.github.io/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="深度学习" scheme="http://alexzou14.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="笔记" scheme="http://alexzou14.github.io/tags/%E7%AC%94%E8%AE%B0/"/>
    
      <category term="超分辨率" scheme="http://alexzou14.github.io/tags/%E8%B6%85%E5%88%86%E8%BE%A8%E7%8E%87/"/>
    
  </entry>
  
  <entry>
    <title>Feedback Network for Image Super-Resolution论文阅读笔记</title>
    <link href="http://alexzou14.github.io/2020/04/08/SRFBN/"/>
    <id>http://alexzou14.github.io/2020/04/08/SRFBN/</id>
    <published>2020-04-08T03:19:33.000Z</published>
    <updated>2020-04-08T03:34:21.410Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="https://cdn.jsdelivr.net/npm/aplayer@1.10/dist/APlayer.min.css"><script src="https://cdn.jsdelivr.net/npm/aplayer@1.10/dist/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="https://cdn.jsdelivr.net/npm/meting@1.2/dist/Meting.min.js"></script><h3 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h3><p>由于现有的基于深度学习的图像SR方法尚未充分利用人类视觉系统中常见的反馈机制，所以本文基于这一想法提出了一个图像超分辨率反馈网络(SRFBN)通过高层信息来细化低层信息。具体这种反馈机制，是用具有约束的RNN中的隐状态来实现这种反馈机制。这种反馈机制能够产生非常强的高层表征能力。。此外，作者还引入了curriculum learning 策略，使网络非常适合于更复杂的任务。</p><hr><h3 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h3><p>随着网络的深度增加，参数的数量也会增加。大容量网络将占用大量存储资源并遭受过度拟合问题。为了减少网络参数，通常采用循环结构。因为具有重复结构的这些网络可以以前馈方式共享信息。但是，前馈方式使得先前的层不可能从以下层访问有用信息，即使采用跳过连接也是如此。<br>在本文中，作者提出了一种新的图像SR网络，即超分辨率反馈网络（SRFBN），以便通过反馈连接使用高级信息来改进低级信息。 所提出的SRFBN本质上是具有反馈块（FB）的RNN，其专门用于图像SR任务。<br>总之，我们的主要贡献如下：</p><ul><li>提出采用反馈机制的图像超分辨率反馈网络（SRFBN）。 通过反馈连接在自上而下的反馈流中提供高级信息。 同时，这种具有反馈连接的循环结构提供了强大的早期重建能力，并且仅需要很少的参数。</li><li>提出反馈块（FB），它不仅可以有效地处理反馈信息流，还可以通过上采样层和下采样层以及密集跳过连接来丰富高级表示。</li><li>为SRFBN提出curriculum -based训练策略，其中将具有增加的重建难度的HR图像作为连续迭代的目标馈入网络。 该策略使网络能够逐步学习复杂的退化模型，而对于那些只有一步预测的方法，同样的策略是不可能的。</li></ul><hr><h3 id="Related-work"><a href="#Related-work" class="headerlink" title="Related work"></a>Related work</h3><h4 id="Deep-learning-based-image-super-resolution"><a href="#Deep-learning-based-image-super-resolution" class="headerlink" title="Deep learning based image super-resolution"></a>Deep learning based image super-resolution</h4><p>这一部分作者主要回顾了深度学习在超分变率领域的应用和方法回顾。从2015年董超提出SRCN开始将深度学习引入超分辨领域，近几年的超分领域的不断发展，提出了很多方法，如：VDSR，EDSR等</p><h4 id="Feedback-mechanism"><a href="#Feedback-mechanism" class="headerlink" title="Feedback mechanism"></a>Feedback mechanism</h4><p>反馈机制能够允许该网络携带当前的输出去纠正之前的一些状态。<br>针对图片超分辨率中的feedback mechanism，有两个必要条件：<br>1）迭代 ；见下图(b)<br>2）重新路由（rerouting）系统的输出，以纠正（correct）每个循环中的输入， 在本文提出的网络结构中，实施feedback mechanism，有三个必不可少的部分:<br>1）在每一次迭代过程中都绑定loss；<br>2）使用循环结构（实现迭代的过程）；<br>3）在每一次迭代过程中提供低分辨率图片的输入。<br>上面三个部分缺一不可。</p><blockquote><p><img src="http://sotavision.cn/showdoc/server/../Public/Uploads/2020-03-11/5e68efd37e4e0.png" alt="" loading="lazy"></p></blockquote><h4 id="Curriculum-learning"><a href="#Curriculum-learning" class="headerlink" title="Curriculum learning"></a>Curriculum learning</h4><p>Curriculum learning逐渐增加了学习目标的难度，众所周知，这是改进训练程序的有效策略。早期的课程学习工作主要集中在一项任务上。 Pentina等以连续的方式将课程学习扩展到多个任务。虽然之前的工作主要集中在单个degradation过程，但我们对案例强制执行curriculum ，其中LR图像被多种类型的劣化所破坏。包含易于做出决策的curriculum可以针对一个问题进行解决，以逐步恢复损坏的LR图像。</p><hr><h3 id="Proposed-Method"><a href="#Proposed-Method" class="headerlink" title="Proposed Method"></a>Proposed Method</h3><blockquote><p><img src="http://sotavision.cn/showdoc/server/../Public/Uploads/2020-03-11/5e68f4db2d8fc.png" alt="" loading="lazy"></p></blockquote><p>见图我们提出的SRFBN可以被展开为T个迭代，迭代的顺序是从1到t，为了使得中间的状态能够携带一些输出信息，我们在每一次的迭代后面都会绑定一个loss值。<br>网络结构被分为三个模块：LR特征提取模块，反馈模块，重建模块</p><h4 id="Network-structure"><a href="#Network-structure" class="headerlink" title="Network structure"></a>Network structure</h4><ul><li><p>LR特征提取模块:这个LR特征提取模块由一个conv(3, 4m)和一个conv(3, m)组成，浅层特征提取可以由下列表达式表达：\(F_{in}^t = f_{LRFB}(I_{LR})\)</p></li><li><p>反馈模块:第t个权重共享模块的输出可以表示为：\(F_{out}^t=f_{FB}(F_{out}^{t-1},F_{in}^t)\)</p></li><li><p>第t个权重共享模块的中间监督输出：\(I_{SR}^t=I_{Res}^t+f_{UP}(I_{LR})\)其中\(I_{Res}^t=f_{RB}(F_{out}^t)\)</p></li></ul><h4 id="Feedback-block"><a href="#Feedback-block" class="headerlink" title="Feedback block"></a>Feedback block</h4><p><img src="http://sotavision.cn/showdoc/server/../Public/Uploads/2020-03-11/5e68feb50db97.png" alt="" loading="lazy"><br>简而言之，反馈模块就是反复上采样再下采样操作，同时，对所有上采样后的特征用dense connection，也对下采样后的特征用dense connection，中间用1*1卷积来降低计算量。<br>输入特征：\(L_0^t = C_0([F_{out}^{t-1},F_{in}^t])\)<br>上采样后的特征：\(H_g^t = C_g^{\uparrow}([L_0^t,L_1^t,…,L_{g-1}^t])\)<br>下采样后的特征：\(L_g^t = C_g^{\downarrow}([H_1^t,H_2^t,…,H_{g-1}^t])\)<br>输出特征：\(f_{out}^t = C_{FF}([L_1^t,L_2^t,…,L_{G}^t])\)</p><h4 id="Loss-function"><a href="#Loss-function" class="headerlink" title="Loss function"></a>Loss function</h4><p>loss function 为：\(L({\Theta}) = \dfrac{1}{T} \displaystyle\sum_{t=1}^{T}W^t||I_{HR}^t - I_{SR}^t||\)。控制数值\(W^t\)实验中都为1。</p><h4 id="Curriculum-learning-strategy"><a href="#Curriculum-learning-strategy" class="headerlink" title="Curriculum learning strategy"></a>Curriculum learning strategy</h4><p> 简而言之就是，中间监督的真值会根据任务难度进行选择，比如单一的bicubic降采样退化，所有的真值都是一样的；而对于BD（bicubic+blur）退化，头两个中间监督输出用带高斯模糊的真值，之后的中间监督用不带高斯模糊的真值。</p><hr><h3 id="Experiment"><a href="#Experiment" class="headerlink" title="Experiment"></a>Experiment</h3><h4 id="Experiments-Details"><a href="#Experiments-Details" class="headerlink" title="Experiments Details"></a>Experiments Details</h4><p>实验细节可以参考官方代码：<a href="https://github.com/Paper99/SRFBN_CVPR19" target="_blank" rel="noopener">https://github.com/Paper99/SRFBN_CVPR19</a></p><h4 id="Network-analysis"><a href="#Network-analysis" class="headerlink" title="Network analysis"></a>Network analysis</h4><p>在这一小节中，我们探讨了迭代次数(表示为T)和反馈块中投影组的数目(表示为G)的影响。</p><blockquote><p><img src="http://sotavision.cn/showdoc/server/../Public/Uploads/2020-03-11/5e6906c0a2c1c.png" alt="" loading="lazy"></p></blockquote><p>选择更大的T或G都有助于取得更好的结果。值得注意的是，小T和G仍然优于VDSR</p><h4 id="Comparisons-with-state-of-the-arts"><a href="#Comparisons-with-state-of-the-arts" class="headerlink" title="Comparisons with state-of-the-arts"></a>Comparisons with state-of-the-arts</h4><blockquote><p><img src="http://sotavision.cn/showdoc/server/../Public/Uploads/2020-03-11/5e69077a67a19.png" alt="" loading="lazy"></p></blockquote><h4 id="Results-with-BI-degradation-model"><a href="#Results-with-BI-degradation-model" class="headerlink" title="Results with BI degradation model"></a>Results with BI degradation model</h4><blockquote><p><img src="http://sotavision.cn/showdoc/server/../Public/Uploads/2020-03-11/5e6909a18b73d.png" alt="" loading="lazy"><br>这里没用放RCAN和RDN的数据，实际上SRFBN没达到SOTA，现在最好的还是RCAN</p></blockquote><h4 id="Results-with-BD-and-DN-degradation-models"><a href="#Results-with-BD-and-DN-degradation-models" class="headerlink" title="Results with BD and DN degradation models"></a>Results with BD and DN degradation models</h4><blockquote><p><img src="http://sotavision.cn/showdoc/server/../Public/Uploads/2020-03-11/5e6909ac81cd8.png" alt="" loading="lazy"></p></blockquote><hr><h3 id="Conclusions"><a href="#Conclusions" class="headerlink" title="Conclusions"></a>Conclusions</h3><p>本文提出了一种新的图像SR网络-超分辨率反馈网络(SRFBN)，通过增强高层次的图像表示来忠实地重建SR图像。网络中的反馈块(FB)可以有效地处理反馈信息流和特征重用。此外，还提出了一种curriculum学习策略，使网络能够很好地适应复杂退化模型破坏低分辨率图像的复杂任务。综合实验结果表明，所提出的SRFBN能以极小的参数提供与现有方法相比的比较或更好的性能。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;https://cdn.jsdelivr.net/npm/aplayer@1.10/dist/APlayer.min.css&quot;&gt;&lt;script 
      
    
    </summary>
    
    
      <category term="论文笔记" scheme="http://alexzou14.github.io/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="深度学习" scheme="http://alexzou14.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="笔记" scheme="http://alexzou14.github.io/tags/%E7%AC%94%E8%AE%B0/"/>
    
      <category term="超分辨率" scheme="http://alexzou14.github.io/tags/%E8%B6%85%E5%88%86%E8%BE%A8%E7%8E%87/"/>
    
  </entry>
  
  <entry>
    <title>Guided Super-Resolution as Pixel-to-Pixel Transformation论文阅读笔记</title>
    <link href="http://alexzou14.github.io/2020/04/08/GuideSR/"/>
    <id>http://alexzou14.github.io/2020/04/08/GuideSR/</id>
    <published>2020-04-08T03:11:46.000Z</published>
    <updated>2020-04-08T03:17:26.306Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="https://cdn.jsdelivr.net/npm/aplayer@1.10/dist/APlayer.min.css"><script src="https://cdn.jsdelivr.net/npm/aplayer@1.10/dist/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="https://cdn.jsdelivr.net/npm/meting@1.2/dist/Meting.min.js"></script><h3 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h3><p>本文提出了引导超分的概念。输入是某种模态的低分辨率图像数据，如用ToF相机获取的深度图像。目标输出是输入图像的高分辨率版本。其中还有一个概念叫做Guided Image——引导图像，是输入图像在另一种模态下的高分辨率图像，如使用传统相机获取的深度图像。传统建模的思想是：将低分辨率图像进行上采样，从引导图像中获取高频信息补充到上采样后的输入图像。本文作者利用了机器学习的方法将这个问题转化成了引导图像到输入图像模态的pixel-to-pixel的映射，和风格迁移有着很大的相似之处。Pixel-to-pixel mapping是通过多层感知机进行的实现，通过最小化源图像和下采样目标图像之间的差异来学习其权重，即为我们所要求解的映射。本文提出的算法只需要对映射函数进行正则化，避免了对输出结果的直接处理，而且是无监督学习的算法。</p><hr><h3 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h3><p>很多计算机视觉任务都可以看作引导超分辨率的实例，例如，环境测绘，机器人视觉等等。引导超分辨率可以看成输入一张低分辨率图，再出入一张相似的高清图来还原目标图像得到高分辨率图。如下图所示：</p><blockquote><p><img src="http://sotavision.cn/showdoc/server/../Public/Uploads/2020-03-23/5e7815665ee78.png" alt="" loading="lazy"></p></blockquote><p>换一个角度看，引导超分辨又可以看作是引导滤波器的推广形式，引导滤波器将源图像映射到目标图像  通过计算每个像素处的相同大小，依赖于源和引导图像中的局部邻域，从而得到源图像的高清版本。作者提出了另一种关于超分辨率的解释，源图像和引导图像的作用被交换，即从一个图像的像素级映射 到另一个而不改变分辨率，并通过要求其下采样版本与源映像匹配来约束输出。<br>文中认为，这种导超分辨率的观点有两个非常实际的优点：</p><ul><li>从所需分辨率开始，只使用1×1个核，不同的输入位置不会混合，这避免了模糊。</li><li>通过对所有像素使用相同的映射函数，并在其参数之前放置收缩，这样就会得到不需要正则化输出图像的适定问题</li></ul><p>本文的贡献是</p><ul><li>一种新的引导超分辨率公式，即无监督学习从像素到像素的转换受低分辨率光源限制。</li><li>我们对两个任务进行了实验：深度图的超分辨率和树高图的超分辨率。它们表明我们的方法在高上采样因子上明显优于其他的超分辨率方法</li></ul><hr><h3 id="Related-work"><a href="#Related-work" class="headerlink" title="Related work"></a>Related work</h3><h4 id="Guided-filtering"><a href="#Guided-filtering" class="headerlink" title="Guided filtering"></a>Guided filtering</h4><p>关于引导滤波，一般的原理是通过应用一个滤波器来增强源图像，该滤波器的输出不仅取决于源图像的局部邻域，而且还取决于从引导图像中的相同邻域导出的权重。引导滤波已经被用于各种各样的图像处理应用中，从去噪或着色等低级任务一直到立体匹配等。</p><h4 id="Guided-super-resolution"><a href="#Guided-super-resolution" class="headerlink" title="Guided super-resolution"></a>Guided super-resolution</h4><p>对于超分辨深度以及诸如色调映射和图像着色等低层操作，已经探索了将引导滤波扩展到超分辨问题。这些方法中有基于上述局部滤波原理的局部方法和将上采样任务描述为全局能量最小化的全局方法。</p><h4 id="Learned-guided-super-resolution"><a href="#Learned-guided-super-resolution" class="headerlink" title="Learned guided super-resolution"></a>Learned guided super-resolution</h4><p>这些方法迄今为止都是非监督的。还有一系列的工作，从例子中学习如何向上采样源图像同时将高频细节从引导图像传输到目标输出上。<br>这中数据处理方法的优点有：从真实图像数据中学习如何最佳融合源图像和引导图像可能比手工启发式方法更好。<br>与所有监督学习的缺点一样：</p><ol><li>必须获取足够数量的训练数据</li><li>通过这样设计的算法可能在训练数据上过拟合，但是在真实场景下表现不好</li></ol><p>高分辨率“引导”图像通过标准语义分割网络生成“目标”分割映射，使用一个损失函数鼓励目标具有相同的目标。 标签分布作为低分辨率的源图。</p><hr><h3 id="Proposed-Method"><a href="#Proposed-Method" class="headerlink" title="Proposed Method"></a>Proposed Method</h3><blockquote><p><img src="http://sotavision.cn/showdoc/server/../Public/Uploads/2020-03-23/5e7825da3d75f.png" alt="" loading="lazy"></p></blockquote><h4 id="Notation-and-Preliminaries"><a href="#Notation-and-Preliminaries" class="headerlink" title="Notation and Preliminaries"></a>Notation and Preliminaries</h4><p>首先对符号做出如下的定义：Source Image记作S（S.size=M×M），Guided Image记作G（G.size=N×N×C），目标图像记作T（T.size=N×N）。N和M之间的数量关系是通过尺度为D的上采样算子决定的，即N=D·M。S中的第m个像素记作\(S_m\),\(S_m\)是由T中的D×D范围内的block即b(m)经过某种非线性均值方式获得到的，即<br>\[ s_m=\dfrac{1}{D^2} \sum_{n\in b(m)}t_n = {\left\langle t_n \right\rangle}_{b(m)} \]<br>目标是Given S and G，esitimate\(\hat{T}\)</p><h4 id="Proposed-Solution"><a href="#Proposed-Solution" class="headerlink" title="Proposed Solution"></a>Proposed Solution</h4><p>设Guided Image和Target Image之间的映射为参数记作\(\theta\)的函数\(f_{\theta}\), 二者满足如下的关系\(\hat{t}_ n =f_{\theta}(g_n)\)，s和t之间的距离度量采用1-Norm，目标函数建立如下:<br>\[ \hat{\theta}=\arg\min_{\theta}\sum_{m}\left|{s_m - \left\langle{f_{\theta}(g_n)}\right\rangle_{b(m)}}\right| \]<br>这里是一个很典型的参数估计问题：\(f_{\theta}\)的形式为多层卷积感知器，参数记作\(\theta\) 。为了避免ill-posed problem求解出过拟合的参数\(\theta\)，作者引入了对\(\theta\)的 2-Norm 正则项，如下所示。参数\(\lambda\)控制了正则化的强度。<br>\[ \hat{\theta}=\arg\min_{\theta}\sum_{m}\left|{s_m - \left\langle{f_{\theta}(g_n)}\right\rangle_{b(m)}}\right|+\lambda\left|{\theta}\right|^2 \]<br>最后为了避免由于色彩、角点等其它特征造成的映射歧义，\(f_{\theta}\)函数引入了坐标变量 \(x_{n}\)。坐标和图像数据分别进行训练，将最终的结果merge到一起来进行参数优化。形式如下式所示:<br>\[ \hat{\theta}=\arg\min_{\theta}\sum_{m}\left|{s_m - \left\langle{f_{\theta}(g_n,x_n)}\right\rangle_{b(m)}}\right|+\lambda\left|{\theta}\right|^2 \]<br>通过这种架构，还可以通过设置各个超参数λg(=0.001)，λx(=0.0001)，λhead(=0.0001)来不同地对每个分支进行处理。目标函数采用了梯度下降法进行求解。</p><blockquote><p><img src="http://sotavision.cn/showdoc/server/../Public/Uploads/2020-03-23/5e782eec854c0.png" alt="" loading="lazy"></p></blockquote><hr><h3 id="Experimental-Results"><a href="#Experimental-Results" class="headerlink" title="Experimental Results"></a>Experimental Results</h3><p>论文代码：<a href="https://github.com/riccardodelutio/PixTransform" target="_blank" rel="noopener">https://github.com/riccardodelutio/PixTransform</a></p><h4 id="Evaluation-Settings"><a href="#Evaluation-Settings" class="headerlink" title="Evaluation Settings"></a>Evaluation Settings</h4><p>在所有实验中，作者将目标分辨率设置为256×256像素。 在不同的上采样因子（即×4，×8，×16和×32）下评估算法，分别对应于64×64、32×32、16×16和8×8的源分辨率。 所对比的方法：BiCubic、Guided Filter、Fast Bilateral Solver、Static-Dynamic Solver、MSG-Net。对比指标：Percentage of Bad Pixels（PBP），均方误差，平均绝对误差。<br>\[PBP_{\delta} = \dfrac{1}{N^2}\sum_n[|\hat{t_n}-t_n|&gt;\delta]\]<br>这里没有使用SSIM和PSNR的原因，个人分析认为：Guided SR不同于重建，因此不存在质量评价的这一标准，只需要考量和Ground Truth之间的差异即可。</p><h4 id="Analysis"><a href="#Analysis" class="headerlink" title="Analysis"></a>Analysis</h4><blockquote><p><img src="http://sotavision.cn/showdoc/server/../Public/Uploads/2020-03-23/5e7830c4e71b9.png" alt="" loading="lazy"></p></blockquote><p>这里展示了不同的映射函数的结果</p><h4 id="Comparisons-with-state-of-the-arts"><a href="#Comparisons-with-state-of-the-arts" class="headerlink" title="Comparisons with state-of-the-arts"></a>Comparisons with state-of-the-arts</h4><p><img src="http://sotavision.cn/showdoc/server/../Public/Uploads/2020-03-23/5e78312f76337.png" alt="" loading="lazy"></p><p>由上面显示的结果可看出，这个方法在处理分割图像超分上是目前最优的</p><hr><p>###Conclusions</p><ol><li>提出了一种新的、无监督的引导超分辨率方法。关键思想是将问题看作是高分辨率引导图像向低分辨率源图像域的像素级变换。</li><li>即使在高上采样因子上该方法也能够恢复非常精细的结构和非常锋利的边缘</li><li>利用了机器学习的方法将这个问题转化成了引导图像到输入图像模态的pixel-to-pixel的映射</li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;https://cdn.jsdelivr.net/npm/aplayer@1.10/dist/APlayer.min.css&quot;&gt;&lt;script 
      
    
    </summary>
    
    
      <category term="论文笔记" scheme="http://alexzou14.github.io/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="深度学习" scheme="http://alexzou14.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="笔记" scheme="http://alexzou14.github.io/tags/%E7%AC%94%E8%AE%B0/"/>
    
      <category term="超分辨率" scheme="http://alexzou14.github.io/tags/%E8%B6%85%E5%88%86%E8%BE%A8%E7%8E%87/"/>
    
  </entry>
  
  <entry>
    <title>Deep Laplacian Pyramid Networks for Fast and Accurate Super-Resolution论文阅读笔记</title>
    <link href="http://alexzou14.github.io/2020/04/08/LapSRN/"/>
    <id>http://alexzou14.github.io/2020/04/08/LapSRN/</id>
    <published>2020-04-08T03:04:02.000Z</published>
    <updated>2020-04-08T03:10:11.532Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="https://cdn.jsdelivr.net/npm/aplayer@1.10/dist/APlayer.min.css"><script src="https://cdn.jsdelivr.net/npm/aplayer@1.10/dist/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="https://cdn.jsdelivr.net/npm/meting@1.2/dist/Meting.min.js"></script><h3 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h3><p>当时卷积神经网络经常被用到对单图像超分辨率的高质量重建中。在这篇论文中，作者提出了Laplacian金字塔超分辨率网络（LapSRN）来逐步重构高分辨率图像的子频带残差（sub-band residuals）。在每个金字塔的层，我们将粗分辨率特征图作为输入，预测高频残差（high-frequency residuals），并使用反卷积（transposed convolutions）来进行向上采样到finer level。LapSRN方法并没有将双三次差值（bicubic interpolation）作为预处理步骤，从而减小了计算的复杂性。作者使用了一个强大的Charbonnier损失函数对所提出的LapSRN进行了深入的监督，实现了高质量的重建。我们的网络通过渐进的重构，在一个前馈的过程（feed-forward）中产生了多尺度的预测，从而促进了资源感知(resorce-aware)的应用。对基准数据集进行了大量的定量和定性评估，结果表明，该算法在速度和精度方面优于最先进的方法。</p><hr><h3 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h3><p>当时CNN在单一图像超分问题上取得了巨大成功，并不断的发展。有很多方法被提出：</p><blockquote><p><img src="http://sotavision.cn/showdoc/server/../Public/Uploads/2020-04-08/5e8d2cb679ad6.png" alt="" loading="lazy"></p></blockquote><p>作者在这部分说明了当时CNN方法在SR问题上存在三个主要问题：</p><ol><li>当时的方法使用了预先定义的上采样操作，将输入图像提升到所需分辨率大小再输入进网络，这样增加了很多不必要的计算成本。</li><li>当时的方法在优化网络的时候，采用L2 loss function，这样会导致图像过于平滑</li><li>大多数方法都是一个上采样的步骤重建图像，这增加了训练大尺度因子的难度。</li></ol><p>本文中的LapSRN与其他方法的区别：</p><ol><li>直接用LR提取特征，并用Charbonnier的损失函数</li><li>速度上，作者的方法比一般的SRCNN要快，和FSRCNN相近但是质量要更高</li><li>渐进重构：在一条前馈通道中可以生成多个SR预测，更好的应用在不同的场景中。</li></ol><hr><h3 id="Related-work"><a href="#Related-work" class="headerlink" title="Related work"></a>Related work</h3><h4 id="SR-based-on-internal-databases"><a href="#SR-based-on-internal-databases" class="headerlink" title="SR based on internal databases"></a>SR based on internal databases</h4><p>一些方法利用自然图像中的自相似性属性，基于低分辨率输入图像的尺度空间金字塔构造LR-HR补丁对。然而内部数据库包含的相关训练补丁比外部数据库要多，LR-HR补丁对的数量可能不足以覆盖图像中较大的纹理变化。Singh等人将补丁分解成定向频率子带（directional frequency sub-bands ），并独立地在每个子带金字塔（sub-band pyramid）中确定更好的匹配。</p><h4 id="SR-based-on-external-databases"><a href="#SR-based-on-external-databases" class="headerlink" title="SR based on external databases"></a>SR based on external databases</h4><p>大量的SR方法通过从外部数据库中收集的图像对学习LR-HR映射，使用监督学习算法。不是直接在整个数据库上对复杂的补丁空间进行建模，而是通过k均值、稀疏字典或随机森林来划分图像数据库，并学习每个集群的局部线性回归。</p><h4 id="Convolutional-neural-networks-based-SR"><a href="#Convolutional-neural-networks-based-SR" class="headerlink" title="Convolutional neural networks based SR"></a>Convolutional neural networks based SR</h4><p>与在补丁空间中对LR-HR映射建模不同的是，SRCNN联合优化了所有的步骤，并在图像空间中学习了非线性映射。VDSR网络通过将网络深度从3个层增加到20个卷积层，显示了对SRCNN的显著改进。为了促使训练出一个更快收敛速度的更深的模型，VDSR训练网络来预测剩余的值，而不是实际的像素值。Wang等人将稀疏编码的领域知识与深度CNN结合起来，并训练一个级联网络（SCN），逐步将图像提升到所需的比例因子。Kim等人提出了一个具有深度递归层（DRCN）的浅层网络，以减少参数的数量。<br>LapSRN与上述方法不同：</p><ol><li>加入了残差和反卷积组成的上采样滤波器，有效的抑制了双三插值导致的重构伪影，降低计算复杂度。</li><li>利用Charbonnier损失函数提高重建精度</li><li>渐进重构的方法可以适用于不同的比例因子</li></ol><hr><h3 id="Proposed-Method"><a href="#Proposed-Method" class="headerlink" title="Proposed Method"></a>Proposed Method</h3><h4 id="Network-structure"><a href="#Network-structure" class="headerlink" title="Network structure"></a>Network structure</h4><p>网络结构如下图所示：</p><blockquote><p><img src="http://sotavision.cn/showdoc/server/../Public/Uploads/2020-04-08/5e8d3168dec26.png" alt="" loading="lazy"></p></blockquote><p><strong>Feature extraction branch</strong>：<br>通过stack convolution来获取非线性特征映射<br><strong>Image reconstruction branch</strong>：<br>在每一个pyramid level，最后加上deconv来提升图像的2x分辨率<br><strong>参数共享</strong><br>本文网络在两个地方进行参数共享，减少了参数量<br>1.在各个pyramid level之间参数共享， 称之为Recursive block</p><p>因为laplacian pyramid是在x2的基础上得到x4，由于各个level中的结构相似性，因此在各个level，参数得以共享<br>形式如下：</p><blockquote><p><img src="http://sotavision.cn/showdoc/server/../Public/Uploads/2020-04-08/5e8d319ef0847.png" alt="" loading="lazy"></p></blockquote><p>2.每个pyramid level之中参数共享</p><p>受DRCN和DRRN启发，作者在每个pyramid level中进行参数共享，如下图:</p><blockquote><p><img src="http://sotavision.cn/showdoc/server/../Public/Uploads/2020-04-08/5e8d326d42b45.png" alt="" loading="lazy"></p></blockquote><h4 id="Loss-function"><a href="#Loss-function" class="headerlink" title="Loss function"></a>Loss function</h4><p>本文认为相同的LR patch 可能有多种corresponding HR patches，而L2范数并不能capture the underlying multi-modal distributions of HR patches. 因此L2范数重建出的图像往往过平滑。<br>本文提出了一种抗噪性强的loss functions：</p><div>\[ L(\hat{y},y;\theta)=\dfrac{1}{N}\displaystyle\sum_{i=1}^N \sum_{s=1}^L \rho(\hat{y}_s^{(i)}-y_s^{(i)})=\dfrac{1}{N}\displaystyle\sum_{i=1}^N \sum_{s=1}^L \rho((\hat{y}_s^{(i)}-x_s^{(i)})-r_s^{(i)}) \]</div><p>其中\(\rho(x)=\sqrt{x^2+\varepsilon ^2}\)</p><hr><h3 id="Experiment"><a href="#Experiment" class="headerlink" title="Experiment"></a>Experiment</h3><h4 id="Experiments-Details"><a href="#Experiments-Details" class="headerlink" title="Experiments Details"></a>Experiments Details</h4><p>论文具体细节待复现。</p><h4 id="Model-analysis"><a href="#Model-analysis" class="headerlink" title="Model analysis"></a>Model analysis</h4><p><strong>Residual learning</strong>：</p><blockquote><p><img src="http://sotavision.cn/showdoc/server/../Public/Uploads/2020-04-08/5e8d3d9e29c77.png" alt="" loading="lazy"></p></blockquote><p>残差学习可以提高网络的收敛速度，是PSNR更稳定。<br><strong>Loss function</strong>：</p><blockquote><p><img src="http://sotavision.cn/showdoc/server/../Public/Uploads/2020-04-08/5e8d3dea66dc8.png" alt="" loading="lazy"></p></blockquote><p>L1 loss比L2 loss 产生的图像更加的清晰。<br><strong>Network depth</strong>：</p><blockquote><p><img src="http://sotavision.cn/showdoc/server/../Public/Uploads/2020-04-08/5e8d3e3107068.png" alt="" loading="lazy"></p></blockquote><p>当网络深度d=10的时候速度最快，效果最好。</p><h4 id="Comparisons-with-state-of-the-arts"><a href="#Comparisons-with-state-of-the-arts" class="headerlink" title="Comparisons with state-of-the-arts"></a>Comparisons with state-of-the-arts</h4><p>各个超分辨率算法实验结果：</p><blockquote><p><img src="http://sotavision.cn/showdoc/server/../Public/Uploads/2020-04-08/5e8d3e8ab7b3c.png" alt="" loading="lazy"><br><img src="http://sotavision.cn/showdoc/server/../Public/Uploads/2020-04-08/5e8d3ec48f9bc.png" alt="" loading="lazy"></p></blockquote><hr><h3 id="Conclusions"><a href="#Conclusions" class="headerlink" title="Conclusions"></a>Conclusions</h3><ol><li>本文是通过将低分辨率图像直接作为输入到网络中，通过逐级放大，在减少计算量的同时，也有效的提高了精度</li><li>提出了一种鲁棒的loss function, robust Charbonnier loss function.</li><li>对各个金字塔的level之间和每个level之内，通过recursive进行参数共享</li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;https://cdn.jsdelivr.net/npm/aplayer@1.10/dist/APlayer.min.css&quot;&gt;&lt;script 
      
    
    </summary>
    
    
      <category term="论文笔记" scheme="http://alexzou14.github.io/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="深度学习" scheme="http://alexzou14.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="笔记" scheme="http://alexzou14.github.io/tags/%E7%AC%94%E8%AE%B0/"/>
    
      <category term="超分辨率" scheme="http://alexzou14.github.io/tags/%E8%B6%85%E5%88%86%E8%BE%A8%E7%8E%87/"/>
    
  </entry>
  
  <entry>
    <title>SROBB:Targeted Perceptual Loss for Single Image Super-Resolution论文阅读笔记</title>
    <link href="http://alexzou14.github.io/2020/04/07/SROBB/"/>
    <id>http://alexzou14.github.io/2020/04/07/SROBB/</id>
    <published>2020-04-07T11:34:04.000Z</published>
    <updated>2020-04-07T12:01:38.635Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="https://cdn.jsdelivr.net/npm/aplayer@1.10/dist/APlayer.min.css"><script src="https://cdn.jsdelivr.net/npm/aplayer@1.10/dist/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="https://cdn.jsdelivr.net/npm/meting@1.2/dist/Meting.min.js"></script><h3 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h3><p>由于传统学习方法能力有限，没有考虑任何的语义信息产生了较大的误差，所以本文提出了一种更客观的地从知觉损失中获益的新方法。优化了一个基于深度网络的解码器，该解码器具有目标函数，可以使用相应的方式在不同的语义级别下处理图像。该方法利用我们提出的OBB(对象、背景和边界)标签，由分割标签生成，在考虑背景纹理相似性的同时，估计一个合适的边界感知损失。我们展示了我们提出的方法可以得到更真实的纹理和更清晰的边缘，并且在标准基准测试的定性结果和广泛的用户研究结果方面都优于其他最先进的算法。</p><hr><h3 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h3><p>最近，SISR的另一个突破是将感知损失函数用于训练前馈网络，而不是使用每像素损失函数，如均方误差(MSE)。它解决了MSE优化导致的纹理模糊问题，同时也带来了对抗性的损失，在感知图像质量方面实现了近真实感重建。虽然感知损失在SISR中取得了很大的成功，但将其应用于整体图像，不考虑语义信息，限制了其网络能力。在感知功能方面，最先进的方法使用不同层次的特征来恢复原始图像;这个选择决定了他们是关注局部信息(如边缘)、中层特性(如纹理)还是与语义信息对应的高层特性。在这些方法中，以同样的方法计算了整个图像的感知损失，这意味着在边缘、前景或图像背景上使用了相同级别的特征。</p><blockquote><p><img src="http://sotavision.cn/showdoc/server/../Public/Uploads/2020-03-24/5e796c1108431.png" alt="" loading="lazy"><br>图1所示。针对高分辨率图像的特点，提出了一种在训练过程中利用分割标签对不同语义层次的图像进行分割的方法;我们优化了我们的SISR模型，通过最小化感知误差，分别对应的边缘只在物体边界和纹理的背景区域。结果从左至右:原始图像，超分辨率图像仅使用像素丢失函数，像素丢失+感知丢失函数和像素丢失+目标感知丢失函数(our)。</p></blockquote><p>为了解决上述问题，我们提出了一种新的方法，以更客观的方式从知觉损失中获益。图1显示了我们所提议的方法的概述。特别是，我们使用像素级的分割注释来构建我们所提议的OBB标签，从而能够找到目标感知特性，这些特性可以用来最小化不同图像区域的适当损失:例如，边缘的损失和训练过程中图像纹理的损失。我们展示了我们的方法使用目标知觉损失优于其他最先进的算法在定性结果和用户研究实验，并导致更现实的纹理和更锋利的边缘。</p><hr><h3 id="Related-work"><a href="#Related-work" class="headerlink" title="Related work"></a>Related work</h3><p>这节作者主要回顾了近期SR的各种方法如SRCN，DRCN等。尽管针对SISR任务提出了不同的体系结构，但是基于优化的方法的行为主要是由目标函数的选择驱动的。这些方法所使用的目标函数大多包含一个损失项，即超分辨率HR图像与真实HR图像之间的像素距离。然而，由于所有可能的解决方案的像素平均，仅使用这个函数就会导致图像模糊和超平滑。<br>感知驱动的方法在视觉质量方面显著提高了图像的超分辨率。基于感知相似度的思想，提出了一种利用预先训练的特征提取器的特定层(如VGG)来最小化特征空间中的感知损失。在相似的工作中，提出的内容损失来生成具有自然图像统计的图像，它关注的是特征分布而不是仅仅比较外观。SRGAN提出在感知损失的基础上，利用对抗性损失来支持自然图像流形上的输出。虽然这些方法产生了接近于光真实感的结果，但它们以同样的方式估计了整个图像的重建误差，没有利用任何可以提高视觉质量的语义信息。<br>在这项工作中，我们研究了一种利用图像内部语义信息的新方法，产生具有精细结构的逼真的超分辨率图像。</p><hr><h3 id="Proposed-Method"><a href="#Proposed-Method" class="headerlink" title="Proposed Method"></a>Proposed Method</h3><p>作者引入了一个损失函数，它包含三项:1-像素级损失(MSE)、2-对抗性损失和3-新目标感知损失函数。<br>MSE和对抗性损失术语的定义如下:</p><ul><li>像素级别损失是目前为止SR中最常用的损失函数，它在图像域中计算原始图像与超分辨率图像之间的像素方向均方误差(MSE)。使用它作为一个独立的目标函数的主要缺点是解决了一个覆盖的重建。接受MSE损失训练的网络试图找到合理解决方案的像素平均，这导致感知质量较差，边缘和纹理中缺乏高频细节。</li><li>在SRGAN的启发下，我们将SR模型建立在一个对抗性的环境中，给出了一个可行的解决方案。特别地，我们使用一个额外的网络(鉴别器)，它可以与我们的SR解码器竞争。生成器(SR解码器)试图生成伪图像来欺骗鉴别器，而鉴别器的目标是将生成的结果与真实的HR图像区分开来。这种设置的结果在感知上优于通过最小化像素方面的MSE和经典感知损失得到的解决方案。</li></ul><h4 id="Targeted-perceptual-loss"><a href="#Targeted-perceptual-loss" class="headerlink" title="Targeted perceptual loss"></a>Targeted perceptual loss</h4><p>作者提出了一种新的方法，以有针对性的方式利用感知相似性，重建更有吸引力的边缘和纹理。</p><blockquote><p><img src="http://sotavision.cn/showdoc/server/../Public/Uploads/2020-03-24/5e7979791a063.png" alt="" loading="lazy"><br>图2. 选择不同CNN层来估计图像不同区域感知损失的效果，如边缘和纹理:(a)使用更深的卷积层(中层特征)，VGG-16[29]的ReLU 4-1， (b)使用早期卷积层(低层特征)，VGG-16网络的ReLU 1-2。</p></blockquote><p>目标丢失函数尝试在区域周围选择更真实的纹理，其中纹理的类型似乎很重要，例如，树，同时尝试解决边界区域周围更锋利的边缘。为此，作者首先在图像中定义三种类型的区域:1-背景、2-边界和3-对象，然后使用不同的函数计算每个区域的目标知觉损失。<br><strong>背景</strong>( \(\mathcal G_b\) ):我们认为四类是背景：“天空”、“植物”、“地面”和“水”。 我们选择这些类别是因为它们的具体外观；这些标签区域的整体纹理更多。我们计算中层CNN特征来估计SR和HR图像之间的感知相似性。 在这里，我们使用VGG-16的ReLU4-3层来实现这一目的。<br><strong>边界</strong>( \(\mathcal G_e\) ):所有分隔对象和背景的边缘都被认为是边界。通过一些预处理，我们将这些边缘扩展为一条贯穿所有边界的条带。我们估计了一个早期CNN层在SR和HR图像之间的特征距离，这个特征距离更侧重于低层空间信息，主要是边缘和斑点。特别地，我们最小化了VGG-16的ReLU 2-2层的感知损失。<br><strong>目标</strong>( \(\mathcal G_o\) ):由于现实世界中物体的形状和纹理种类繁多，因此判断早期的特征对于感知缺失功能来说是更合适还是更适合使用深层的特征对于感知缺失功能来说是一个挑战;例如，在斑马的图像中，更清晰的边缘比整体纹理更重要。尽管如此，强迫网络估计树的精确边缘可能会误导优化过程。因此，我们不考虑任何类型的知觉损失对定义为对象的领域加权为零，并依赖于MSE和对抗性损失。</p><p>为了计算特定图像区域的感知损失，我们对语义类进行二值分割掩码(对感兴趣的类像素值为1，其他地方像素值为0)。每个掩码都分类地表示图像的不同区域，并分别按元素乘上HR图像和估计的超分辨率图像SR。我们可以得出，掩码HR与超分辨率图像在特征空间中的所有非零距离都对应于该图像可见区域的内容:边界( \(M_{OBB}^{boundaries}\) )用掩码对应边缘，背景( \(M_{OBB}^{background}\) )用掩码对应纹理。<br>整体目标感知损失函数为:<br>\(L_{perc.} = \alpha\cdot \mathcal G_{e}(I^{SR}\circ M_{OBB}^{boundary}, I^{HR}\circ M_{OBB}^{boundary})\)<br>\(+\beta\cdot \mathcal G_{b}(I^{SR}\circ M_{OBB}^{boundary}, I^{HR}\circ M_{OBB}^{boundary})+\gamma\cdot \mathcal G_{o}\)</p><h4 id="OBB-Object-background-and-boundary-label"><a href="#OBB-Object-background-and-boundary-label" class="headerlink" title="OBB: Object, background and boundary label"></a>OBB: Object, background and boundary label</h4><p>为了充分利用基于感知丢失的图像超分辨率，作者通过提出的目标丢失函数来增强语义细节(对象、背景和边界出现在图像上的地方)。此外，现有的分割任务注释，如coco-stuff只提供了关于对象和背景的空间信息，没有使用表示边缘区域的类，即本文中的边界。因此，作者提出了我们的标注方法(图3)，为图像的语义信息提供更好的空间控制。</p><blockquote><p><img src="http://sotavision.cn/showdoc/server/../Public/Uploads/2020-03-24/5e797f7c995a5.png" alt="" loading="lazy"><br>图3. 构造一个OBB标签。我们根据“对象”、“背景”或“边界”类的初始像素级标签为每个区域分配一个类。</p></blockquote><p>为了得到一个较厚的条带，将不同的类分开，作者通过计算了一个d1大小的圆盘的膨胀。将结果区域标记为“boundary”类。将分割标签中的“sky”、“plant”、“ground”和“water”类作为“Background”。所有剩余的对象类都被认为是“对象”类。</p><h4 id="Architecture"><a href="#Architecture" class="headerlink" title="Architecture"></a>Architecture</h4><blockquote><p><img src="http://sotavision.cn/showdoc/server/../Public/Uploads/2020-03-24/5e79803d2eab1.png" alt="" loading="lazy"><br>图4. SR解码器原理图。我们在训练SR解码器的同时，使用了目标感知损失以及MSE和对抗性损失。在该模式中，k、n和s分别对应内核大小、特征映射数和步长大小。</p></blockquote><p>为了与SRGAN方法进行公平的比较，并对所提出的目标感知丢失进行消融研究，我们使用与SRGAN相同的SR解码器。</p><hr><h3 id="Experiment"><a href="#Experiment" class="headerlink" title="Experiment"></a>Experiment</h3><h4 id="Experiments-Details"><a href="#Experiments-Details" class="headerlink" title="Experiments Details"></a>Experiments Details</h4><p>为了创建OBB标签，作者使用COCO-Stuff数据集中的一组随机的50K图像，其中包含用于分割任务的91个类的语义标签。<br>训练过程分两步进行;首先，对SR解码器进行了25个周期的预训练，仅以像素方向的均方误差为损失函数。在此基础上，增加了目标知觉损失函数和对抗性损失函数，训练时间延长了55个迭代。每一项的权重在新的有针对性的知觉丧失,α和β,设置为2×10−6和1.5×10−6,分别。与SRGAN相同，将对抗性损失函数和MSE损失函数的权重分别设置为1.0和1×10−3。我们将用于生成OBB标签的磁盘直径d1设置为2.0。在这两个步骤中都使用了Adam优化器。将学习率设置为1×10−3，然后每20个迭代衰减10倍。我们还交替优化了与SRGAN提出的参数相似的鉴别器。</p><h4 id="Qualitative-Results"><a href="#Qualitative-Results" class="headerlink" title="Qualitative Results"></a>Qualitative Results</h4><p>我们的方法主要是利用分割标签对具有边界和背景的感知损失项的解码器进行优化。虽然我们没有将知觉损失专门应用于物体区域，但是我们的实验表明，训练后的模型与其他方法相比，在某种程度上泛化了，它重建了更真实的物体。</p><blockquote><p><img src="http://sotavision.cn/showdoc/server/../Public/Uploads/2020-03-24/5e79828535900.png" alt="" loading="lazy"><br><img src="http://sotavision.cn/showdoc/server/../Public/Uploads/2020-03-24/5e798298319e8.png" alt="" loading="lazy"></p></blockquote><h4 id="Quantitative-Results"><a href="#Quantitative-Results" class="headerlink" title="Quantitative Results"></a>Quantitative Results</h4><blockquote><p><img src="http://sotavision.cn/showdoc/server/../Public/Uploads/2020-03-24/5e7982c291776.png" alt="" loading="lazy"><br>表1 Set5和Set14测试集的“婴儿”和“狒狒”图像的双三次插值、LapSRN、SRGAN和SROBB(我们的)的比较。最佳度量(SSIM, PSNR，lpip)用粗体突出显示。可视化比较如图5所示。</p></blockquote><p>学习感知图像Patch相似度(LPIPS)度量是最近引入的一种基于参考的图像质量评估度量，它的目的是估计两幅图像之间的感知相似度。</p><hr><h3 id="Conclusions"><a href="#Conclusions" class="headerlink" title="Conclusions"></a>Conclusions</h3><ol><li>针对基于CNN的单图像超分辨率，提出了一种新的目标感知丢失函数。</li><li>提出的目标函数用相关的损失项对图像的不同区域进行惩罚，即在训练过程中对边缘和纹理使用边缘损失和纹理损失。</li><li>引入了我们的OBB标签，从像素分割标签创建，以提供更好的空间控制的语义信息的图像。</li><li>提出的定向知觉损失训练在感知效果上更令人满意，并且优于目前最先进的SR方法。</li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;https://cdn.jsdelivr.net/npm/aplayer@1.10/dist/APlayer.min.css&quot;&gt;&lt;script 
      
    
    </summary>
    
    
      <category term="论文笔记" scheme="http://alexzou14.github.io/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="深度学习" scheme="http://alexzou14.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="笔记" scheme="http://alexzou14.github.io/tags/%E7%AC%94%E8%AE%B0/"/>
    
      <category term="超分辨率" scheme="http://alexzou14.github.io/tags/%E8%B6%85%E5%88%86%E8%BE%A8%E7%8E%87/"/>
    
  </entry>
  
  <entry>
    <title>Natural and Realistic Single Image Super-Resolution with Explicit Natural Manifold Discrimination论文阅读笔记</title>
    <link href="http://alexzou14.github.io/2020/04/07/NatSR/"/>
    <id>http://alexzou14.github.io/2020/04/07/NatSR/</id>
    <published>2020-04-07T10:52:26.000Z</published>
    <updated>2020-04-07T11:30:50.910Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="https://cdn.jsdelivr.net/npm/aplayer@1.10/dist/APlayer.min.css"><script src="https://cdn.jsdelivr.net/npm/aplayer@1.10/dist/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="https://cdn.jsdelivr.net/npm/meting@1.2/dist/Meting.min.js"></script><h3 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h3><p>许多使用CNN网络的SISR模型使用失真导向（distortion-oriented）的损失函数，这类模型很难恢复真实图像纹理内容和细节，看起来较模糊，没有较好的视觉效果。恢复真实纹理和细节在图像超分辨领域仍然是一项挑战，目前有一些关于这方面的工作，如SRGAN，EnhancedNet，SFT-GAN，但是，这些方法在生成这些不真实（fake）细节时，通常会产生不需要的伪影，整幅图像看起来总有一些不自然。文中，提出了一种重建真实超分辨率图像的方法，重构的图像具有非常高的视觉效果并保持图像的真实性。作者在低层图像域定义了真实先验（naturalness prior），并约束重构图像在自然流形（natural manifold）。</p><hr><h3 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h3><p>单一图像超分率（SISR）是一个传统的图像恢复问题，主要目的是将低分辨率图像恢复成高分辨率图像。由于SISR是一个不定问题，不同的SR方法会产生不一样的图像。目前做SR的方法无论是使用MSE还是使用GAN的方法，都会产生不自然的图像效果。使用MSE损失函数会使得恢复图像过于平滑，使用现有感知损失如SRGAN，EhanceNet,会产生一些不需要的细节、伪影等。如下图：</p><blockquote><p><img src="http://sotavision.cn/showdoc/server/../Public/Uploads/2020-03-24/5e79cd373c660.png" alt="" loading="lazy"></p></blockquote><p>所以本文借鉴了SR的领域先验知识，提出一个方法来约束低级领域先验来代替高级语言。设计一个判别器判定图像的自然程度，以此为监督对图像做SR，这是这篇论文的基本思路。<br>本文主要贡献有：</p><ol><li>设计了基于CNN网络的自然流形鉴别器（natural manifold discriminator）。</li><li>基于不规则残差学习的CNN结构，distortion-oriented，即fractal residual super-resolution (FRSR)。</li><li>我们提出了一种面向感知的SISR方法，（NatSR)网络，可生成真实纹理和自然细节，获得高视觉质量。</li></ol><hr><h3 id="Related-work"><a href="#Related-work" class="headerlink" title="Related work"></a>Related work</h3><h4 id="Single-Image-Super-Resolution"><a href="#Single-Image-Super-Resolution" class="headerlink" title="Single Image Super-Resolution"></a>Single Image Super-Resolution</h4><p>这里主要讲了早期SR方法，不管是传统方法还是深度学习方法都是一鉴别失真为导向的，目的是为了达到更高的PSNR。这些方法往往会导致生成的图像过于平滑，感知细节丢失等问题。</p><h4 id="Perception-Oriented-Super-Resolution"><a href="#Perception-Oriented-Super-Resolution" class="headerlink" title="Perception Oriented Super-Resolution"></a>Perception Oriented Super-Resolution</h4><p>由于上面SR方法为了达到更高的PSNR从而使得生成图像更平滑，所以近期面向感知的方法受到广泛关注，并且Johson提出像素域感知不是感知质量的最优解，特征空间损失才更符合人类感知模型。所以近期的SFT-GAN通过调整目标像素语义类别来限制特征空间达到更好的效果。</p><h4 id="Modeling-the-SISR"><a href="#Modeling-the-SISR" class="headerlink" title="Modeling the SISR"></a>Modeling the SISR</h4><p>论文认为LR和HR在频率域上的关系如下所示，如果对LR做SR的结果可能会出现图下面的情形:</p><blockquote><p><img src="http://sotavision.cn/showdoc/server/../Public/Uploads/2020-03-24/5e79d02537df0.png" alt="" loading="lazy"></p></blockquote><p>所以LR-HR的对应关系可以描述成：\( I_{LR}=h(I_{HR})^{\downarrow} \)<br>由于SISR是为给定的LR找到HR，所以它通常被建模为找到条件似然\( p(I_{HR}|I_{LR}) \)。</p><hr><h3 id="Proposed-Method"><a href="#Proposed-Method" class="headerlink" title="Proposed Method"></a>Proposed Method</h3><h4 id="Natural-Manifold-Discrimination"><a href="#Natural-Manifold-Discrimination" class="headerlink" title="Natural Manifold Discrimination"></a>Natural Manifold Discrimination</h4><h5 id="Designing-Natural-Manifold"><a href="#Designing-Natural-Manifold" class="headerlink" title="Designing Natural Manifold"></a>Designing Natural Manifold</h5><p>根据上图这里将图像HR space分成如下图的三类，如下图所示：</p><blockquote><p><img src="http://sotavision.cn/showdoc/server/../Public/Uploads/2020-03-24/5e79d1009156b.png" alt="" loading="lazy"></p></blockquote><p>其中模糊集A可以定义成：\( A={I_A|I_A=(1-\alpha)h(I_{LR}^{\uparrow})+\alpha I_{HR}} \)<br>模糊集A中的图像降置为LR可以描述为：</p><p>\( h(I_A)^{\downarrow} \)<br>\( =h((1-\alpha)h(I_{LR}^{\uparrow})+\alpha I_{HR})^{\downarrow} \)<br>\( =h((1-\alpha)h(I_{LR}^{\uparrow}))^{\downarrow}+h(\alpha I_{HR})^{\downarrow} \)<br>\( =(1-\alpha)h(I_{LR}^{\uparrow})^{\downarrow}+\alpha h(I_{HR})^{\downarrow} \)<br>\( =(1-\alpha)h(I_{LR})+\alpha h(I_{LR}) \)<br>\( =I_{LR} \)</p><p>噪声集B可以描述为：\( B={I_B|I_B=I_{HR}+n} \)<br>模糊集B中的图像降置为LR可以描述为：</p><p>\( h(I_B)^{\downarrow} \)<br>\( =h(I_{HR}+n)^{\downarrow} \)<br>\( =h(I_{HR})^{\downarrow}+h(n)^{\downarrow} \)<br>\( =h(I_{HR})^{\downarrow} \)<br>\( =I_{LR} \)</p><p>论文论证了模糊和带噪的SR图片下采样后都会等价于LR图片。</p><h5 id="Natural-Manifold-Discriminator"><a href="#Natural-Manifold-Discriminator" class="headerlink" title="Natural Manifold Discriminator"></a>Natural Manifold Discriminator</h5><p>这里论文构建了两类这样的图片，然后设计了一个判别器进行分类学习，具体设置见论文，判别器如下所示：</p><blockquote><p><img src="http://sotavision.cn/showdoc/server/../Public/Uploads/2020-03-24/5e79d4d569e85.png" alt="" loading="lazy"></p></blockquote><p>其中这个判别器的交叉熵损失函数定义为：</p><div>    \[ - \text{E}_{x\in{A\cup B}}[\log(1-D_{NM}(x))]-\text{E}_{x\in{N}}[\log(D_{NM}(x))] \]</div><h4 id="Natural-and-Realistic-Super-Resolution"><a href="#Natural-and-Realistic-Super-Resolution" class="headerlink" title="Natural and Realistic Super-Resolution"></a>Natural and Realistic Super-Resolution</h4><p>论文的SR网络，使用了残差dense block，网络结构如下所示：</p><blockquote><p><img src="http://sotavision.cn/showdoc/server/../Public/Uploads/2020-03-24/5e79d72147ae5.png" alt="" loading="lazy"><br><img src="http://sotavision.cn/showdoc/server/../Public/Uploads/2020-03-24/5e79d698d801c.png" alt="" loading="lazy"><br><img src="http://sotavision.cn/showdoc/server/../Public/Uploads/2020-03-24/5e79d73b9ccbf.png" alt="" loading="lazy"></p></blockquote><h4 id="Training-Loss-Function"><a href="#Training-Loss-Function" class="headerlink" title="Training Loss Function"></a>Training Loss Function</h4><ul><li>Reconstruction Loss</li></ul><p>\[ L_{Recon}=\text{E}[\left|I_{HR}-I_{SR} \right|_1] \]</p><ul><li>Naturalness Loss</li></ul><p>\[ L_{Natural}=\text{E}[-\log(D_{MN}(I_{SR}))] \]</p><ul><li>Adversarial Loss<br>使用相对真实生成对抗网络RaGAN，与ESRGAN使用的GAN相同。</li></ul><div>    \[ L_{G}=-\text{E}_{x_r\sim \text P_r}[\log(\tilde{D}(x_r))]-\text{E}_{x_f\sim \text P_g}[\log(1-\tilde{D}(x_f))] \]    \[ L_{D}=-\text{E}_{x_f\sim \text P_g}[\log(\tilde{D}(x_f))]-\text{E}_{x_r\sim \text P_r}[\log(1-\tilde{D}(x_r))] \]</div><p>其中：</p><div>    \[ \tilde{D}(x_r)=\text{sigmoid}(C(x_r)-\text E_{x_f\sim \text P_g}[C(x_f)]) \]    \[ \tilde{D}(x_f)=\text{sigmoid}(C(x_f)-\text E_{x_r\sim \text P_r}[C(x_r)]) \]</div>- Overall Loss<p>\[ L=\lambda_1L_{Recon}+\lambda_2L_{Natural}+\lambda_3L_{G} \]<br>FRSR:\( \lambda_2=\lambda_3=0 \)<br>NatSR：\( \lambda_1=1,\lambda_2=10^{-3}\text{and}\lambda_3=10^{-3} \)</p><hr><h3 id="Experiment"><a href="#Experiment" class="headerlink" title="Experiment"></a>Experiment</h3><h4 id="Experiments-Details"><a href="#Experiments-Details" class="headerlink" title="Experiments Details"></a>Experiments Details</h4><p>实验细节可以参考官方代码：<a href="https://github.com/JWSoh/NatSR" target="_blank" rel="noopener">https://github.com/JWSoh/NatSR</a>.</p><h4 id="FR-IQA-Results"><a href="#FR-IQA-Results" class="headerlink" title="FR-IQA Results"></a>FR-IQA Results</h4><blockquote><p><img src="http://sotavision.cn/showdoc/server/../Public/Uploads/2020-03-24/5e79dc87c7f6d.png" alt="" loading="lazy"></p></blockquote><ul><li>考虑到参数的数量，我们的FRSR也是一种有效的方法</li><li>对于面向感知的方法，我们的方法比SRGAN和EnhanceNet在像素域上更接近原始图像</li></ul><h4 id="NR-IQA-Results"><a href="#NR-IQA-Results" class="headerlink" title="NR-IQA Results"></a>NR-IQA Results</h4><blockquote><p><img src="http://sotavision.cn/showdoc/server/../Public/Uploads/2020-03-24/5e79dced15055.png" alt="" loading="lazy"><br>从图像评价指标上看，NatSR算法并不是最好的，但是作者认为，在视觉效果上，NatSR效果更好，在图像纹理和细节方面表现更好。</p></blockquote><hr><h3 id="Conclusions"><a href="#Conclusions" class="headerlink" title="Conclusions"></a>Conclusions</h3><ol><li>提出了一种基于残差密集块和分形残差学习的网络结构</li><li>提出了基于CNN的自然流形判别器（NMD）</li><li>设计了一种感知损失函数</li><li>与具有相似参数的模型相比，我们的方法具有相当大的增益。</li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;https://cdn.jsdelivr.net/npm/aplayer@1.10/dist/APlayer.min.css&quot;&gt;&lt;script 
      
    
    </summary>
    
    
      <category term="论文笔记" scheme="http://alexzou14.github.io/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="深度学习" scheme="http://alexzou14.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="笔记" scheme="http://alexzou14.github.io/tags/%E7%AC%94%E8%AE%B0/"/>
    
      <category term="超分辨率" scheme="http://alexzou14.github.io/tags/%E8%B6%85%E5%88%86%E8%BE%A8%E7%8E%87/"/>
    
  </entry>
  
  <entry>
    <title>Image Super-Resolution via Deep Recursive Residual Network论文阅读笔记</title>
    <link href="http://alexzou14.github.io/2020/04/07/DRRN/"/>
    <id>http://alexzou14.github.io/2020/04/07/DRRN/</id>
    <published>2020-04-07T10:47:09.000Z</published>
    <updated>2020-04-07T10:50:52.829Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="https://cdn.jsdelivr.net/npm/aplayer@1.10/dist/APlayer.min.css"><script src="https://cdn.jsdelivr.net/npm/aplayer@1.10/dist/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="https://cdn.jsdelivr.net/npm/meting@1.2/dist/Meting.min.js"></script><h3 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h3><p>因为卷积神经网络可以有效的学习低分辨率到高分辨率图像的非线性映射，所以在当时基于卷积神经网络的模型在单一图像超分辨率的问题上取得了巨大成功。同时CNN模型要耗费大量的运算资源，网络参数非常多。因此本文提出了一种非常深的残差网络，使用递归学习来控制参数数目和增加深度残差学习减少网络的训练难度。</p><hr><h3 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h3><p>由于SISR的目标是从LR图像上还原其中高频信息，所以这个问题应用很广，在医疗成像，安全监控等。在近期，CNN方法广泛的应用与解决各种不定问题，SISR问题自从SRCNN将CNN方法引入到图像超分辨率问题中，CNN方法在超分领域取得了很大的进步。作者回顾了当时众多方法的优缺点，SRCNN的计算成本高，后面就有人提出了ESPCN用亚像素卷积恢复图像，提高了图像恢复的效率。SRCNN的网络很浅，得到的结论不全对，后面Kim提出的VDSR利用比较深的网络和跳跃连接来恢复图像。后面为了减少网络参数，提出了DRCN利用递归来减少网络的参数。现在该领域存在的问题就是非常深的网络需要非常多的参数，大模型网络需要很大的存储空间，对移动系统的适应度很差。<br>作者受到以上方法的启发，提出了一种深度残差递归网络（DRRN）。<br>DRRN的创新点：</p><ol><li>在VDSR中只用了输入到输出端的全局残差（GRL），可以有效的降低训练深网络的难度，在此基础之上作者又添加了一个局部残差（LRL），有效的减少图像经深网络处理后细节的丢失。</li><li>和DRCN相比，作者提出了一个由多个残差单元组成的残差块进行参数共享，并且设计了一个多路径结构的递归块来解决梯度爆炸。</li></ol><hr><h3 id="Related-work"><a href="#Related-work" class="headerlink" title="Related work"></a>Related work</h3><blockquote><p><img src="http://sotavision.cn/showdoc/server/../Public/Uploads/2020-04-07/5e8c50dca8b27.png" alt="" loading="lazy"></p></blockquote><h4 id="SResNet"><a href="#SResNet" class="headerlink" title="SResNet"></a>SResNet</h4><p>ResNet中的残差学习框架简化了深度网络的训练，残差单元公式化为：<br>\[ F(\text x)=H(\text x)-\text x, \hat{\text x}=U(\text x)=\sigma(F(\text x,W)+h(\text x)) \]<br>h(x)为恒等映射(identity mapping)，F为残差函数，残差网络的核心就是去学习增加的残差函数F，这样的设计让网络更加容易训练并且不会产生过拟合</p><h4 id="VDSR"><a href="#VDSR" class="headerlink" title="VDSR"></a>VDSR</h4><p>VDSR中在输入的低分辨率图片和输出的高分辨图片之间引入了GRL，VDSR主要有三个特点</p><ul><li>VDSR在残差分支中使用了20个权重层，这能让网络的reception field增大</li><li>GRL(Global Residual Learning)能让VDSR快速收敛</li><li>通过尺度扩展，单个VDSR网络对不同尺度的图像具有较强的鲁棒性</li></ul><h4 id="DRCN"><a href="#DRCN" class="headerlink" title="DRCN"></a>DRCN</h4><p>添加更多的权层会引入更多的参数，其中模型可能会过拟合，并且可能会导致模型更大难以存储和复现，为了解决这些问题，作者在网络中引入了一个递归层，每层的递归层都是监督式的，包含三个部分:</p><ul><li>第一部分为embedding net，对输入中的特征进行提取，</li><li>第二层为为inference net，相当于特征的非线性变换， 将T个递归堆叠在一个递归层中，在这些递归之间共享权重</li><li>第三层为reconstruction net，即特征图重建</li></ul><hr><h3 id="Proposed-Method"><a href="#Proposed-Method" class="headerlink" title="Proposed Method"></a>Proposed Method</h3><h4 id="Network-structure"><a href="#Network-structure" class="headerlink" title="Network structure"></a>Network structure</h4><blockquote><p><img src="http://sotavision.cn/showdoc/server/../Public/Uploads/2020-04-07/5e8c50ce1fa5a.png" alt="" loading="lazy"></p></blockquote><p>作者在DRRN中将GRL和递归网络结合在一起使用，将多个残差单元堆积在一起，如下两幅图所示，在ResNet中，不同的残差单元对identity branch使用不同的输入，但是在DRRN中，使用了多路径结构，所有残差单元共享相同的identity branch的输入</p><blockquote><p><img src="http://sotavision.cn/showdoc/server/../Public/Uploads/2020-04-07/5e8c51101c2e9.png" alt="" loading="lazy"></p></blockquote><h4 id="Residual-Unit"><a href="#Residual-Unit" class="headerlink" title="Residual Unit"></a>Residual Unit</h4><blockquote><p><img src="http://sotavision.cn/showdoc/server/../Public/Uploads/2020-04-07/5e8c515dc7328.png" alt="" loading="lazy"></p></blockquote><p>在DRRN的残差单元将激活层放到了权重层的前面，通过别人的论文可知，这样的设计让网络更加容易训练,而每个残差单元之间的残差路径有助于学习高度复杂的有限元结构，同一路径有助于训练过程中的梯度反向传播，与ResNet中的链模式相比，这种模式更有利于学习，不容易过拟合，具体公式如下， 由于残差单元被递归学习，权重w在一个递归区中共享，但是在不同的递归区内不同。<br>\[ H^u=F(H^{u-1},W^u)+H^{u-1} \]<br>\[ H^u= G(H^{u-1})=F(H^{u-1},W)+H^{0} \]</p><h4 id="Recursive-Block"><a href="#Recursive-Block" class="headerlink" title="Recursive Block"></a>Recursive Block</h4><p>DRRN的网络结构如下图所示，通过叠加几个递归块，然后用卷积层重建LR和HR图像之间的残差。然后将剩余图像从输入LR image添加到全局标识映射中。整个DRRN网络结构如下图所示。实际上，VDSR可以看作是DRRN的一个特例。，当残差单元为0时，DRRN变为VDSR</p><blockquote><p><img src="http://sotavision.cn/showdoc/server/../Public/Uploads/2020-04-07/5e8c533c2b4c1.png" alt="" loading="lazy"></p></blockquote><p>DRRN网络中递归块的个数和残差单元数最终影响网络的性能，所以第u个残差单元可以表示为：<br>\[ H_b^u=G(H^{u-1})=F(H_b^{u-1},W_b)+H_b^0 \]<br>第b个递归块的输出可以表达为：<br>\[ \text x_b=H_b^UG^{(U)}(f_b(\text x_{b-1}))=G(G(\cdots(G(f_b(\text x_{b-1}))\cdots)) \]</p><h4 id="Network-Structure"><a href="#Network-Structure" class="headerlink" title="Network Structure"></a>Network Structure</h4><p>DRRN总的网络层数为：\( d=(1+2\times U)\times B +1 \)，R代表递归块,则输出的表达式为：<br>\[ \text y=D(\text x)=f_{Rec}(R_B(R_{B-1}(\cdots(R_1(\text x))\cdots)))+\text x \]</p><h4 id="Loss-function"><a href="#Loss-function" class="headerlink" title="Loss function"></a>Loss function</h4><p>作者在这里使用了MSE的loss function：<br>\[ L(\Theta)=\dfrac{1}{2N}\displaystyle\sum_{i=1}^N ||\tilde{\text x}^{(i)}-D(\text x^{i})||^2 \]</p><hr><h3 id="Experiment"><a href="#Experiment" class="headerlink" title="Experiment"></a>Experiment</h3><h4 id="Experiments-Details"><a href="#Experiments-Details" class="headerlink" title="Experiments Details"></a>Experiments Details</h4><p>实验细节可以参考官方代码：<a href="https://github.com/tyshiwo/DRRN" target="_blank" rel="noopener">https://github.com/tyshiwo/DRRN</a> CVPR17.<br>数据增强：图像翻转，旋转（90°、180°、270°），旋转后的图像水平翻转，数据量增加7倍，运用不同的放大倍数图像进行同一个模型训练，训练图像大小为31x31。</p><p>训练策略：SGD min-batch 128, momentum 0.9，初始学习率为0.1，每10轮学习率下降一半，同时使用梯度自动裁切技术。</p><p>卷积核相关：卷积核大小为3x3，个数为128</p><h4 id="Study-of-B-and-U"><a href="#Study-of-B-and-U" class="headerlink" title="Study of B and U"></a>Study of B and U</h4><blockquote><p><img src="http://sotavision.cn/showdoc/server/../Public/Uploads/2020-04-07/5e8c586b4e1ee.png" alt="" loading="lazy"></p></blockquote><p>B1U25可以使用更少的参数实现最先进的结果。</p><h4 id="Comparisons-with-state-of-the-arts"><a href="#Comparisons-with-state-of-the-arts" class="headerlink" title="Comparisons with state-of-the-arts"></a>Comparisons with state-of-the-arts</h4><p>利用52层的网络结构，利用递残差网络结构有两种实现方式：B1U25(k=297K), B17U1(k=7375K)(k表示参数数量)，与其他模型的比较结果如下PSNR和SSIM：</p><blockquote><p><img src="http://sotavision.cn/showdoc/server/../Public/Uploads/2020-04-07/5e8c591a3a9a0.png" alt="" loading="lazy"></p></blockquote><p>本文还利用了一种信息保真度(Information Fidelity Criterion IFC )的评价指标：</p><blockquote><p><img src="http://sotavision.cn/showdoc/server/../Public/Uploads/2020-04-07/5e8c593338956.png" alt="" loading="lazy"></p></blockquote><hr><h3 id="Conclusions"><a href="#Conclusions" class="headerlink" title="Conclusions"></a>Conclusions</h3><ol><li>针对单图像超分辨率的深度递归网络（DRRN）。</li><li>提出了一个增强的残差单元结构在一个递归块中递归学习</li><li>提出了局部残差和全局残差共同作用于网络，权值共享等提高网络的性能。</li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;https://cdn.jsdelivr.net/npm/aplayer@1.10/dist/APlayer.min.css&quot;&gt;&lt;script 
      
    
    </summary>
    
    
      <category term="论文笔记" scheme="http://alexzou14.github.io/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="深度学习" scheme="http://alexzou14.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="笔记" scheme="http://alexzou14.github.io/tags/%E7%AC%94%E8%AE%B0/"/>
    
      <category term="超分辨率" scheme="http://alexzou14.github.io/tags/%E8%B6%85%E5%88%86%E8%BE%A8%E7%8E%87/"/>
    
  </entry>
  
  <entry>
    <title>EnhanceNet：Single ImageSuper-Resolution through Automated Texture Synthesis论文阅读笔记</title>
    <link href="http://alexzou14.github.io/2020/04/06/EhanceNet/"/>
    <id>http://alexzou14.github.io/2020/04/06/EhanceNet/</id>
    <published>2020-04-06T09:43:53.000Z</published>
    <updated>2020-04-06T09:57:27.146Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="https://cdn.jsdelivr.net/npm/aplayer@1.10/dist/APlayer.min.css"><script src="https://cdn.jsdelivr.net/npm/aplayer@1.10/dist/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="https://cdn.jsdelivr.net/npm/meting@1.2/dist/Meting.min.js"></script><p>文章地址：<a href="https://arxiv.org/pdf/1612.07919.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1612.07919.pdf</a></p><h3 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h3><p>在当时，图像超分辨率的性能评判指标是通过PSNR来测量的，但是PSNR与人感知相关性差，PSNR过小会造成图像过于平滑缺少纹理看上去不够自然。作者提出了一种结合纹理损失的自动纹理合成的新颖应用，该损失专注于创建逼真的纹理，而不是针对训练过程中像素精度的地面真实图像再现进行优化。 通过在对抗训练环境中使用前馈全卷积神经网络，我们可以在高放大倍率下显着提高图像质量。 在大量数据集上进行的广泛实验证明了我们方法的有效性，在定量和定性基准方面都产生了最新的结果。</p><hr><h3 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h3><p>超分辨率问题的不定性：对HR图像进行降采样时，大量不同的HR图像会产生相同的LR图像。这导致SISR成为高度复杂的问题。一个关键问题是大量降采样因子导致高频信息丢失，从而使超分辨图像中的纹理区域变得模糊，过分平滑且外观不自然。如下图所示：</p><blockquote><p><img src="http://sotavision.cn/showdoc/server/../Public/Uploads/2020-03-30/5e8155f778b87.png" alt="" loading="lazy"></p></blockquote><p>原因是，选择了当前最先进的方法所采用的目标函数：大多数系统将HR Ground Truth 图像与其从HR真实图像重建之间的像素平均均方误差（MSE）降至最低。 然而，LR观察与人类对图像质量的感知之间的相关性很差。 虽然易于最小化，但最佳的MSE估计器返回许多可能解的平均值，这使得SISR结果看起来不自然且难以置信。 在超分辨率的背景下，这种均值回归问题是众所周知的事实，但是，对自然图像的高维多峰分布进行建模仍然是一个具有挑战性的问题。</p><blockquote><p><img src="http://sotavision.cn/showdoc/server/../Public/Uploads/2020-03-30/5e81576a3af3b.png" alt="" loading="lazy"></p></blockquote><p>作者因此提出了一种新的文理合成网络改进方案，结合了对抗训练的感知损失。</p><hr><h3 id="Related-work"><a href="#Related-work" class="headerlink" title="Related work"></a>Related work</h3><p>作者在这部分简要的说明了早期传统方法容易产生模糊和伪影。当时比较受欢迎的方法有基于外部实例的方法和董超老师提出来的深度学习方法。因为这些模型都是通过优化最小均方误差（MSE）得到的，所以这些结果都趋于模糊和缺少高频细节的。一种感知损失的提出，虽然PSNR更低但是也锐化了输出结果。对抗网络就是当时可以得到比较好的锐化结果，但是同时这个网络也会是的生成图像添加一些不必要的伪影。作者就是在这些工作基础上展开的。</p><hr><h3 id="Proposed-Method"><a href="#Proposed-Method" class="headerlink" title="Proposed Method"></a>Proposed Method</h3><h4 id="Single-image-super-resolution"><a href="#Single-image-super-resolution" class="headerlink" title="Single image super-resolution"></a>Single image super-resolution</h4><p>一张高分辨率图片\( I_{HR}\in[0,1]^{\alpha w\times \alpha h\times c} \)下采样为低分辨率图像：<br>\( I_{LR}=d_{\alpha}(I_{HR})\in [0,1]^{w\times h\times c} \)<br>使用下采样操作表示为：<br>\( d_{\alpha}:[0,1]^{\alpha w\times \alpha h\times c}\rightarrow [0,1]^{w\times h\times c} \)<br>固定比例因子\( \alpha &gt;1 \)，图像宽度\(w\)，高度\(h\)和颜色通道\(c\)。 SISR的任务是提供一个从\(I_{LR}\)估计\(I_{HR}\)的近似逆\(f\approx d^{-1}\)：<br>由于下采样操作d是non-injective 的，并且存在大量可能的图像\(I_{est}\)，而\(d(I_{est})=I_{LR}\)保持不变，因此该问题很复杂。<br>最近的学习方法旨在通过使当前估计和ground truth图像之间的欧几里得损失\(\left |{I_{est}-I_{HR}}\right|_2^2\)最小化，通过多层神经网络来近似\(f\)。 尽管这些模型通过PSNR可以得出出色的结果，但所得图像往往看起来模糊并且缺少原始图像中存在的高频纹理。 这是SISR模棱两可的直接结果：由于下采样会从输入图像中去除高频信息，因此没有方法希望以像素为单位重现所有精细的细节。 因此，即使是最先进的模型也要学习在那些区域中生成所有可能纹理的均值，以使输出图像的欧几里得损失最小化。</p><h4 id="Network-structure"><a href="#Network-structure" class="headerlink" title="Network structure"></a>Network structure</h4><p>网络结构采用全卷积的方式，使得输入图像可以是任意尺寸。受到VGG网络的启发，卷积核全部采用3*3的尺寸，在保持一定量参数的情况下构建更深的网络。网络的输入是低分辨率图像，在网络末端采用最近邻的方法上采样达到高分辨率图像的尺寸，这样有利于降低计算复杂度，参数的初始化采用Xavier，网络的学习目标是超分辨率图像与输入的低分辨率图像线性插值的差，整个网络架构如下图所示</p><blockquote><p><img src="http://sotavision.cn/showdoc/server/../Public/Uploads/2020-03-30/5e81610a96e33.png" alt="" loading="lazy"></p></blockquote><p>超分辨率重建网络+感知网络+对抗网络(判别器)<br>第一不用bicubic interpolation,原因是，过多的引入冗余，而且计算量大。第二，不用transpose convolution ,原因是，过多的引入冗余，同时感受野增加了。另外，transconvolution容易产生checkerboard artifacts，需要加入而外的惩罚loss。<br>超分辨率重建网络=ResNet block(下采样path)+nearest upsampling(上采样路径).</p><h4 id="Loss-function"><a href="#Loss-function" class="headerlink" title="Loss function"></a>Loss function</h4><h5 id="Pixel-wise-loss-in-the-image-space"><a href="#Pixel-wise-loss-in-the-image-space" class="headerlink" title="Pixel-wise loss in the image-space"></a>Pixel-wise loss in the image-space</h5><div>    \( L_E=\left\|I_{est}-I_{HR}\right\|_2^2 \)</div>其中 <div>    \(\left\|I\right\|_2^2=\dfrac{1}{whc}\displaystyle\sum_{w,h,c}(I_{w,h,c})^2\)</div>pixel-wise loss强调的是两幅图像之间每个对应像素的匹配，这与人眼的感知结果有所区别。通过pixel-wise loss训练的图片通常会较为平滑，缺少高频信息。即使输出图片具有较高的PSNR，视觉效果也并没有很突出。<h5 id="Perceptual-loss-in-feature-space"><a href="#Perceptual-loss-in-feature-space" class="headerlink" title="Perceptual loss in feature space"></a>Perceptual loss in feature space</h5><p>这个损失的计算是把\(I_{est}\)和\(I_{HR}\)送入一个函数\(\phi\)计算在\(\phi\)映射下的L2损失：<br>\(L_P=\left|\phi (I_{est})-\phi(I_{HR})\right|_2^2\)<br>我们避免了要求网络输出图像与原始高分辨率图像pixel-wise上的一致，而是鼓励两幅图具有相似的特征。<br>一般\(\phi\)可以用预训练好的VGG19在第二和第五池化层的输出。</p><h5 id="Texture-matching-loss"><a href="#Texture-matching-loss" class="headerlink" title="Texture matching loss"></a>Texture matching loss</h5><p>该损失的表示为：\(L_P=\left|G(\phi (I_{est}))-G(\phi(I_{HR}))\right|_2^2\)其中G为gram matrix:\(G(F)=FF^T\)<br>这个和分割迁移的损失类似，用在这的主要目的是生成和HR图像相似的局部纹路。经验是16*16生成的效果最好。</p><h5 id="Adversarial-training"><a href="#Adversarial-training" class="headerlink" title="Adversarial training"></a>Adversarial training</h5><p>对抗训练是一种最新技术，已被证明是产生逼真的图像的有用机制。 在原始设置中，对生成网络G进行了训练， 同时，判别网络D被训练以区分来自数据集的真实图像x和生成的样本G(z)。这种方法导致了minimax游戏，其中训练了生成器以使其loss最小化:\(L_A=-\log (D(G(z)))\)<br>鉴别器loss最小化:\(L_D=-\log (D(x))-\log (1-D(G(z)))\)</p><blockquote><p><img src="http://sotavision.cn/showdoc/server/../Public/Uploads/2020-03-30/5e8166cd4e466.png" alt="" loading="lazy"></p></blockquote><hr><h3 id="Experiment"><a href="#Experiment" class="headerlink" title="Experiment"></a>Experiment</h3><h4 id="Experiments-Details"><a href="#Experiments-Details" class="headerlink" title="Experiments Details"></a>Experiments Details</h4><p>具体实验细节待复现</p><h4 id="Effect-of-different-losses"><a href="#Effect-of-different-losses" class="headerlink" title="Effect of different losses"></a>Effect of different losses</h4><p>我们使用表2中列举的Loss组合进行训练，并将结果展示在表3中。更多的结果在补充中。</p><blockquote><p><img src="http://sotavision.cn/showdoc/server/../Public/Uploads/2020-03-30/5e81674cef1ed.png" alt="" loading="lazy"><br><img src="http://sotavision.cn/showdoc/server/../Public/Uploads/2020-03-30/5e81678163846.png" alt="" loading="lazy"></p></blockquote><h4 id="Evaluation-of-perceptual-quality"><a href="#Evaluation-of-perceptual-quality" class="headerlink" title="Evaluation of perceptual quality"></a>Evaluation of perceptual quality</h4><p>在ImgeNet数据集上对用户进行问卷调查，其中91.0%选择由ENet-PAT产生的图像</p><hr><h3 id="Conclusions"><a href="#Conclusions" class="headerlink" title="Conclusions"></a>Conclusions</h3><p>本文主要贡献：</p><ul><li>我们提出了一种体系结构，该体系结构能够通过欧几里得损失训练或对抗性训练，感知损失和新提出的用于超分辨率的纹理转移损失的新颖组合，通过定量和定性测量产生最新结果 。</li></ul><p>存在的限制性：</p><ul><li>由于SISR是一个严重的问题，因此仍然存在一些局限性。 虽然由ENet-PAT生成的图像看上去逼真，但它们与像素像素的真实图像不匹配。 此外，对抗训练有时会在输出中产生伪影，这些伪影会大大减少，但不会增加纹理损失，因此无法完全消除。</li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;https://cdn.jsdelivr.net/npm/aplayer@1.10/dist/APlayer.min.css&quot;&gt;&lt;script 
      
    
    </summary>
    
    
      <category term="论文笔记" scheme="http://alexzou14.github.io/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="深度学习" scheme="http://alexzou14.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="笔记" scheme="http://alexzou14.github.io/tags/%E7%AC%94%E8%AE%B0/"/>
    
      <category term="超分辨率" scheme="http://alexzou14.github.io/tags/%E8%B6%85%E5%88%86%E8%BE%A8%E7%8E%87/"/>
    
  </entry>
  
  <entry>
    <title>Enhanced Deep Residual Networks for Single Image Super-Resolution论文阅读笔记</title>
    <link href="http://alexzou14.github.io/2020/04/06/EDSR/"/>
    <id>http://alexzou14.github.io/2020/04/06/EDSR/</id>
    <published>2020-04-06T09:41:06.000Z</published>
    <updated>2020-04-06T09:43:06.824Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="https://cdn.jsdelivr.net/npm/aplayer@1.10/dist/APlayer.min.css"><script src="https://cdn.jsdelivr.net/npm/aplayer@1.10/dist/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="https://cdn.jsdelivr.net/npm/meting@1.2/dist/Meting.min.js"></script><p>论文地址：<a href="https://arxiv.org/pdf/1707.02921.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1707.02921.pdf</a></p><h3 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h3><p> 随着深度卷积神经网络（DCNN）的发展，最近在图像超分辨率方面的研究也取得了进展。尤其，残差学习技术表现出很好的性能。在这篇文章中，作者介绍了：</p><ol><li>提出一种增强的深度超分辨率网络（enhanced deep super-resolution，简称 EDSR），其性能超过当前最先进的超分辨率（SR）方法。</li><li>作者提出的模型通过删除常规残差网络中不必要的模块进行优化，实现了显著的性能提高。</li><li>提出一种新的多尺度深度超分辨率系统（multi-scale deep super-resolution，简称MDSR）和训练方法，可以将单个模型中不同的放大因子（upscaling factors）重建为高分辨率图像。</li></ol><hr><h3 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h3><p>作者首先介绍了DeepNet的局限性：</p><ol><li>模型的重构性能对较小的体系结构变化很敏感，相同模型基于不同的初始化和训练技巧可以得到不同等级的性能表现。</li><li>大多数现行的SR算法将不同的scale视为独立的问题，并没有利用SR间不同scale的关联，所以每一种scale都需要单独训练。</li></ol><p>VDSR模型是可以同时解决不同scale问题的，这说明尺度特定的模型的训练是有冗余的，但是VDSR的LR是需要通过bicubic成HR尺寸的，没有直接在LR空间进行计算，损失了LR空间的图像细节并增大了计算复杂度。<br>为了解决这个问题呢，作者主要采用了以下两点：</p><ol><li>移除了不必要的模型，主要就是移除了BN</li><li>设计了带有一个单一主分支的基准（多尺度 —— multi-scale）模块 ，含有 B = 16 的残差模块，所以大部分参数都可以在不同尺度间进行共享，</li></ol><hr><h3 id="Related-work"><a href="#Related-work" class="headerlink" title="Related work"></a>Related work</h3><p>这里作者回顾了超分辨率研究的方法，从传统方法到神经网络方法，然后重点说明了深度学习方法在近几年的发展。董超老师提出的SRCNN将深度学习方法引入到超分领域中，让后不断的发展，VDSR引入了跳层连接有效的缓解了梯度爆炸问题等等。<br>作者还提出MSE或者L2 loss可能并不能保证在PSNR和SSIM上有更好的性能，所以这里采用了L1 Loss函数。</p><hr><h3 id="Proposed-Method"><a href="#Proposed-Method" class="headerlink" title="Proposed Method"></a>Proposed Method</h3><h4 id="EDSR"><a href="#EDSR" class="headerlink" title="EDSR"></a>EDSR</h4><p>提出了一个增强版的结构简单的残差网络模型：</p><blockquote><p><img src="http://sotavision.cn/showdoc/server/../Public/Uploads/2020-03-31/5e82ec74e94c1.png" alt="" loading="lazy"></p></blockquote><p>去掉BN后，相同的计算资源下，EDSR就可以堆叠更多的网络层或者使每层提取更多的特征，从而得到更好的性能表现。但是增加特征图的数量将使训练过程不稳定，作者提出了一个residual scaling来解决该问题。在每个residual block的最后一个卷积层后添加一个constant scaling layer，可以帮助稳定训练（尤其是在应用很多的filter时）。</p><blockquote><p><img src="http://sotavision.cn/showdoc/server/../Public/Uploads/2020-03-31/5e82ed24cf1e5.png" alt="" loading="lazy"></p></blockquote><p>当训练缩放因子为×3，×4的EDSR时，作者用预训练过的×2的网络来初始化模型参数。这个策略加速了训练并且提升了最后的性能表现。对于×4，使用预训练×2的模型训练过程中会收敛很快。</p><blockquote><p><img src="http://sotavision.cn/showdoc/server/../Public/Uploads/2020-03-31/5e82ee74cee60.png" alt="" loading="lazy"></p></blockquote><h4 id="MDSR"><a href="#MDSR" class="headerlink" title="MDSR"></a>MDSR</h4><p>在MDSR网络头部设置了一个预处理模块，用来减少输入图像不同尺寸的差异，每个预处理模块包含两个卷积核为5×5的残差块。在MDSR的末尾，不同的scale-upsampling模型平行设置来处理各种尺度的SR预测。</p><p>MDSR withB = 80 and F = 64.</p><blockquote><p><img src="http://sotavision.cn/showdoc/server/../Public/Uploads/2020-03-31/5e82ef178192c.png" alt="" loading="lazy"></p></blockquote><hr><h3 id="Experiment"><a href="#Experiment" class="headerlink" title="Experiment"></a>Experiment</h3><h4 id="Experiments-Details"><a href="#Experiments-Details" class="headerlink" title="Experiments Details"></a>Experiments Details</h4><ul><li>实验数据用的是比赛提供的DIV2K，它有800张训练图像，100张验证图像，100张测试图像。每张都是2k分辨率。</li><li>预处理是，每张图像减去DIV2K的总平均值。另外在训练时，损失函数用L1而不是L2，源码用torch7封装。</li><li>提一下MDSR的训练，是*2 *3 *4三中的尺度随机混合作为训练集，在更新梯度时，只有对应尺度的那部分参数更新。</li></ul><h4 id="Geometric-Self-ensemble"><a href="#Geometric-Self-ensemble" class="headerlink" title="Geometric Self-ensemble"></a>Geometric Self-ensemble</h4><p>这是一个很神奇的方法，测试时，把图像90度旋转以及翻转，总共有8种不同的图像，分别进网络然后变换回原始位置，8张图像再取平均。这个方法可以使测试结果有略微提高。Note that geometric self-ensemble is valid only for symmetric downsampling methods such as bicubic downsampling.</p><h4 id="Comparisons-with-state-of-the-arts"><a href="#Comparisons-with-state-of-the-arts" class="headerlink" title="Comparisons with state-of-the-arts"></a>Comparisons with state-of-the-arts</h4><blockquote><p><img src="http://sotavision.cn/showdoc/server/../Public/Uploads/2020-03-31/5e82f02b03e36.png" alt="" loading="lazy"><br><img src="http://sotavision.cn/showdoc/server/../Public/Uploads/2020-03-31/5e82f0647f1ae.png" alt="" loading="lazy"></p></blockquote><hr><h3 id="Conclusions"><a href="#Conclusions" class="headerlink" title="Conclusions"></a>Conclusions</h3><ol><li>在Residual block中删除了BN，卷积完成后也不Relu。并控制了残差规模来降低训练复杂度。</li><li>MDSR具有尺度相关的模块和共享的主网络，能够在统一的框架内有效地处理各种尺度的超分辨率问题</li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;https://cdn.jsdelivr.net/npm/aplayer@1.10/dist/APlayer.min.css&quot;&gt;&lt;script 
      
    
    </summary>
    
    
      <category term="论文笔记" scheme="http://alexzou14.github.io/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="深度学习" scheme="http://alexzou14.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="笔记" scheme="http://alexzou14.github.io/tags/%E7%AC%94%E8%AE%B0/"/>
    
      <category term="超分辨率" scheme="http://alexzou14.github.io/tags/%E8%B6%85%E5%88%86%E8%BE%A8%E7%8E%87/"/>
    
  </entry>
  
  <entry>
    <title>Image Restoration Using Convolutional Auto-encoders with Symmetric Skip Connections论文阅读笔记</title>
    <link href="http://alexzou14.github.io/2020/04/06/RED/"/>
    <id>http://alexzou14.github.io/2020/04/06/RED/</id>
    <published>2020-04-06T09:31:55.000Z</published>
    <updated>2020-04-06T09:36:03.663Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="https://cdn.jsdelivr.net/npm/aplayer@1.10/dist/APlayer.min.css"><script src="https://cdn.jsdelivr.net/npm/aplayer@1.10/dist/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="https://cdn.jsdelivr.net/npm/meting@1.2/dist/Meting.min.js"></script><h3 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h3><p>本文提出了一种非常深的深卷积自动编码网络。因为卷积层在消除图像失真的同时捕获了图像内容的抽象细节，反卷积层具有向上采样特征图和恢复图像细节的能力，所以这个网络有很多的卷积和反卷积层构成，学习了从LR图像到HR图像端到端的映射。由于更深的网络很难训练存在梯度爆炸的问题，所以本文采用了将卷积层和反卷积层使用跳跃连接。卷积层和反卷积层的跳跃连接有两个优点:</p><ol><li>这种设计允许信号直接反向传播到底层，从而解决了梯度爆炸的问题，使得训练深度网络更加容易。提高了性能</li><li>这样的设计有利于恢复更干净的图像。</li></ol><p>同时这个模型可以用于图像去噪，去除JPEG压缩伪影和图像嵌入等问题。</p><hr><h3 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h3><p>图像恢复的任务是从其损坏的观察中恢复干净的图像，这已知是一种不适定的逆问题。 通过适应不同类型的腐败分布，相同的数学模型适用于图像去噪和超分辨率等问题。当时，深度神经网络已经在图像处理和计算机视觉任务中表现出优越的性能，从高级识别， 语义分割到低级去噪，超分辨率，去模糊，修复和恢复来自压缩图像的原始图像。 尽管深度神经网络取得了进展，但一些研究问题仍有待解决。 例如，更深层次的网络通常可以实现更好的性能吗？ 可以设计一个能够处理不同级别降置的深度模型吗？<br>观察DNN最近在图像处理任务上的优越性能，我们提出了一种基于卷积神经网络（CNN）的图像恢复框架。我们观察到，为了获得良好的恢复性能，训练非常深的模型是有益的。同时，我们表明，由于大容量网络的优势，在处理多个不同级别的损坏时，单个网络可以实现非常有前途的性能。具体而言，所提出的框架学习从损坏的图像到干净的图像的端到端完全卷积映射。该网络由多层卷积和反卷积运算符组成。由于更深层次的网络往往更难以训练，我们建议将卷积层和反卷积层对称地连接到跳层连接，训练过程收敛得更快，更有可能获得高质量的局部最优。我们的主要贡献总结如下。</p><ol><li>提出了一个非常深入的图像恢复网络结构，卷积层作为特征处理编码，反卷积层解码恢复图像。</li><li>提出了在卷积层与反卷积层之间增加跳跃连接层，跳跃连接有助于将梯度反向传播到底层，并将图像细节传播到顶层。</li><li>将这个网络应用在其他各种应用场景中取得了良好的恢复性能。</li></ol><hr><h3 id="Related-work"><a href="#Related-work" class="headerlink" title="Related work"></a>Related work</h3><p>关工作文献中的图像修复已经做了大量工作。诸如Total variation，BM3D算法和基于字典学习的方法等传统方法在图像去噪和超分辨率等图像恢复主题上表现出非常好的性能。由于图像恢复通常是一个不适定的问题，正规化的使用已被证明是必不可少的。堆积去噪自动编码器是最着名的DNN模型之一，可用于图像恢复。组合稀疏编码和DNN预训练与去噪自动编码器，用于低级视觉任务，如图像去噪和修复。其他基于神经网络的方法，如多层感知器和CNN用于图像去噪，以及DNN用于图像或视频超分辨率和压缩伪影减少这些年来一直在积极研究。Burger等人提出了一种基于补丁的算法，用简单的多层感知器学习。他们还得出结论，对于大型网络，大型训练数据，神经网络可以实现最先进的图像去噪性能。 Jain和Seung 提出了一个完全卷积的CNN用于去噪。他们发现CNN提供与小波和马尔可夫随机场（MRF）方法相当甚至更优越的性能。崔等人在多尺度上对输入图像采用非局部自相似（NLSS）搜索，然后以逐层方式使用协同局部自编码器进行超分辨率。董超老师等人提出直接学习低/高分辨率图像之间的端到端映射。王等人认为，传统稀疏编码所代表的领域专业知识可以结合起来，以实现进一步改进的结果。 DNN方法的一个优点是这些方法纯粹是数据驱动的，并且没有关于噪声分布的假设。</p><hr><h3 id="Proposed-Method"><a href="#Proposed-Method" class="headerlink" title="Proposed Method"></a>Proposed Method</h3><h4 id="Network-structure"><a href="#Network-structure" class="headerlink" title="Network structure"></a>Network structure</h4><p>网络整体结构，如下图：</p><blockquote><p><img src="http://sotavision.cn/showdoc/server/../Public/Uploads/2020-04-06/5e8ae477ba16d.png" alt="" loading="lazy"></p></blockquote><p>该框架完全是卷积和反卷积的。在每次卷积和反卷积之后添加校正层。卷积层充当特征提取器，它保留图像中对象的主要组件，同时消除损坏。然后组合去卷积层以恢复图像内容的细节。反卷积层的输出是输入图像的“干净”版本。此外，跳过连接也从卷积层添加到其对应的镜像反卷积层。<br>卷积特征映射以元素方式传递给解卷积特征映射并与解卷积特征映射求和，并在校正后传递到下一层。对于低级别的图像恢复问题，我们更喜欢在网络中不使用池化或解除池化，因为通常池化会丢弃对这些任务至关重要的有用图像细节。</p><h4 id="Deconvolution-decoder"><a href="#Deconvolution-decoder" class="headerlink" title="Deconvolution decoder"></a>Deconvolution decoder</h4><p>卷积：特征提取，随卷积进行，图像特征被提取，同时噪声的效果被降低，经过多层卷积后，图像的特征被提取出来，也降低了噪声的影响。</p><p>反卷积：针对特征的上采样，完成由图像特征到图像的转换，由于利用的是过滤后的噪声后的图像特征，因此达到了降噪、图像修复的目的。</p><p>文中通过实验说明利用反卷积结构而不用padding 和upsampling的原因，如下图，反卷积对图像细节有补偿作用。</p><blockquote><p><img src="http://sotavision.cn/showdoc/server/../Public/Uploads/2020-04-06/5e8af334c374c.png" alt="" loading="lazy"></p></blockquote><h4 id="Skip-connections"><a href="#Skip-connections" class="headerlink" title="Skip connections"></a>Skip connections</h4><p>作用：</p><p>1）保留更多的图像细节，协助反卷积层完成图像的恢复工作；</p><p>2）反向传播过程中的梯度反向，减少梯度消失，加快模型训练，文章有一些公式推导说明跳层连接对梯度变化的好处，详见论文；</p><p>利用图像修复实验说明跳层作用：</p><blockquote><p><img src="http://sotavision.cn/showdoc/server/../Public/Uploads/2020-04-06/5e8af364b2498.png" alt="" loading="lazy"><br><img src="http://sotavision.cn/showdoc/server/../Public/Uploads/2020-04-06/5e8af36f42064.png" alt="" loading="lazy"></p></blockquote><h4 id="Training"><a href="#Training" class="headerlink" title="Training"></a>Training</h4><p>局部模块结构：</p><blockquote><p><img src="http://sotavision.cn/showdoc/server/../Public/Uploads/2020-04-06/5e8af2f3cbf08.png" alt="" loading="lazy"></p></blockquote><p>卷积和反卷积层可以表示为：\( F(X)=\max(0,W_k \ast X+B_k) \)其中\( * \)代表卷积和反卷积操作。<br>ReLU的激活函数可以表达为：\( F(X_1,X_2)=\max(0,X_1+X_2) \)</p><p>使用的loss function为MSE：\( L(\Theta)=\dfrac{1}{N}\displaystyle\sum_{i=1}^N||F(X^i;\Theta)-Y^i||_F^2 \)<br>使用了SGD的方法来进行训练。</p><hr><h3 id="Experiment"><a href="#Experiment" class="headerlink" title="Experiment"></a>Experiment</h3><h4 id="Experiments-Details"><a href="#Experiments-Details" class="headerlink" title="Experiments Details"></a>Experiments Details</h4><p>参考code: <a href="https://github.com/SSinyu/RED_CNN" target="_blank" rel="noopener">https://github.com/SSinyu/RED_CNN</a></p><h4 id="Network-analysis"><a href="#Network-analysis" class="headerlink" title="Network analysis"></a>Network analysis</h4><p><strong>filter number</strong>:<br>滤波器大小为3x3,patch大小为50x50,跳层层数为2，不同的滤波个数，32,64,128，结果如下：</p><blockquote><p><img src="http://sotavision.cn/showdoc/server/../Public/Uploads/2020-04-06/5e8af5a0db0b2.png" alt="" loading="lazy"></p></blockquote><p><strong>filter size</strong>:<br>滤波器个数为64,patch大小为50x50,跳层层数为2，不同的滤波器大小为，3x3,5x5,7x7,9x9，结果如下：</p><blockquote><p><img src="http://sotavision.cn/showdoc/server/../Public/Uploads/2020-04-06/5e8af5d533455.png" alt="" loading="lazy"></p></blockquote><p><strong>training patch size</strong>:<br>滤波器个数为64,滤波器大小为3x3 ,跳层层数为2，不同的patch size，25x25, 50x50,75x75,100x100结果如下：</p><blockquote><p><img src="http://sotavision.cn/showdoc/server/../Public/Uploads/2020-04-06/5e8af618ac327.png" alt="" loading="lazy"></p></blockquote><p><strong>step size pf skip connections</strong>:<br>滤波器个数为64,滤波器大小为3x3 ,跳层层数为2,4,7，不同的patch size为50x50结果如下：</p><blockquote><p><img src="http://sotavision.cn/showdoc/server/../Public/Uploads/2020-04-06/5e8af63939f59.png" alt="" loading="lazy"></p></blockquote><p>结论：滤波器个数越多，大小越大，训练图像越大，跳层越少性能越好；</p><h4 id="Comparisons-with-state-of-the-arts"><a href="#Comparisons-with-state-of-the-arts" class="headerlink" title="Comparisons with state-of-the-arts"></a>Comparisons with state-of-the-arts</h4><p>文章中进行了图像去噪、超解像、JPEG deblocking、Non-blind deblurring、图像修复实验，列举一下超解像相关结果：</p><blockquote><p><img src="http://sotavision.cn/showdoc/server/../Public/Uploads/2020-04-06/5e8af66598c25.png" alt="" loading="lazy"></p></blockquote><p>与VDSR和DRCN进行了比较：</p><blockquote><p><img src="http://sotavision.cn/showdoc/server/../Public/Uploads/2020-04-06/5e8af686692bb.png" alt="" loading="lazy"></p></blockquote><hr><h3 id="Conclusions"><a href="#Conclusions" class="headerlink" title="Conclusions"></a>Conclusions</h3><ol><li>作者提出了一个非常深入的图像恢复网络结构，卷积层作为特征处理编码，反卷积层解码恢复图像。</li><li>作者提出了在卷积层与反卷积层之间增加跳跃连接层，跳跃连接有助于将梯度反向传播到底层，并将图像细节传播到顶层。</li><li>将这个网络应用在其他各种应用场景中取得了良好的恢复性能。</li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;https://cdn.jsdelivr.net/npm/aplayer@1.10/dist/APlayer.min.css&quot;&gt;&lt;script 
      
    
    </summary>
    
    
      <category term="论文笔记" scheme="http://alexzou14.github.io/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="深度学习" scheme="http://alexzou14.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="笔记" scheme="http://alexzou14.github.io/tags/%E7%AC%94%E8%AE%B0/"/>
    
      <category term="超分辨率" scheme="http://alexzou14.github.io/tags/%E8%B6%85%E5%88%86%E8%BE%A8%E7%8E%87/"/>
    
  </entry>
  
  <entry>
    <title>Deeply-Recursive Convolutional Network for Image Super-Resolution论文阅读笔记</title>
    <link href="http://alexzou14.github.io/2020/04/05/DRCN/"/>
    <id>http://alexzou14.github.io/2020/04/05/DRCN/</id>
    <published>2020-04-05T15:41:36.000Z</published>
    <updated>2020-04-05T16:54:36.505Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="https://cdn.jsdelivr.net/npm/aplayer@1.10/dist/APlayer.min.css"><script src="https://cdn.jsdelivr.net/npm/aplayer@1.10/dist/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="https://cdn.jsdelivr.net/npm/meting@1.2/dist/Meting.min.js"></script><h3 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h3><p>本文提出了使用深度递归卷积网络（DRCN）的图像超分辨率方法（SR）。 DRCN有一个非常深的递归层（最多16个递归）。增加递归深度可以提高性能，而不会为附加卷积引入新参数。由于网络特别深，所以存在梯度爆炸的问题，无法用标准梯度下降来学习。作者就利用率跳跃连接和递归监督的方式解决了这一问题。</p><hr><h3 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h3><p>对于图像超分辨率（SR），卷积网络的感受野决定了可以利用的上下文信息的量来推断丢失的高频分量。由于SR是一个不定逆向问题，收集和分析更多的邻域像素可对恢复下采样后可能丢失的信息提供更多线索。各种计算机视觉任务的深度卷积网络（DCN）通常使用非常大的感受野，在扩展感受野的许多方法中，增加网络深度是一种可能的方式。利用大于1×1的过滤器大小的卷积（转换）层或者减少中间表示的维度的池层可以使用的。 这两种方法都有缺点：层引入更多的参数和池层。网络层通常丢弃一些像素级别的信息。<br>对于图像恢复问题，如超分辨率和去噪，图像细节非常重要。因此，这些问题的大多数深度学习方法不能使用池化技术。通过添加新的网络层来增加深度，基本上会引入更多的参数。可能会出现两个问题。首先，很可能会过度拟合，需要更多的训练数据，使模型变得越来越大。<br>为了解决这些问题，作者使用深度递归卷积网络（DRCN）。DRCN根据需要重复地使用相同的卷积层参数。参数的数量不会增加，而执行了更多的递归。作者发现用广泛使用的随机梯度下降法优化的DRCN不容易收敛。这是由于爆炸/消失的梯度。提出了一个高性能的模型结构，能够捕捉像素长程的依赖，在保持较小模型的情况下，有更宽的感受野。<br>作者提出两种方法来缓解训练的难度：对所有递归进行监督，使用从输入到重建层的跳跃连接。</p><hr><h3 id="Related-work"><a href="#Related-work" class="headerlink" title="Related work"></a>Related work</h3><h4 id="Single-Image-Super-Resolution"><a href="#Single-Image-Super-Resolution" class="headerlink" title="Single-Image Super-Resolution"></a>Single-Image Super-Resolution</h4><p>早期单一图像超分辨率早期使用插值的方法，但是效果很差，后面又提出了统计先验和内部patch方法，最近基于学习的方法SRCN的提出，证明了端到端的方法在SR问题上的可行性。</p><h4 id="Recursive-Neural-Network-in-Computer-Vision"><a href="#Recursive-Neural-Network-in-Computer-Vision" class="headerlink" title="Recursive Neural Network in Computer Vision"></a>Recursive Neural Network in Computer Vision</h4><p>递归神经网络在语义分割和特征提取上有很多的应用，但是在图像超分上还是没有应用。</p><hr><h3 id="Proposed-Method"><a href="#Proposed-Method" class="headerlink" title="Proposed Method"></a>Proposed Method</h3><h4 id="Basic-Model"><a href="#Basic-Model" class="headerlink" title="Basic Model"></a>Basic Model</h4><p>整个网络分三个部分，每个部分都只有一层隐层，只有inference 网络是递归的。<br>结构如下图：</p><blockquote><p><img src="http://sotavision.cn/showdoc/server/../Public/Uploads/2020-04-05/5e89e70041905.png" alt="" loading="lazy"></p></blockquote><p><strong>Embedding network</strong>:用于从RGB和灰度图像产生feature map，相当于特征提取,可以用一下表达式表示：<br>\( H_{-1}=\max (0,W_{-1} \ast \text{x}+b_{-1}) \)<br>\( H_{0}=\max (0,W_{0}\ast H_{-1}+b_{0}) \)<br>\( f_{1}(\text x)=H_0 \)<br>其中 \( f_1(\text{x}) \) 表示特征提取的输出。<br><strong>Inference network</strong>:解决超分辨率，相当于特征的非线性变换,其中 \( g(H)=\max(0,W \ast H+b) \) ，递归可以表示为：<br>\( H_d=g(H_{d-1})=\max(0,W \ast H_{d-1}+b) \),<br>故递归层输出为：\( f_2(H)=(g\circ g\circ g\circ \cdots \circ)g(H)=g^D(H) \)<br><strong>reconstruction network</strong>:用于把多个通道的转成三个通道的图片。重建层可以表达为：<br>$$H_{D+1}=\max(0,W_{D+1} \ast H_D+b_{D+1})$$<br>\( \hat{y}=\max(0,W_{D+2} \ast H_{D+1}+b_{D+2}) \)，\( f_3(H)=\hat{y} \)</p><p>这个模型的优点就是简单强大，缺点就是递归层数多的时候很难训练，会产生梯度爆炸。</p><h4 id="Advanced-Model"><a href="#Advanced-Model" class="headerlink" title="Advanced Model"></a>Advanced Model</h4><p>进阶网络结构如下：</p><blockquote><p><img src="http://sotavision.cn/showdoc/server/../Public/Uploads/2020-04-05/5e89ed8fe90af.png" alt="" loading="lazy"></p></blockquote><p><strong>Recursive-Supervision</strong>：<br>为了解决梯度爆炸和寻找最佳递归次数的问题，提出了一种进一步的模型，所以递归上增加一个监督。监督可以表示为: </p><div>\[ \hat{ y}_d = f_3(\text{x},g^{(d)}(f_1(\text x))) \]</div>，其中 <div>\[ f_3(\text x, H_d)=\text x+f_3(H_d) \]</div>，<p>然后最终输出可以表达为: \( \hat{y}= \displaystyle\sum_{d=1}^D {w_d \cdot \hat{\text y}_d} \)</p><p><strong>skip-connection</strong>:使用该技巧的原因是超分辨率重建，低分辨率图像和网络输出的高分辨率图像有很高的相关性，而随着递归网络深度加深学习不到这种线性映射，所以通过该技巧可以使用大部分低分辨率信息用于重建高分辨率图像。</p><p>其中的\( H_1 \)到是\( H_D \)个共享参数的卷积层。DRCN将每一层的卷积结果都通过同一个Reconstruction Net得到一个重建结果，从而共得到D个重建结果，再把它们加权平均得到最终的输出。另外，受到ResNet的启发，DRCN通过skip connection将输入图像与\( H_d \)的输出相加后再作为Reconstruction Net的输入，相当于使Inference Net去学习高分辨率图像与低分辨率图像的差，即恢复图像的高频部分。</p><h4 id="Loss-function"><a href="#Loss-function" class="headerlink" title="Loss function"></a>Loss function</h4><p>中间层的loss function为：</p><div>    \[ l_1(\theta)= \displaystyle\sum_{d=1}^D \displaystyle\sum_{i=1}^N{ \dfrac{1}{2DN}|| \text y^{(i)}-\hat{\text y}_d^{(i)}||^2} \]</div>最后一层的loss function为：<div>    \[ l_2(\theta)= \displaystyle\sum_{i=1}^N{ \dfrac{1}{2N}|| \text y^{(i)}- \displaystyle\sum_{d=1}^D w_d \cdot \hat{\text y}_d^{(i)}||^2} \]</div><p>最终的loss function为：\( L(\theta)=\alpha l_1(\theta)+(1-\alpha)l_2(\theta)+\beta||\theta||^2 \)其中\( \alpha \)和\( \beta \)为可以调参数。</p><hr><h3 id="Experiment"><a href="#Experiment" class="headerlink" title="Experiment"></a>Experiment</h3><h4 id="Experiments-Details"><a href="#Experiments-Details" class="headerlink" title="Experiments Details"></a>Experiments Details</h4><p>实现细节待复现</p><h4 id="Study-of-Deep-Recursions"><a href="#Study-of-Deep-Recursions" class="headerlink" title="Study of Deep Recursions"></a>Study of Deep Recursions</h4><p>作者发现recursion d越大，psnr越高</p><blockquote><p><img src="http://sotavision.cn/showdoc/server/../Public/Uploads/2020-04-05/5e89f8ca3719c.png" alt="" loading="lazy"></p></blockquote><p>并且发现如果不是recursion的unit全部连到final output，而是单一的某一层，效果会变差，证明了recursion ensemble的效果</p><blockquote><p><img src="http://sotavision.cn/showdoc/server/../Public/Uploads/2020-04-05/5e89f945719f7.png" alt="" loading="lazy"></p></blockquote><h4 id="Comparisons-with-state-of-the-arts"><a href="#Comparisons-with-state-of-the-arts" class="headerlink" title="Comparisons with state-of-the-arts"></a>Comparisons with state-of-the-arts</h4><blockquote><p><img src="http://sotavision.cn/showdoc/server/../Public/Uploads/2020-04-05/5e89f97d92e5b.png" alt="" loading="lazy"></p></blockquote><hr><h3 id="Conclusions"><a href="#Conclusions" class="headerlink" title="Conclusions"></a>Conclusions</h3><ol><li>作者提出了一种使用深度递归卷积网络的超分辨率方法，有效地共享权重参数</li><li>使用递归监督和跳跃连接解决梯度爆炸和最佳递归层数问题</li><li>作者的方法在当时超分问题取得了较好的效果</li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;https://cdn.jsdelivr.net/npm/aplayer@1.10/dist/APlayer.min.css&quot;&gt;&lt;script 
      
    
    </summary>
    
    
      <category term="论文笔记" scheme="http://alexzou14.github.io/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="深度学习" scheme="http://alexzou14.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="笔记" scheme="http://alexzou14.github.io/tags/%E7%AC%94%E8%AE%B0/"/>
    
      <category term="超分辨率" scheme="http://alexzou14.github.io/tags/%E8%B6%85%E5%88%86%E8%BE%A8%E7%8E%87/"/>
    
  </entry>
  
  <entry>
    <title>FSRNet：End-to-End Learning Face Super-Resolution with Facial Priors论文阅读笔记</title>
    <link href="http://alexzou14.github.io/2020/04/02/FSRNet/"/>
    <id>http://alexzou14.github.io/2020/04/02/FSRNet/</id>
    <published>2020-04-02T09:03:11.000Z</published>
    <updated>2020-04-02T09:34:32.089Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="https://cdn.jsdelivr.net/npm/aplayer@1.10/dist/APlayer.min.css"><script src="https://cdn.jsdelivr.net/npm/aplayer@1.10/dist/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="https://cdn.jsdelivr.net/npm/meting@1.2/dist/Meting.min.js"></script><h3 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h3><p>人脸超分辨率是图像超分辨率的一个特殊任务，特别的人脸先验信息可以更好的被利用，还原出更好的超清图像。作者发现，现有的方法在恢复非常低的超分辨率人脸图像存在问题，恢复不清晰的情况。因此提出了一个端到端的训练方式和充分利用人脸的几何先验知识来组成的一个网络。该网络先构建了一个粗SR网络来恢复出一个粗高分辨率图片，再将粗HR图像送去两个分支网络中，一个是细SR编码，另一个是先验信息评估，细SR编码用来提取特征图，先验信息评估用来评价估计解析图。再将得到的特征图和先验信息送入细SR编码还原HR图像。为了生成更真实的图像，作者还提出了一个FSRGAN将对抗损失引入FSRNet中。</p><hr><h3 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h3><p>面部超分辨率（SR）是特定的一类图像超分辨率问题。目前大多数人脸图像超分辨算法是由通用的图像超分辨算法加以适当修改得到的。之前的面部超分辨率方法大多是由通用的图像超分辨率算法加以适当修改得到的，大多数没有加人脸先验知识，所以导致恢复效果不好。并且用于人脸超分辨率的图像不是端到端的训练。这篇文章受到人脸几何先验和端到端的训练的启发，提出了一个端到端的深度可训练面部超分辨网络，充分利用人脸图像的几何先验信息，即面部landmark的heatmap和人脸解析图，来对低分辨率人脸图像进行超分辨率。<br>本文主要贡献有：</p><ul><li>第一个提出用人脸几何先验的知识进行端到端学习的人脸超分辨率方法。</li><li>同时引入了两种几何先验，face landmark 和面部解析</li><li>提出的FSRnet在模糊未对齐和非常低的分辨率的图像，通过8倍放大，是目前最好的水平。同时用FSRnetGAN网络可以进一步生成更加逼真的images。</li><li>对于人脸超分辨率，人脸对齐和面部解析 最为新的评价标准。进一步证明，该方法可以解决传统的视觉感知度量方法的不一致性。</li></ul><p>选择形状作为先验的两种考虑：首先，当分辨率从高到低时，形状比纹理保存得更好，因此更有可能被提取出来以提高超分辨率。形状的表示要比纹理的表示好一些 。人脸解析 是不同人脸组成的分割估计。</p><blockquote><p><img src="http://sotavision.cn/showdoc/server/../Public/Uploads/2020-03-26/5e7c114a3c0a8.png" alt="FSRNet1" loading="lazy"></p></blockquote><hr><h3 id="Related-work"><a href="#Related-work" class="headerlink" title="Related work"></a>Related work</h3><h4 id="Facial-Prior-Knowledge"><a href="#Facial-Prior-Knowledge" class="headerlink" title="Facial Prior Knowledge"></a>Facial Prior Knowledge</h4><p>有很多使用了人脸先验信息的人脸超分方法，都是为了更好的从低分辨率图像恢复成高分辨率图像。但是早期的方法在低放缩倍率下估计人脸先验是非常困难的。随着近期深度卷积网络在人脸超分上的引用成功， 宋等人提出了一种两阶段的方法，首先生成面部COMP由CNNs合成，然后通过成分增强方法合成细粒度的面部结构。与上述方法不同的是，我们的FSRNet完全LEV 擦除面部地标热图和解析地图的端到端培训方式。</p><h4 id="End-to-end-Training"><a href="#End-to-end-Training" class="headerlink" title="End-to-end Training"></a>End-to-end Training</h4><p>端到端的训练模式被广泛运用在一般的图像超分问题中，DRRN的提出有效的解决了网络参数数量和网络准确率的问题。与上述仅依靠深层模型力量的方法不同，我们的FSRNet不仅是一个端到端的可训练神经网络，而且结合了来自人脸先验的丰富信息，更好的将低分辨率人脸图像还原成高分辨率图像。</p><hr><h3 id="Proposed-Method"><a href="#Proposed-Method" class="headerlink" title="Proposed Method"></a>Proposed Method</h3><h4 id="Network-structure"><a href="#Network-structure" class="headerlink" title="Network structure"></a>Network structure</h4><p>网络结构如下图：</p><blockquote><p><img src="http://sotavision.cn/showdoc/server/../Public/Uploads/2020-03-26/5e7c16555126e.png" alt="FSRNet2" loading="lazy"></p></blockquote><p>FSRNet是由四个部分组成：coarse SR network, fine SR encoder, prior estimation network和fine SR decoder</p><p>粗SR网络将输入的LR图像初步还原成一个粗SR图像,这个粗SR网络可以表达成：$$y_c=C(\text{x})$$<br><strong>先验信息估计网络</strong>：从最近成功的叠加热图回归在人体姿势估计中受到启发，文章提出在先验信息估计网络中使用一个 HourGlass 结构来估计面部landmark 的 heatmap 和解析图。因为这两个先验信息都可以表示 2D的人脸形状，所以在先验信息估计网络中，特征在两个任务之间是共享的， 除了最后一层。为了有效整合各种尺度的特征并保留不同尺度的空间信息，HourGlass block 在对称层之间使用 skip-connection 机制。最后，共享的 HG 特征连接到两个分离的 1×1 卷积层来生成 landmark heatmap和解析图。<br><strong>精细的SR编码器</strong>：受到 ResNet 在超分辨任务中的成功的启发，文章使用 residual block 进行特征提取。考虑到计算的开销，先验信息的特征会降采样到 64×64。为了使得特征尺寸一致，编码器首先经过一个 3×3，stride为 2 的卷积层来把特征图降采样到 64×64。然后再使用 ResNet 结构提取图像特征。</p><p>对应的先验评估网络P和细SR编码网络F分别可以表示成：<br>$$\text{p}=P(\text{y}_c)$$,$$\text{f}=F(\text{y}_c)$$<br><strong>精细的SR解码器</strong>：解码器把先验信息和图像特征组合为输入，首先将先验特征 p 和图像特征 f 进行concatenate，作为输入。然后通过 3×3 的卷积层把特征图的通道数减少为 64。然后一个 4×4 的反卷积层被用来把特征图的 size 上采样到 128×128。然后使用 3 个 residual block 来对特征进行解码。最后的 3×3 卷积层被用来得到最终的 HR 图像。</p><p>将coarse SRimages送入特征提取和先验估计网络中:<br>$$\text{y}=D(\text{f},\text{p})$$</p><h4 id="Loss-function"><a href="#Loss-function" class="headerlink" title="Loss function"></a>Loss function</h4><p>给定训练集：\( \{\text{x}^{(i)},\tilde{\text{y}}^{(i)},\tilde{\text{p}}^{(i)}\}_{i=1}^N \),FSRNet的损失函数为( \( \tilde{\text{y}},\tilde{\text{p}} \)为ground truth):</p><p>$$L_F(\Theta)=\dfrac{1}{2N}\sum_{i=1}^N { {\left|{\tilde{\text{y}}^{(i)}-\text{y}_c^{(i)}}\right|^2+\left|{\tilde{\text{y}}^{(i)}-\text{y}^{(i)}}\right|^2+\lambda\left|{\tilde{\text{p}}^{(i)}-\text{p}^{(i)}}\right|^2} } $$</p><h4 id="FSRGAN"><a href="#FSRGAN" class="headerlink" title="FSRGAN"></a>FSRGAN</h4><p>FSRGAN网络结构如下图：</p><blockquote><p><img src="http://sotavision.cn/showdoc/server/../Public/Uploads/2020-03-26/5e7c1ea4770b2.png" alt="FSRNet3" loading="lazy"></p></blockquote><p>GAN LOSS为：$$L_C(\text{F,C})=\mathbb{E}[\log\text{C}(\tilde{\text{y}},\text{x})]+\mathbb{E}[\log(1-\text{C}({\text{F(x)}},\text{x}))]$$<br>感知损失为：$$L_P=\left|{\phi(\text{y})-\phi(\tilde\text{y})}\right|^2$$</p><hr><h3 id="Experiment"><a href="#Experiment" class="headerlink" title="Experiment"></a>Experiment</h3><h4 id="Implementation-Details"><a href="#Implementation-Details" class="headerlink" title="Implementation Details"></a>Implementation Details</h4><p>实验详情可以参考代码：<a href="https://github.com/cydiachen/FSRNET_pytorch" target="_blank" rel="noopener">https://github.com/cydiachen/FSRNET_pytorch</a></p><h4 id="Prior-Knowledge-for-Face-Super-Resolution"><a href="#Prior-Knowledge-for-Face-Super-Resolution" class="headerlink" title="Prior Knowledge for Face Super-Resolution"></a>Prior Knowledge for Face Super-Resolution</h4><blockquote><p><img src="http://sotavision.cn/showdoc/server/../Public/Uploads/2020-03-26/5e7c20908f199.png" alt="FSRNet4" loading="lazy"><br><img src="http://sotavision.cn/showdoc/server/../Public/Uploads/2020-03-26/5e7c20a18f698.png" alt="FSRNet5" loading="lazy"></p></blockquote><p>把先验信息估计网络移除以后，构建了一个 Baseline 网络。基于 Baseline 网络，引入 ground truth 人脸先验信息（landmark heatmap 和解析图）到拼接层，得到一个新的网络。<br>结论：</p><ul><li>解析图比 landmark heatmap 含有更多人脸图像超分辨的信息，带来的提升更大；</li><li>全局的解析图比局部的解析图更有用；</li><li>landmark 数量增加所带来的提升很小</li></ul><h4 id="Network-analysis"><a href="#Network-analysis" class="headerlink" title="Network analysis"></a>Network analysis</h4><h5 id="Effects-of-Estimated-Priors"><a href="#Effects-of-Estimated-Priors" class="headerlink" title="Effects of Estimated Priors"></a>Effects of Estimated Priors</h5><blockquote><p><img src="http://sotavision.cn/showdoc/server/../Public/Uploads/2020-03-26/5e7c2210976f2.png" alt="FSRNet6" loading="lazy"></p></blockquote><p>Baseline_v1：完全不包含先验信息<br>Baseline_v2：包含先验信息，但不进行监督训练<br>结论：</p><ol><li>即使不进行监督训练，先验信息也能帮助到SR任务，可能是因为先验信息提供了更多的高频信息。</li><li>越多先验信息，越好。</li><li>最佳性能为25.85dB，但是使用ground truth信息时，能达到26.55dB。说明估计得到的先验信息并不完美，更好的先验信息估计网络可能会得到更好的结果。</li></ol><h5 id="Effects-of-Hourglass-Numbers"><a href="#Effects-of-Hourglass-Numbers" class="headerlink" title="Effects of Hourglass Numbers"></a>Effects of Hourglass Numbers</h5><p>强大的先验信息预测网络会得到更好的结果，所以探究Hourglass数量h对网络性能的影响。分别取1，2，4，结果为25.69，25.87，25.95。<br>不同的Hourglass数量对landmark估计的影响：</p><blockquote><p><img src="http://sotavision.cn/showdoc/server/../Public/Uploads/2020-03-26/5e7c2276370ed.png" alt="FSRNet7" loading="lazy"></p></blockquote><p>可以看到 h 数量增加时，先验信息估计网络结构越深，学习能力越强，性能越好。</p><h4 id="Comparisons-with-state-of-the-arts"><a href="#Comparisons-with-state-of-the-arts" class="headerlink" title="Comparisons with state-of-the-arts"></a>Comparisons with state-of-the-arts</h4><blockquote><p><img src="http://sotavision.cn/showdoc/server/../Public/Uploads/2020-03-26/5e7c22b5ad9fa.png" alt="FSRNet8" loading="lazy"></p></blockquote><p>放大8倍后的性能比较，虽然FSRGAN的两项指标（PSNR/SSIM）都不如FSRNet，但是从视觉效果上看更加真实。这也与目前的一个共识相对应：基于生成对抗网络的模型可以恢复视觉上合理的图像，但是在一些指标上（PSNR , SSIM）的值会低。而基于MSE的深度模型会生成平滑的图像，但是有高的PSNR/SSIIM。</p><hr><h3 id="Conclusions"><a href="#Conclusions" class="headerlink" title="Conclusions"></a>Conclusions</h3><ol><li>本文提出了深度端到端的可训练的人脸超分辨网络FSRNet</li><li>FSRNet的关键在于先验信息估计网络，这个网络不仅有助于改善PSNR/SSIM，还提供从非常低分辨率的图像精确估计几何先验信息（landmark heatmap和解析图）的解决方案。</li><li>实验结果表明FSRNet比当前的SOTA的方法要更好，即使在未对齐的人脸图像上。</li></ol><p>未来的工作可以有以下几个方面：</p><ol><li>设计一个更好的先验信息估计网络。</li><li>迭代地学习精细的SR网络。</li><li>调研其他有用的脸部先验信息。 </li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;https://cdn.jsdelivr.net/npm/aplayer@1.10/dist/APlayer.min.css&quot;&gt;&lt;script 
      
    
    </summary>
    
    
      <category term="论文笔记" scheme="http://alexzou14.github.io/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="深度学习" scheme="http://alexzou14.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="笔记" scheme="http://alexzou14.github.io/tags/%E7%AC%94%E8%AE%B0/"/>
    
      <category term="超分辨率" scheme="http://alexzou14.github.io/tags/%E8%B6%85%E5%88%86%E8%BE%A8%E7%8E%87/"/>
    
  </entry>
  
</feed>
