<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>秩同道合的小站</title>
  
  <subtitle>寻找志趣相投的伙伴！</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://alexzou14.github.io/"/>
  <updated>2020-04-08T03:34:21.410Z</updated>
  <id>http://alexzou14.github.io/</id>
  
  <author>
    <name>秩同道合</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Feedback Network for Image Super-Resolution论文阅读笔记</title>
    <link href="http://alexzou14.github.io/2020/04/08/SRFBN/"/>
    <id>http://alexzou14.github.io/2020/04/08/SRFBN/</id>
    <published>2020-04-08T03:19:33.000Z</published>
    <updated>2020-04-08T03:34:21.410Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="https://cdn.jsdelivr.net/npm/aplayer@1.10/dist/APlayer.min.css"><script src="https://cdn.jsdelivr.net/npm/aplayer@1.10/dist/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="https://cdn.jsdelivr.net/npm/meting@1.2/dist/Meting.min.js"></script><h3 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h3><p>由于现有的基于深度学习的图像SR方法尚未充分利用人类视觉系统中常见的反馈机制，所以本文基于这一想法提出了一个图像超分辨率反馈网络(SRFBN)通过高层信息来细化低层信息。具体这种反馈机制，是用具有约束的RNN中的隐状态来实现这种反馈机制。这种反馈机制能够产生非常强的高层表征能力。。此外，作者还引入了curriculum learning 策略，使网络非常适合于更复杂的任务。</p><hr><h3 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h3><p>随着网络的深度增加，参数的数量也会增加。大容量网络将占用大量存储资源并遭受过度拟合问题。为了减少网络参数，通常采用循环结构。因为具有重复结构的这些网络可以以前馈方式共享信息。但是，前馈方式使得先前的层不可能从以下层访问有用信息，即使采用跳过连接也是如此。<br>在本文中，作者提出了一种新的图像SR网络，即超分辨率反馈网络（SRFBN），以便通过反馈连接使用高级信息来改进低级信息。 所提出的SRFBN本质上是具有反馈块（FB）的RNN，其专门用于图像SR任务。<br>总之，我们的主要贡献如下：</p><ul><li>提出采用反馈机制的图像超分辨率反馈网络（SRFBN）。 通过反馈连接在自上而下的反馈流中提供高级信息。 同时，这种具有反馈连接的循环结构提供了强大的早期重建能力，并且仅需要很少的参数。</li><li>提出反馈块（FB），它不仅可以有效地处理反馈信息流，还可以通过上采样层和下采样层以及密集跳过连接来丰富高级表示。</li><li>为SRFBN提出curriculum -based训练策略，其中将具有增加的重建难度的HR图像作为连续迭代的目标馈入网络。 该策略使网络能够逐步学习复杂的退化模型，而对于那些只有一步预测的方法，同样的策略是不可能的。</li></ul><hr><h3 id="Related-work"><a href="#Related-work" class="headerlink" title="Related work"></a>Related work</h3><h4 id="Deep-learning-based-image-super-resolution"><a href="#Deep-learning-based-image-super-resolution" class="headerlink" title="Deep learning based image super-resolution"></a>Deep learning based image super-resolution</h4><p>这一部分作者主要回顾了深度学习在超分变率领域的应用和方法回顾。从2015年董超提出SRCN开始将深度学习引入超分辨领域，近几年的超分领域的不断发展，提出了很多方法，如：VDSR，EDSR等</p><h4 id="Feedback-mechanism"><a href="#Feedback-mechanism" class="headerlink" title="Feedback mechanism"></a>Feedback mechanism</h4><p>反馈机制能够允许该网络携带当前的输出去纠正之前的一些状态。<br>针对图片超分辨率中的feedback mechanism，有两个必要条件：<br>1）迭代 ；见下图(b)<br>2）重新路由（rerouting）系统的输出，以纠正（correct）每个循环中的输入， 在本文提出的网络结构中，实施feedback mechanism，有三个必不可少的部分:<br>1）在每一次迭代过程中都绑定loss；<br>2）使用循环结构（实现迭代的过程）；<br>3）在每一次迭代过程中提供低分辨率图片的输入。<br>上面三个部分缺一不可。</p><blockquote><p><img src="http://sotavision.cn/showdoc/server/../Public/Uploads/2020-03-11/5e68efd37e4e0.png" alt="" loading="lazy"></p></blockquote><h4 id="Curriculum-learning"><a href="#Curriculum-learning" class="headerlink" title="Curriculum learning"></a>Curriculum learning</h4><p>Curriculum learning逐渐增加了学习目标的难度，众所周知，这是改进训练程序的有效策略。早期的课程学习工作主要集中在一项任务上。 Pentina等以连续的方式将课程学习扩展到多个任务。虽然之前的工作主要集中在单个degradation过程，但我们对案例强制执行curriculum ，其中LR图像被多种类型的劣化所破坏。包含易于做出决策的curriculum可以针对一个问题进行解决，以逐步恢复损坏的LR图像。</p><hr><h3 id="Proposed-Method"><a href="#Proposed-Method" class="headerlink" title="Proposed Method"></a>Proposed Method</h3><blockquote><p><img src="http://sotavision.cn/showdoc/server/../Public/Uploads/2020-03-11/5e68f4db2d8fc.png" alt="" loading="lazy"></p></blockquote><p>见图我们提出的SRFBN可以被展开为T个迭代，迭代的顺序是从1到t，为了使得中间的状态能够携带一些输出信息，我们在每一次的迭代后面都会绑定一个loss值。<br>网络结构被分为三个模块：LR特征提取模块，反馈模块，重建模块</p><h4 id="Network-structure"><a href="#Network-structure" class="headerlink" title="Network structure"></a>Network structure</h4><ul><li><p>LR特征提取模块:这个LR特征提取模块由一个conv(3, 4m)和一个conv(3, m)组成，浅层特征提取可以由下列表达式表达：\(F_{in}^t = f_{LRFB}(I_{LR})\)</p></li><li><p>反馈模块:第t个权重共享模块的输出可以表示为：\(F_{out}^t=f_{FB}(F_{out}^{t-1},F_{in}^t)\)</p></li><li><p>第t个权重共享模块的中间监督输出：\(I_{SR}^t=I_{Res}^t+f_{UP}(I_{LR})\)其中\(I_{Res}^t=f_{RB}(F_{out}^t)\)</p></li></ul><h4 id="Feedback-block"><a href="#Feedback-block" class="headerlink" title="Feedback block"></a>Feedback block</h4><p><img src="http://sotavision.cn/showdoc/server/../Public/Uploads/2020-03-11/5e68feb50db97.png" alt="" loading="lazy"><br>简而言之，反馈模块就是反复上采样再下采样操作，同时，对所有上采样后的特征用dense connection，也对下采样后的特征用dense connection，中间用1*1卷积来降低计算量。<br>输入特征：\(L_0^t = C_0([F_{out}^{t-1},F_{in}^t])\)<br>上采样后的特征：\(H_g^t = C_g^{\uparrow}([L_0^t,L_1^t,…,L_{g-1}^t])\)<br>下采样后的特征：\(L_g^t = C_g^{\downarrow}([H_1^t,H_2^t,…,H_{g-1}^t])\)<br>输出特征：\(f_{out}^t = C_{FF}([L_1^t,L_2^t,…,L_{G}^t])\)</p><h4 id="Loss-function"><a href="#Loss-function" class="headerlink" title="Loss function"></a>Loss function</h4><p>loss function 为：\(L({\Theta}) = \dfrac{1}{T} \displaystyle\sum_{t=1}^{T}W^t||I_{HR}^t - I_{SR}^t||\)。控制数值\(W^t\)实验中都为1。</p><h4 id="Curriculum-learning-strategy"><a href="#Curriculum-learning-strategy" class="headerlink" title="Curriculum learning strategy"></a>Curriculum learning strategy</h4><p> 简而言之就是，中间监督的真值会根据任务难度进行选择，比如单一的bicubic降采样退化，所有的真值都是一样的；而对于BD（bicubic+blur）退化，头两个中间监督输出用带高斯模糊的真值，之后的中间监督用不带高斯模糊的真值。</p><hr><h3 id="Experiment"><a href="#Experiment" class="headerlink" title="Experiment"></a>Experiment</h3><h4 id="Experiments-Details"><a href="#Experiments-Details" class="headerlink" title="Experiments Details"></a>Experiments Details</h4><p>实验细节可以参考官方代码：<a href="https://github.com/Paper99/SRFBN_CVPR19" target="_blank" rel="noopener">https://github.com/Paper99/SRFBN_CVPR19</a></p><h4 id="Network-analysis"><a href="#Network-analysis" class="headerlink" title="Network analysis"></a>Network analysis</h4><p>在这一小节中，我们探讨了迭代次数(表示为T)和反馈块中投影组的数目(表示为G)的影响。</p><blockquote><p><img src="http://sotavision.cn/showdoc/server/../Public/Uploads/2020-03-11/5e6906c0a2c1c.png" alt="" loading="lazy"></p></blockquote><p>选择更大的T或G都有助于取得更好的结果。值得注意的是，小T和G仍然优于VDSR</p><h4 id="Comparisons-with-state-of-the-arts"><a href="#Comparisons-with-state-of-the-arts" class="headerlink" title="Comparisons with state-of-the-arts"></a>Comparisons with state-of-the-arts</h4><blockquote><p><img src="http://sotavision.cn/showdoc/server/../Public/Uploads/2020-03-11/5e69077a67a19.png" alt="" loading="lazy"></p></blockquote><h4 id="Results-with-BI-degradation-model"><a href="#Results-with-BI-degradation-model" class="headerlink" title="Results with BI degradation model"></a>Results with BI degradation model</h4><blockquote><p><img src="http://sotavision.cn/showdoc/server/../Public/Uploads/2020-03-11/5e6909a18b73d.png" alt="" loading="lazy"><br>这里没用放RCAN和RDN的数据，实际上SRFBN没达到SOTA，现在最好的还是RCAN</p></blockquote><h4 id="Results-with-BD-and-DN-degradation-models"><a href="#Results-with-BD-and-DN-degradation-models" class="headerlink" title="Results with BD and DN degradation models"></a>Results with BD and DN degradation models</h4><blockquote><p><img src="http://sotavision.cn/showdoc/server/../Public/Uploads/2020-03-11/5e6909ac81cd8.png" alt="" loading="lazy"></p></blockquote><hr><h3 id="Conclusions"><a href="#Conclusions" class="headerlink" title="Conclusions"></a>Conclusions</h3><p>本文提出了一种新的图像SR网络-超分辨率反馈网络(SRFBN)，通过增强高层次的图像表示来忠实地重建SR图像。网络中的反馈块(FB)可以有效地处理反馈信息流和特征重用。此外，还提出了一种curriculum学习策略，使网络能够很好地适应复杂退化模型破坏低分辨率图像的复杂任务。综合实验结果表明，所提出的SRFBN能以极小的参数提供与现有方法相比的比较或更好的性能。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;https://cdn.jsdelivr.net/npm/aplayer@1.10/dist/APlayer.min.css&quot;&gt;&lt;script 
      
    
    </summary>
    
    
      <category term="论文笔记" scheme="http://alexzou14.github.io/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="深度学习" scheme="http://alexzou14.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="笔记" scheme="http://alexzou14.github.io/tags/%E7%AC%94%E8%AE%B0/"/>
    
      <category term="超分辨率" scheme="http://alexzou14.github.io/tags/%E8%B6%85%E5%88%86%E8%BE%A8%E7%8E%87/"/>
    
  </entry>
  
  <entry>
    <title>Guided Super-Resolution as Pixel-to-Pixel Transformation论文阅读笔记</title>
    <link href="http://alexzou14.github.io/2020/04/08/GuideSR/"/>
    <id>http://alexzou14.github.io/2020/04/08/GuideSR/</id>
    <published>2020-04-08T03:11:46.000Z</published>
    <updated>2020-04-08T03:17:26.306Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="https://cdn.jsdelivr.net/npm/aplayer@1.10/dist/APlayer.min.css"><script src="https://cdn.jsdelivr.net/npm/aplayer@1.10/dist/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="https://cdn.jsdelivr.net/npm/meting@1.2/dist/Meting.min.js"></script><h3 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h3><p>本文提出了引导超分的概念。输入是某种模态的低分辨率图像数据，如用ToF相机获取的深度图像。目标输出是输入图像的高分辨率版本。其中还有一个概念叫做Guided Image——引导图像，是输入图像在另一种模态下的高分辨率图像，如使用传统相机获取的深度图像。传统建模的思想是：将低分辨率图像进行上采样，从引导图像中获取高频信息补充到上采样后的输入图像。本文作者利用了机器学习的方法将这个问题转化成了引导图像到输入图像模态的pixel-to-pixel的映射，和风格迁移有着很大的相似之处。Pixel-to-pixel mapping是通过多层感知机进行的实现，通过最小化源图像和下采样目标图像之间的差异来学习其权重，即为我们所要求解的映射。本文提出的算法只需要对映射函数进行正则化，避免了对输出结果的直接处理，而且是无监督学习的算法。</p><hr><h3 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h3><p>很多计算机视觉任务都可以看作引导超分辨率的实例，例如，环境测绘，机器人视觉等等。引导超分辨率可以看成输入一张低分辨率图，再出入一张相似的高清图来还原目标图像得到高分辨率图。如下图所示：</p><blockquote><p><img src="http://sotavision.cn/showdoc/server/../Public/Uploads/2020-03-23/5e7815665ee78.png" alt="" loading="lazy"></p></blockquote><p>换一个角度看，引导超分辨又可以看作是引导滤波器的推广形式，引导滤波器将源图像映射到目标图像  通过计算每个像素处的相同大小，依赖于源和引导图像中的局部邻域，从而得到源图像的高清版本。作者提出了另一种关于超分辨率的解释，源图像和引导图像的作用被交换，即从一个图像的像素级映射 到另一个而不改变分辨率，并通过要求其下采样版本与源映像匹配来约束输出。<br>文中认为，这种导超分辨率的观点有两个非常实际的优点：</p><ul><li>从所需分辨率开始，只使用1×1个核，不同的输入位置不会混合，这避免了模糊。</li><li>通过对所有像素使用相同的映射函数，并在其参数之前放置收缩，这样就会得到不需要正则化输出图像的适定问题</li></ul><p>本文的贡献是</p><ul><li>一种新的引导超分辨率公式，即无监督学习从像素到像素的转换受低分辨率光源限制。</li><li>我们对两个任务进行了实验：深度图的超分辨率和树高图的超分辨率。它们表明我们的方法在高上采样因子上明显优于其他的超分辨率方法</li></ul><hr><h3 id="Related-work"><a href="#Related-work" class="headerlink" title="Related work"></a>Related work</h3><h4 id="Guided-filtering"><a href="#Guided-filtering" class="headerlink" title="Guided filtering"></a>Guided filtering</h4><p>关于引导滤波，一般的原理是通过应用一个滤波器来增强源图像，该滤波器的输出不仅取决于源图像的局部邻域，而且还取决于从引导图像中的相同邻域导出的权重。引导滤波已经被用于各种各样的图像处理应用中，从去噪或着色等低级任务一直到立体匹配等。</p><h4 id="Guided-super-resolution"><a href="#Guided-super-resolution" class="headerlink" title="Guided super-resolution"></a>Guided super-resolution</h4><p>对于超分辨深度以及诸如色调映射和图像着色等低层操作，已经探索了将引导滤波扩展到超分辨问题。这些方法中有基于上述局部滤波原理的局部方法和将上采样任务描述为全局能量最小化的全局方法。</p><h4 id="Learned-guided-super-resolution"><a href="#Learned-guided-super-resolution" class="headerlink" title="Learned guided super-resolution"></a>Learned guided super-resolution</h4><p>这些方法迄今为止都是非监督的。还有一系列的工作，从例子中学习如何向上采样源图像同时将高频细节从引导图像传输到目标输出上。<br>这中数据处理方法的优点有：从真实图像数据中学习如何最佳融合源图像和引导图像可能比手工启发式方法更好。<br>与所有监督学习的缺点一样：</p><ol><li>必须获取足够数量的训练数据</li><li>通过这样设计的算法可能在训练数据上过拟合，但是在真实场景下表现不好</li></ol><p>高分辨率“引导”图像通过标准语义分割网络生成“目标”分割映射，使用一个损失函数鼓励目标具有相同的目标。 标签分布作为低分辨率的源图。</p><hr><h3 id="Proposed-Method"><a href="#Proposed-Method" class="headerlink" title="Proposed Method"></a>Proposed Method</h3><blockquote><p><img src="http://sotavision.cn/showdoc/server/../Public/Uploads/2020-03-23/5e7825da3d75f.png" alt="" loading="lazy"></p></blockquote><h4 id="Notation-and-Preliminaries"><a href="#Notation-and-Preliminaries" class="headerlink" title="Notation and Preliminaries"></a>Notation and Preliminaries</h4><p>首先对符号做出如下的定义：Source Image记作S（S.size=M×M），Guided Image记作G（G.size=N×N×C），目标图像记作T（T.size=N×N）。N和M之间的数量关系是通过尺度为D的上采样算子决定的，即N=D·M。S中的第m个像素记作\(S_m\),\(S_m\)是由T中的D×D范围内的block即b(m)经过某种非线性均值方式获得到的，即<br>\[ s_m=\dfrac{1}{D^2} \sum_{n\in b(m)}t_n = {\left\langle t_n \right\rangle}_{b(m)} \]<br>目标是Given S and G，esitimate\(\hat{T}\)</p><h4 id="Proposed-Solution"><a href="#Proposed-Solution" class="headerlink" title="Proposed Solution"></a>Proposed Solution</h4><p>设Guided Image和Target Image之间的映射为参数记作\(\theta\)的函数\(f_{\theta}\), 二者满足如下的关系\(\hat{t}_ n =f_{\theta}(g_n)\)，s和t之间的距离度量采用1-Norm，目标函数建立如下:<br>\[ \hat{\theta}=\arg\min_{\theta}\sum_{m}\left|{s_m - \left\langle{f_{\theta}(g_n)}\right\rangle_{b(m)}}\right| \]<br>这里是一个很典型的参数估计问题：\(f_{\theta}\)的形式为多层卷积感知器，参数记作\(\theta\) 。为了避免ill-posed problem求解出过拟合的参数\(\theta\)，作者引入了对\(\theta\)的 2-Norm 正则项，如下所示。参数\(\lambda\)控制了正则化的强度。<br>\[ \hat{\theta}=\arg\min_{\theta}\sum_{m}\left|{s_m - \left\langle{f_{\theta}(g_n)}\right\rangle_{b(m)}}\right|+\lambda\left|{\theta}\right|^2 \]<br>最后为了避免由于色彩、角点等其它特征造成的映射歧义，\(f_{\theta}\)函数引入了坐标变量 \(x_{n}\)。坐标和图像数据分别进行训练，将最终的结果merge到一起来进行参数优化。形式如下式所示:<br>\[ \hat{\theta}=\arg\min_{\theta}\sum_{m}\left|{s_m - \left\langle{f_{\theta}(g_n,x_n)}\right\rangle_{b(m)}}\right|+\lambda\left|{\theta}\right|^2 \]<br>通过这种架构，还可以通过设置各个超参数λg(=0.001)，λx(=0.0001)，λhead(=0.0001)来不同地对每个分支进行处理。目标函数采用了梯度下降法进行求解。</p><blockquote><p><img src="http://sotavision.cn/showdoc/server/../Public/Uploads/2020-03-23/5e782eec854c0.png" alt="" loading="lazy"></p></blockquote><hr><h3 id="Experimental-Results"><a href="#Experimental-Results" class="headerlink" title="Experimental Results"></a>Experimental Results</h3><p>论文代码：<a href="https://github.com/riccardodelutio/PixTransform" target="_blank" rel="noopener">https://github.com/riccardodelutio/PixTransform</a></p><h4 id="Evaluation-Settings"><a href="#Evaluation-Settings" class="headerlink" title="Evaluation Settings"></a>Evaluation Settings</h4><p>在所有实验中，作者将目标分辨率设置为256×256像素。 在不同的上采样因子（即×4，×8，×16和×32）下评估算法，分别对应于64×64、32×32、16×16和8×8的源分辨率。 所对比的方法：BiCubic、Guided Filter、Fast Bilateral Solver、Static-Dynamic Solver、MSG-Net。对比指标：Percentage of Bad Pixels（PBP），均方误差，平均绝对误差。<br>\[PBP_{\delta} = \dfrac{1}{N^2}\sum_n[|\hat{t_n}-t_n|&gt;\delta]\]<br>这里没有使用SSIM和PSNR的原因，个人分析认为：Guided SR不同于重建，因此不存在质量评价的这一标准，只需要考量和Ground Truth之间的差异即可。</p><h4 id="Analysis"><a href="#Analysis" class="headerlink" title="Analysis"></a>Analysis</h4><blockquote><p><img src="http://sotavision.cn/showdoc/server/../Public/Uploads/2020-03-23/5e7830c4e71b9.png" alt="" loading="lazy"></p></blockquote><p>这里展示了不同的映射函数的结果</p><h4 id="Comparisons-with-state-of-the-arts"><a href="#Comparisons-with-state-of-the-arts" class="headerlink" title="Comparisons with state-of-the-arts"></a>Comparisons with state-of-the-arts</h4><p><img src="http://sotavision.cn/showdoc/server/../Public/Uploads/2020-03-23/5e78312f76337.png" alt="" loading="lazy"></p><p>由上面显示的结果可看出，这个方法在处理分割图像超分上是目前最优的</p><hr><p>###Conclusions</p><ol><li>提出了一种新的、无监督的引导超分辨率方法。关键思想是将问题看作是高分辨率引导图像向低分辨率源图像域的像素级变换。</li><li>即使在高上采样因子上该方法也能够恢复非常精细的结构和非常锋利的边缘</li><li>利用了机器学习的方法将这个问题转化成了引导图像到输入图像模态的pixel-to-pixel的映射</li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;https://cdn.jsdelivr.net/npm/aplayer@1.10/dist/APlayer.min.css&quot;&gt;&lt;script 
      
    
    </summary>
    
    
      <category term="论文笔记" scheme="http://alexzou14.github.io/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="深度学习" scheme="http://alexzou14.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="笔记" scheme="http://alexzou14.github.io/tags/%E7%AC%94%E8%AE%B0/"/>
    
      <category term="超分辨率" scheme="http://alexzou14.github.io/tags/%E8%B6%85%E5%88%86%E8%BE%A8%E7%8E%87/"/>
    
  </entry>
  
  <entry>
    <title>Deep Laplacian Pyramid Networks for Fast and Accurate Super-Resolution论文阅读笔记</title>
    <link href="http://alexzou14.github.io/2020/04/08/LapSRN/"/>
    <id>http://alexzou14.github.io/2020/04/08/LapSRN/</id>
    <published>2020-04-08T03:04:02.000Z</published>
    <updated>2020-04-08T03:10:11.532Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="https://cdn.jsdelivr.net/npm/aplayer@1.10/dist/APlayer.min.css"><script src="https://cdn.jsdelivr.net/npm/aplayer@1.10/dist/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="https://cdn.jsdelivr.net/npm/meting@1.2/dist/Meting.min.js"></script><h3 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h3><p>当时卷积神经网络经常被用到对单图像超分辨率的高质量重建中。在这篇论文中，作者提出了Laplacian金字塔超分辨率网络（LapSRN）来逐步重构高分辨率图像的子频带残差（sub-band residuals）。在每个金字塔的层，我们将粗分辨率特征图作为输入，预测高频残差（high-frequency residuals），并使用反卷积（transposed convolutions）来进行向上采样到finer level。LapSRN方法并没有将双三次差值（bicubic interpolation）作为预处理步骤，从而减小了计算的复杂性。作者使用了一个强大的Charbonnier损失函数对所提出的LapSRN进行了深入的监督，实现了高质量的重建。我们的网络通过渐进的重构，在一个前馈的过程（feed-forward）中产生了多尺度的预测，从而促进了资源感知(resorce-aware)的应用。对基准数据集进行了大量的定量和定性评估，结果表明，该算法在速度和精度方面优于最先进的方法。</p><hr><h3 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h3><p>当时CNN在单一图像超分问题上取得了巨大成功，并不断的发展。有很多方法被提出：</p><blockquote><p><img src="http://sotavision.cn/showdoc/server/../Public/Uploads/2020-04-08/5e8d2cb679ad6.png" alt="" loading="lazy"></p></blockquote><p>作者在这部分说明了当时CNN方法在SR问题上存在三个主要问题：</p><ol><li>当时的方法使用了预先定义的上采样操作，将输入图像提升到所需分辨率大小再输入进网络，这样增加了很多不必要的计算成本。</li><li>当时的方法在优化网络的时候，采用L2 loss function，这样会导致图像过于平滑</li><li>大多数方法都是一个上采样的步骤重建图像，这增加了训练大尺度因子的难度。</li></ol><p>本文中的LapSRN与其他方法的区别：</p><ol><li>直接用LR提取特征，并用Charbonnier的损失函数</li><li>速度上，作者的方法比一般的SRCNN要快，和FSRCNN相近但是质量要更高</li><li>渐进重构：在一条前馈通道中可以生成多个SR预测，更好的应用在不同的场景中。</li></ol><hr><h3 id="Related-work"><a href="#Related-work" class="headerlink" title="Related work"></a>Related work</h3><h4 id="SR-based-on-internal-databases"><a href="#SR-based-on-internal-databases" class="headerlink" title="SR based on internal databases"></a>SR based on internal databases</h4><p>一些方法利用自然图像中的自相似性属性，基于低分辨率输入图像的尺度空间金字塔构造LR-HR补丁对。然而内部数据库包含的相关训练补丁比外部数据库要多，LR-HR补丁对的数量可能不足以覆盖图像中较大的纹理变化。Singh等人将补丁分解成定向频率子带（directional frequency sub-bands ），并独立地在每个子带金字塔（sub-band pyramid）中确定更好的匹配。</p><h4 id="SR-based-on-external-databases"><a href="#SR-based-on-external-databases" class="headerlink" title="SR based on external databases"></a>SR based on external databases</h4><p>大量的SR方法通过从外部数据库中收集的图像对学习LR-HR映射，使用监督学习算法。不是直接在整个数据库上对复杂的补丁空间进行建模，而是通过k均值、稀疏字典或随机森林来划分图像数据库，并学习每个集群的局部线性回归。</p><h4 id="Convolutional-neural-networks-based-SR"><a href="#Convolutional-neural-networks-based-SR" class="headerlink" title="Convolutional neural networks based SR"></a>Convolutional neural networks based SR</h4><p>与在补丁空间中对LR-HR映射建模不同的是，SRCNN联合优化了所有的步骤，并在图像空间中学习了非线性映射。VDSR网络通过将网络深度从3个层增加到20个卷积层，显示了对SRCNN的显著改进。为了促使训练出一个更快收敛速度的更深的模型，VDSR训练网络来预测剩余的值，而不是实际的像素值。Wang等人将稀疏编码的领域知识与深度CNN结合起来，并训练一个级联网络（SCN），逐步将图像提升到所需的比例因子。Kim等人提出了一个具有深度递归层（DRCN）的浅层网络，以减少参数的数量。<br>LapSRN与上述方法不同：</p><ol><li>加入了残差和反卷积组成的上采样滤波器，有效的抑制了双三插值导致的重构伪影，降低计算复杂度。</li><li>利用Charbonnier损失函数提高重建精度</li><li>渐进重构的方法可以适用于不同的比例因子</li></ol><hr><h3 id="Proposed-Method"><a href="#Proposed-Method" class="headerlink" title="Proposed Method"></a>Proposed Method</h3><h4 id="Network-structure"><a href="#Network-structure" class="headerlink" title="Network structure"></a>Network structure</h4><p>网络结构如下图所示：</p><blockquote><p><img src="http://sotavision.cn/showdoc/server/../Public/Uploads/2020-04-08/5e8d3168dec26.png" alt="" loading="lazy"></p></blockquote><p><strong>Feature extraction branch</strong>：<br>通过stack convolution来获取非线性特征映射<br><strong>Image reconstruction branch</strong>：<br>在每一个pyramid level，最后加上deconv来提升图像的2x分辨率<br><strong>参数共享</strong><br>本文网络在两个地方进行参数共享，减少了参数量<br>1.在各个pyramid level之间参数共享， 称之为Recursive block</p><p>因为laplacian pyramid是在x2的基础上得到x4，由于各个level中的结构相似性，因此在各个level，参数得以共享<br>形式如下：</p><blockquote><p><img src="http://sotavision.cn/showdoc/server/../Public/Uploads/2020-04-08/5e8d319ef0847.png" alt="" loading="lazy"></p></blockquote><p>2.每个pyramid level之中参数共享</p><p>受DRCN和DRRN启发，作者在每个pyramid level中进行参数共享，如下图:</p><blockquote><p><img src="http://sotavision.cn/showdoc/server/../Public/Uploads/2020-04-08/5e8d326d42b45.png" alt="" loading="lazy"></p></blockquote><h4 id="Loss-function"><a href="#Loss-function" class="headerlink" title="Loss function"></a>Loss function</h4><p>本文认为相同的LR patch 可能有多种corresponding HR patches，而L2范数并不能capture the underlying multi-modal distributions of HR patches. 因此L2范数重建出的图像往往过平滑。<br>本文提出了一种抗噪性强的loss functions：</p><div>\[ L(\hat{y},y;\theta)=\dfrac{1}{N}\displaystyle\sum_{i=1}^N \sum_{s=1}^L \rho(\hat{y}_s^{(i)}-y_s^{(i)})=\dfrac{1}{N}\displaystyle\sum_{i=1}^N \sum_{s=1}^L \rho((\hat{y}_s^{(i)}-x_s^{(i)})-r_s^{(i)}) \]</div><p>其中\(\rho(x)=\sqrt{x^2+\varepsilon ^2}\)</p><hr><h3 id="Experiment"><a href="#Experiment" class="headerlink" title="Experiment"></a>Experiment</h3><h4 id="Experiments-Details"><a href="#Experiments-Details" class="headerlink" title="Experiments Details"></a>Experiments Details</h4><p>论文具体细节待复现。</p><h4 id="Model-analysis"><a href="#Model-analysis" class="headerlink" title="Model analysis"></a>Model analysis</h4><p><strong>Residual learning</strong>：</p><blockquote><p><img src="http://sotavision.cn/showdoc/server/../Public/Uploads/2020-04-08/5e8d3d9e29c77.png" alt="" loading="lazy"></p></blockquote><p>残差学习可以提高网络的收敛速度，是PSNR更稳定。<br><strong>Loss function</strong>：</p><blockquote><p><img src="http://sotavision.cn/showdoc/server/../Public/Uploads/2020-04-08/5e8d3dea66dc8.png" alt="" loading="lazy"></p></blockquote><p>L1 loss比L2 loss 产生的图像更加的清晰。<br><strong>Network depth</strong>：</p><blockquote><p><img src="http://sotavision.cn/showdoc/server/../Public/Uploads/2020-04-08/5e8d3e3107068.png" alt="" loading="lazy"></p></blockquote><p>当网络深度d=10的时候速度最快，效果最好。</p><h4 id="Comparisons-with-state-of-the-arts"><a href="#Comparisons-with-state-of-the-arts" class="headerlink" title="Comparisons with state-of-the-arts"></a>Comparisons with state-of-the-arts</h4><p>各个超分辨率算法实验结果：</p><blockquote><p><img src="http://sotavision.cn/showdoc/server/../Public/Uploads/2020-04-08/5e8d3e8ab7b3c.png" alt="" loading="lazy"><br><img src="http://sotavision.cn/showdoc/server/../Public/Uploads/2020-04-08/5e8d3ec48f9bc.png" alt="" loading="lazy"></p></blockquote><hr><h3 id="Conclusions"><a href="#Conclusions" class="headerlink" title="Conclusions"></a>Conclusions</h3><ol><li>本文是通过将低分辨率图像直接作为输入到网络中，通过逐级放大，在减少计算量的同时，也有效的提高了精度</li><li>提出了一种鲁棒的loss function, robust Charbonnier loss function.</li><li>对各个金字塔的level之间和每个level之内，通过recursive进行参数共享</li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;https://cdn.jsdelivr.net/npm/aplayer@1.10/dist/APlayer.min.css&quot;&gt;&lt;script 
      
    
    </summary>
    
    
      <category term="论文笔记" scheme="http://alexzou14.github.io/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="深度学习" scheme="http://alexzou14.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="笔记" scheme="http://alexzou14.github.io/tags/%E7%AC%94%E8%AE%B0/"/>
    
      <category term="超分辨率" scheme="http://alexzou14.github.io/tags/%E8%B6%85%E5%88%86%E8%BE%A8%E7%8E%87/"/>
    
  </entry>
  
  <entry>
    <title>SROBB:Targeted Perceptual Loss for Single Image Super-Resolution论文阅读笔记</title>
    <link href="http://alexzou14.github.io/2020/04/07/SROBB/"/>
    <id>http://alexzou14.github.io/2020/04/07/SROBB/</id>
    <published>2020-04-07T11:34:04.000Z</published>
    <updated>2020-04-07T12:01:38.635Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="https://cdn.jsdelivr.net/npm/aplayer@1.10/dist/APlayer.min.css"><script src="https://cdn.jsdelivr.net/npm/aplayer@1.10/dist/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="https://cdn.jsdelivr.net/npm/meting@1.2/dist/Meting.min.js"></script><h3 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h3><p>由于传统学习方法能力有限，没有考虑任何的语义信息产生了较大的误差，所以本文提出了一种更客观的地从知觉损失中获益的新方法。优化了一个基于深度网络的解码器，该解码器具有目标函数，可以使用相应的方式在不同的语义级别下处理图像。该方法利用我们提出的OBB(对象、背景和边界)标签，由分割标签生成，在考虑背景纹理相似性的同时，估计一个合适的边界感知损失。我们展示了我们提出的方法可以得到更真实的纹理和更清晰的边缘，并且在标准基准测试的定性结果和广泛的用户研究结果方面都优于其他最先进的算法。</p><hr><h3 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h3><p>最近，SISR的另一个突破是将感知损失函数用于训练前馈网络，而不是使用每像素损失函数，如均方误差(MSE)。它解决了MSE优化导致的纹理模糊问题，同时也带来了对抗性的损失，在感知图像质量方面实现了近真实感重建。虽然感知损失在SISR中取得了很大的成功，但将其应用于整体图像，不考虑语义信息，限制了其网络能力。在感知功能方面，最先进的方法使用不同层次的特征来恢复原始图像;这个选择决定了他们是关注局部信息(如边缘)、中层特性(如纹理)还是与语义信息对应的高层特性。在这些方法中，以同样的方法计算了整个图像的感知损失，这意味着在边缘、前景或图像背景上使用了相同级别的特征。</p><blockquote><p><img src="http://sotavision.cn/showdoc/server/../Public/Uploads/2020-03-24/5e796c1108431.png" alt="" loading="lazy"><br>图1所示。针对高分辨率图像的特点，提出了一种在训练过程中利用分割标签对不同语义层次的图像进行分割的方法;我们优化了我们的SISR模型，通过最小化感知误差，分别对应的边缘只在物体边界和纹理的背景区域。结果从左至右:原始图像，超分辨率图像仅使用像素丢失函数，像素丢失+感知丢失函数和像素丢失+目标感知丢失函数(our)。</p></blockquote><p>为了解决上述问题，我们提出了一种新的方法，以更客观的方式从知觉损失中获益。图1显示了我们所提议的方法的概述。特别是，我们使用像素级的分割注释来构建我们所提议的OBB标签，从而能够找到目标感知特性，这些特性可以用来最小化不同图像区域的适当损失:例如，边缘的损失和训练过程中图像纹理的损失。我们展示了我们的方法使用目标知觉损失优于其他最先进的算法在定性结果和用户研究实验，并导致更现实的纹理和更锋利的边缘。</p><hr><h3 id="Related-work"><a href="#Related-work" class="headerlink" title="Related work"></a>Related work</h3><p>这节作者主要回顾了近期SR的各种方法如SRCN，DRCN等。尽管针对SISR任务提出了不同的体系结构，但是基于优化的方法的行为主要是由目标函数的选择驱动的。这些方法所使用的目标函数大多包含一个损失项，即超分辨率HR图像与真实HR图像之间的像素距离。然而，由于所有可能的解决方案的像素平均，仅使用这个函数就会导致图像模糊和超平滑。<br>感知驱动的方法在视觉质量方面显著提高了图像的超分辨率。基于感知相似度的思想，提出了一种利用预先训练的特征提取器的特定层(如VGG)来最小化特征空间中的感知损失。在相似的工作中，提出的内容损失来生成具有自然图像统计的图像，它关注的是特征分布而不是仅仅比较外观。SRGAN提出在感知损失的基础上，利用对抗性损失来支持自然图像流形上的输出。虽然这些方法产生了接近于光真实感的结果，但它们以同样的方式估计了整个图像的重建误差，没有利用任何可以提高视觉质量的语义信息。<br>在这项工作中，我们研究了一种利用图像内部语义信息的新方法，产生具有精细结构的逼真的超分辨率图像。</p><hr><h3 id="Proposed-Method"><a href="#Proposed-Method" class="headerlink" title="Proposed Method"></a>Proposed Method</h3><p>作者引入了一个损失函数，它包含三项:1-像素级损失(MSE)、2-对抗性损失和3-新目标感知损失函数。<br>MSE和对抗性损失术语的定义如下:</p><ul><li>像素级别损失是目前为止SR中最常用的损失函数，它在图像域中计算原始图像与超分辨率图像之间的像素方向均方误差(MSE)。使用它作为一个独立的目标函数的主要缺点是解决了一个覆盖的重建。接受MSE损失训练的网络试图找到合理解决方案的像素平均，这导致感知质量较差，边缘和纹理中缺乏高频细节。</li><li>在SRGAN的启发下，我们将SR模型建立在一个对抗性的环境中，给出了一个可行的解决方案。特别地，我们使用一个额外的网络(鉴别器)，它可以与我们的SR解码器竞争。生成器(SR解码器)试图生成伪图像来欺骗鉴别器，而鉴别器的目标是将生成的结果与真实的HR图像区分开来。这种设置的结果在感知上优于通过最小化像素方面的MSE和经典感知损失得到的解决方案。</li></ul><h4 id="Targeted-perceptual-loss"><a href="#Targeted-perceptual-loss" class="headerlink" title="Targeted perceptual loss"></a>Targeted perceptual loss</h4><p>作者提出了一种新的方法，以有针对性的方式利用感知相似性，重建更有吸引力的边缘和纹理。</p><blockquote><p><img src="http://sotavision.cn/showdoc/server/../Public/Uploads/2020-03-24/5e7979791a063.png" alt="" loading="lazy"><br>图2. 选择不同CNN层来估计图像不同区域感知损失的效果，如边缘和纹理:(a)使用更深的卷积层(中层特征)，VGG-16[29]的ReLU 4-1， (b)使用早期卷积层(低层特征)，VGG-16网络的ReLU 1-2。</p></blockquote><p>目标丢失函数尝试在区域周围选择更真实的纹理，其中纹理的类型似乎很重要，例如，树，同时尝试解决边界区域周围更锋利的边缘。为此，作者首先在图像中定义三种类型的区域:1-背景、2-边界和3-对象，然后使用不同的函数计算每个区域的目标知觉损失。<br><strong>背景</strong>( \(\mathcal G_b\) ):我们认为四类是背景：“天空”、“植物”、“地面”和“水”。 我们选择这些类别是因为它们的具体外观；这些标签区域的整体纹理更多。我们计算中层CNN特征来估计SR和HR图像之间的感知相似性。 在这里，我们使用VGG-16的ReLU4-3层来实现这一目的。<br><strong>边界</strong>( \(\mathcal G_e\) ):所有分隔对象和背景的边缘都被认为是边界。通过一些预处理，我们将这些边缘扩展为一条贯穿所有边界的条带。我们估计了一个早期CNN层在SR和HR图像之间的特征距离，这个特征距离更侧重于低层空间信息，主要是边缘和斑点。特别地，我们最小化了VGG-16的ReLU 2-2层的感知损失。<br><strong>目标</strong>( \(\mathcal G_o\) ):由于现实世界中物体的形状和纹理种类繁多，因此判断早期的特征对于感知缺失功能来说是更合适还是更适合使用深层的特征对于感知缺失功能来说是一个挑战;例如，在斑马的图像中，更清晰的边缘比整体纹理更重要。尽管如此，强迫网络估计树的精确边缘可能会误导优化过程。因此，我们不考虑任何类型的知觉损失对定义为对象的领域加权为零，并依赖于MSE和对抗性损失。</p><p>为了计算特定图像区域的感知损失，我们对语义类进行二值分割掩码(对感兴趣的类像素值为1，其他地方像素值为0)。每个掩码都分类地表示图像的不同区域，并分别按元素乘上HR图像和估计的超分辨率图像SR。我们可以得出，掩码HR与超分辨率图像在特征空间中的所有非零距离都对应于该图像可见区域的内容:边界( \(M_{OBB}^{boundaries}\) )用掩码对应边缘，背景( \(M_{OBB}^{background}\) )用掩码对应纹理。<br>整体目标感知损失函数为:<br>\(L_{perc.} = \alpha\cdot \mathcal G_{e}(I^{SR}\circ M_{OBB}^{boundary}, I^{HR}\circ M_{OBB}^{boundary})\)<br>\(+\beta\cdot \mathcal G_{b}(I^{SR}\circ M_{OBB}^{boundary}, I^{HR}\circ M_{OBB}^{boundary})+\gamma\cdot \mathcal G_{o}\)</p><h4 id="OBB-Object-background-and-boundary-label"><a href="#OBB-Object-background-and-boundary-label" class="headerlink" title="OBB: Object, background and boundary label"></a>OBB: Object, background and boundary label</h4><p>为了充分利用基于感知丢失的图像超分辨率，作者通过提出的目标丢失函数来增强语义细节(对象、背景和边界出现在图像上的地方)。此外，现有的分割任务注释，如coco-stuff只提供了关于对象和背景的空间信息，没有使用表示边缘区域的类，即本文中的边界。因此，作者提出了我们的标注方法(图3)，为图像的语义信息提供更好的空间控制。</p><blockquote><p><img src="http://sotavision.cn/showdoc/server/../Public/Uploads/2020-03-24/5e797f7c995a5.png" alt="" loading="lazy"><br>图3. 构造一个OBB标签。我们根据“对象”、“背景”或“边界”类的初始像素级标签为每个区域分配一个类。</p></blockquote><p>为了得到一个较厚的条带，将不同的类分开，作者通过计算了一个d1大小的圆盘的膨胀。将结果区域标记为“boundary”类。将分割标签中的“sky”、“plant”、“ground”和“water”类作为“Background”。所有剩余的对象类都被认为是“对象”类。</p><h4 id="Architecture"><a href="#Architecture" class="headerlink" title="Architecture"></a>Architecture</h4><blockquote><p><img src="http://sotavision.cn/showdoc/server/../Public/Uploads/2020-03-24/5e79803d2eab1.png" alt="" loading="lazy"><br>图4. SR解码器原理图。我们在训练SR解码器的同时，使用了目标感知损失以及MSE和对抗性损失。在该模式中，k、n和s分别对应内核大小、特征映射数和步长大小。</p></blockquote><p>为了与SRGAN方法进行公平的比较，并对所提出的目标感知丢失进行消融研究，我们使用与SRGAN相同的SR解码器。</p><hr><h3 id="Experiment"><a href="#Experiment" class="headerlink" title="Experiment"></a>Experiment</h3><h4 id="Experiments-Details"><a href="#Experiments-Details" class="headerlink" title="Experiments Details"></a>Experiments Details</h4><p>为了创建OBB标签，作者使用COCO-Stuff数据集中的一组随机的50K图像，其中包含用于分割任务的91个类的语义标签。<br>训练过程分两步进行;首先，对SR解码器进行了25个周期的预训练，仅以像素方向的均方误差为损失函数。在此基础上，增加了目标知觉损失函数和对抗性损失函数，训练时间延长了55个迭代。每一项的权重在新的有针对性的知觉丧失,α和β,设置为2×10−6和1.5×10−6,分别。与SRGAN相同，将对抗性损失函数和MSE损失函数的权重分别设置为1.0和1×10−3。我们将用于生成OBB标签的磁盘直径d1设置为2.0。在这两个步骤中都使用了Adam优化器。将学习率设置为1×10−3，然后每20个迭代衰减10倍。我们还交替优化了与SRGAN提出的参数相似的鉴别器。</p><h4 id="Qualitative-Results"><a href="#Qualitative-Results" class="headerlink" title="Qualitative Results"></a>Qualitative Results</h4><p>我们的方法主要是利用分割标签对具有边界和背景的感知损失项的解码器进行优化。虽然我们没有将知觉损失专门应用于物体区域，但是我们的实验表明，训练后的模型与其他方法相比，在某种程度上泛化了，它重建了更真实的物体。</p><blockquote><p><img src="http://sotavision.cn/showdoc/server/../Public/Uploads/2020-03-24/5e79828535900.png" alt="" loading="lazy"><br><img src="http://sotavision.cn/showdoc/server/../Public/Uploads/2020-03-24/5e798298319e8.png" alt="" loading="lazy"></p></blockquote><h4 id="Quantitative-Results"><a href="#Quantitative-Results" class="headerlink" title="Quantitative Results"></a>Quantitative Results</h4><blockquote><p><img src="http://sotavision.cn/showdoc/server/../Public/Uploads/2020-03-24/5e7982c291776.png" alt="" loading="lazy"><br>表1 Set5和Set14测试集的“婴儿”和“狒狒”图像的双三次插值、LapSRN、SRGAN和SROBB(我们的)的比较。最佳度量(SSIM, PSNR，lpip)用粗体突出显示。可视化比较如图5所示。</p></blockquote><p>学习感知图像Patch相似度(LPIPS)度量是最近引入的一种基于参考的图像质量评估度量，它的目的是估计两幅图像之间的感知相似度。</p><hr><h3 id="Conclusions"><a href="#Conclusions" class="headerlink" title="Conclusions"></a>Conclusions</h3><ol><li>针对基于CNN的单图像超分辨率，提出了一种新的目标感知丢失函数。</li><li>提出的目标函数用相关的损失项对图像的不同区域进行惩罚，即在训练过程中对边缘和纹理使用边缘损失和纹理损失。</li><li>引入了我们的OBB标签，从像素分割标签创建，以提供更好的空间控制的语义信息的图像。</li><li>提出的定向知觉损失训练在感知效果上更令人满意，并且优于目前最先进的SR方法。</li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;https://cdn.jsdelivr.net/npm/aplayer@1.10/dist/APlayer.min.css&quot;&gt;&lt;script 
      
    
    </summary>
    
    
      <category term="论文笔记" scheme="http://alexzou14.github.io/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="深度学习" scheme="http://alexzou14.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="笔记" scheme="http://alexzou14.github.io/tags/%E7%AC%94%E8%AE%B0/"/>
    
      <category term="超分辨率" scheme="http://alexzou14.github.io/tags/%E8%B6%85%E5%88%86%E8%BE%A8%E7%8E%87/"/>
    
  </entry>
  
  <entry>
    <title>Natural and Realistic Single Image Super-Resolution with Explicit Natural Manifold Discrimination论文阅读笔记</title>
    <link href="http://alexzou14.github.io/2020/04/07/NatSR/"/>
    <id>http://alexzou14.github.io/2020/04/07/NatSR/</id>
    <published>2020-04-07T10:52:26.000Z</published>
    <updated>2020-04-07T11:30:50.910Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="https://cdn.jsdelivr.net/npm/aplayer@1.10/dist/APlayer.min.css"><script src="https://cdn.jsdelivr.net/npm/aplayer@1.10/dist/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="https://cdn.jsdelivr.net/npm/meting@1.2/dist/Meting.min.js"></script><h3 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h3><p>许多使用CNN网络的SISR模型使用失真导向（distortion-oriented）的损失函数，这类模型很难恢复真实图像纹理内容和细节，看起来较模糊，没有较好的视觉效果。恢复真实纹理和细节在图像超分辨领域仍然是一项挑战，目前有一些关于这方面的工作，如SRGAN，EnhancedNet，SFT-GAN，但是，这些方法在生成这些不真实（fake）细节时，通常会产生不需要的伪影，整幅图像看起来总有一些不自然。文中，提出了一种重建真实超分辨率图像的方法，重构的图像具有非常高的视觉效果并保持图像的真实性。作者在低层图像域定义了真实先验（naturalness prior），并约束重构图像在自然流形（natural manifold）。</p><hr><h3 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h3><p>单一图像超分率（SISR）是一个传统的图像恢复问题，主要目的是将低分辨率图像恢复成高分辨率图像。由于SISR是一个不定问题，不同的SR方法会产生不一样的图像。目前做SR的方法无论是使用MSE还是使用GAN的方法，都会产生不自然的图像效果。使用MSE损失函数会使得恢复图像过于平滑，使用现有感知损失如SRGAN，EhanceNet,会产生一些不需要的细节、伪影等。如下图：</p><blockquote><p><img src="http://sotavision.cn/showdoc/server/../Public/Uploads/2020-03-24/5e79cd373c660.png" alt="" loading="lazy"></p></blockquote><p>所以本文借鉴了SR的领域先验知识，提出一个方法来约束低级领域先验来代替高级语言。设计一个判别器判定图像的自然程度，以此为监督对图像做SR，这是这篇论文的基本思路。<br>本文主要贡献有：</p><ol><li>设计了基于CNN网络的自然流形鉴别器（natural manifold discriminator）。</li><li>基于不规则残差学习的CNN结构，distortion-oriented，即fractal residual super-resolution (FRSR)。</li><li>我们提出了一种面向感知的SISR方法，（NatSR)网络，可生成真实纹理和自然细节，获得高视觉质量。</li></ol><hr><h3 id="Related-work"><a href="#Related-work" class="headerlink" title="Related work"></a>Related work</h3><h4 id="Single-Image-Super-Resolution"><a href="#Single-Image-Super-Resolution" class="headerlink" title="Single Image Super-Resolution"></a>Single Image Super-Resolution</h4><p>这里主要讲了早期SR方法，不管是传统方法还是深度学习方法都是一鉴别失真为导向的，目的是为了达到更高的PSNR。这些方法往往会导致生成的图像过于平滑，感知细节丢失等问题。</p><h4 id="Perception-Oriented-Super-Resolution"><a href="#Perception-Oriented-Super-Resolution" class="headerlink" title="Perception Oriented Super-Resolution"></a>Perception Oriented Super-Resolution</h4><p>由于上面SR方法为了达到更高的PSNR从而使得生成图像更平滑，所以近期面向感知的方法受到广泛关注，并且Johson提出像素域感知不是感知质量的最优解，特征空间损失才更符合人类感知模型。所以近期的SFT-GAN通过调整目标像素语义类别来限制特征空间达到更好的效果。</p><h4 id="Modeling-the-SISR"><a href="#Modeling-the-SISR" class="headerlink" title="Modeling the SISR"></a>Modeling the SISR</h4><p>论文认为LR和HR在频率域上的关系如下所示，如果对LR做SR的结果可能会出现图下面的情形:</p><blockquote><p><img src="http://sotavision.cn/showdoc/server/../Public/Uploads/2020-03-24/5e79d02537df0.png" alt="" loading="lazy"></p></blockquote><p>所以LR-HR的对应关系可以描述成：\( I_{LR}=h(I_{HR})^{\downarrow} \)<br>由于SISR是为给定的LR找到HR，所以它通常被建模为找到条件似然\( p(I_{HR}|I_{LR}) \)。</p><hr><h3 id="Proposed-Method"><a href="#Proposed-Method" class="headerlink" title="Proposed Method"></a>Proposed Method</h3><h4 id="Natural-Manifold-Discrimination"><a href="#Natural-Manifold-Discrimination" class="headerlink" title="Natural Manifold Discrimination"></a>Natural Manifold Discrimination</h4><h5 id="Designing-Natural-Manifold"><a href="#Designing-Natural-Manifold" class="headerlink" title="Designing Natural Manifold"></a>Designing Natural Manifold</h5><p>根据上图这里将图像HR space分成如下图的三类，如下图所示：</p><blockquote><p><img src="http://sotavision.cn/showdoc/server/../Public/Uploads/2020-03-24/5e79d1009156b.png" alt="" loading="lazy"></p></blockquote><p>其中模糊集A可以定义成：\( A={I_A|I_A=(1-\alpha)h(I_{LR}^{\uparrow})+\alpha I_{HR}} \)<br>模糊集A中的图像降置为LR可以描述为：</p><p>\( h(I_A)^{\downarrow} \)<br>\( =h((1-\alpha)h(I_{LR}^{\uparrow})+\alpha I_{HR})^{\downarrow} \)<br>\( =h((1-\alpha)h(I_{LR}^{\uparrow}))^{\downarrow}+h(\alpha I_{HR})^{\downarrow} \)<br>\( =(1-\alpha)h(I_{LR}^{\uparrow})^{\downarrow}+\alpha h(I_{HR})^{\downarrow} \)<br>\( =(1-\alpha)h(I_{LR})+\alpha h(I_{LR}) \)<br>\( =I_{LR} \)</p><p>噪声集B可以描述为：\( B={I_B|I_B=I_{HR}+n} \)<br>模糊集B中的图像降置为LR可以描述为：</p><p>\( h(I_B)^{\downarrow} \)<br>\( =h(I_{HR}+n)^{\downarrow} \)<br>\( =h(I_{HR})^{\downarrow}+h(n)^{\downarrow} \)<br>\( =h(I_{HR})^{\downarrow} \)<br>\( =I_{LR} \)</p><p>论文论证了模糊和带噪的SR图片下采样后都会等价于LR图片。</p><h5 id="Natural-Manifold-Discriminator"><a href="#Natural-Manifold-Discriminator" class="headerlink" title="Natural Manifold Discriminator"></a>Natural Manifold Discriminator</h5><p>这里论文构建了两类这样的图片，然后设计了一个判别器进行分类学习，具体设置见论文，判别器如下所示：</p><blockquote><p><img src="http://sotavision.cn/showdoc/server/../Public/Uploads/2020-03-24/5e79d4d569e85.png" alt="" loading="lazy"></p></blockquote><p>其中这个判别器的交叉熵损失函数定义为：</p><div>    \[ - \text{E}_{x\in{A\cup B}}[\log(1-D_{NM}(x))]-\text{E}_{x\in{N}}[\log(D_{NM}(x))] \]</div><h4 id="Natural-and-Realistic-Super-Resolution"><a href="#Natural-and-Realistic-Super-Resolution" class="headerlink" title="Natural and Realistic Super-Resolution"></a>Natural and Realistic Super-Resolution</h4><p>论文的SR网络，使用了残差dense block，网络结构如下所示：</p><blockquote><p><img src="http://sotavision.cn/showdoc/server/../Public/Uploads/2020-03-24/5e79d72147ae5.png" alt="" loading="lazy"><br><img src="http://sotavision.cn/showdoc/server/../Public/Uploads/2020-03-24/5e79d698d801c.png" alt="" loading="lazy"><br><img src="http://sotavision.cn/showdoc/server/../Public/Uploads/2020-03-24/5e79d73b9ccbf.png" alt="" loading="lazy"></p></blockquote><h4 id="Training-Loss-Function"><a href="#Training-Loss-Function" class="headerlink" title="Training Loss Function"></a>Training Loss Function</h4><ul><li>Reconstruction Loss</li></ul><p>\[ L_{Recon}=\text{E}[\left|I_{HR}-I_{SR} \right|_1] \]</p><ul><li>Naturalness Loss</li></ul><p>\[ L_{Natural}=\text{E}[-\log(D_{MN}(I_{SR}))] \]</p><ul><li>Adversarial Loss<br>使用相对真实生成对抗网络RaGAN，与ESRGAN使用的GAN相同。</li></ul><div>    \[ L_{G}=-\text{E}_{x_r\sim \text P_r}[\log(\tilde{D}(x_r))]-\text{E}_{x_f\sim \text P_g}[\log(1-\tilde{D}(x_f))] \]    \[ L_{D}=-\text{E}_{x_f\sim \text P_g}[\log(\tilde{D}(x_f))]-\text{E}_{x_r\sim \text P_r}[\log(1-\tilde{D}(x_r))] \]</div><p>其中：</p><div>    \[ \tilde{D}(x_r)=\text{sigmoid}(C(x_r)-\text E_{x_f\sim \text P_g}[C(x_f)]) \]    \[ \tilde{D}(x_f)=\text{sigmoid}(C(x_f)-\text E_{x_r\sim \text P_r}[C(x_r)]) \]</div>- Overall Loss<p>\[ L=\lambda_1L_{Recon}+\lambda_2L_{Natural}+\lambda_3L_{G} \]<br>FRSR:\( \lambda_2=\lambda_3=0 \)<br>NatSR：\( \lambda_1=1,\lambda_2=10^{-3}\text{and}\lambda_3=10^{-3} \)</p><hr><h3 id="Experiment"><a href="#Experiment" class="headerlink" title="Experiment"></a>Experiment</h3><h4 id="Experiments-Details"><a href="#Experiments-Details" class="headerlink" title="Experiments Details"></a>Experiments Details</h4><p>实验细节可以参考官方代码：<a href="https://github.com/JWSoh/NatSR" target="_blank" rel="noopener">https://github.com/JWSoh/NatSR</a>.</p><h4 id="FR-IQA-Results"><a href="#FR-IQA-Results" class="headerlink" title="FR-IQA Results"></a>FR-IQA Results</h4><blockquote><p><img src="http://sotavision.cn/showdoc/server/../Public/Uploads/2020-03-24/5e79dc87c7f6d.png" alt="" loading="lazy"></p></blockquote><ul><li>考虑到参数的数量，我们的FRSR也是一种有效的方法</li><li>对于面向感知的方法，我们的方法比SRGAN和EnhanceNet在像素域上更接近原始图像</li></ul><h4 id="NR-IQA-Results"><a href="#NR-IQA-Results" class="headerlink" title="NR-IQA Results"></a>NR-IQA Results</h4><blockquote><p><img src="http://sotavision.cn/showdoc/server/../Public/Uploads/2020-03-24/5e79dced15055.png" alt="" loading="lazy"><br>从图像评价指标上看，NatSR算法并不是最好的，但是作者认为，在视觉效果上，NatSR效果更好，在图像纹理和细节方面表现更好。</p></blockquote><hr><h3 id="Conclusions"><a href="#Conclusions" class="headerlink" title="Conclusions"></a>Conclusions</h3><ol><li>提出了一种基于残差密集块和分形残差学习的网络结构</li><li>提出了基于CNN的自然流形判别器（NMD）</li><li>设计了一种感知损失函数</li><li>与具有相似参数的模型相比，我们的方法具有相当大的增益。</li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;https://cdn.jsdelivr.net/npm/aplayer@1.10/dist/APlayer.min.css&quot;&gt;&lt;script 
      
    
    </summary>
    
    
      <category term="论文笔记" scheme="http://alexzou14.github.io/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="深度学习" scheme="http://alexzou14.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="笔记" scheme="http://alexzou14.github.io/tags/%E7%AC%94%E8%AE%B0/"/>
    
      <category term="超分辨率" scheme="http://alexzou14.github.io/tags/%E8%B6%85%E5%88%86%E8%BE%A8%E7%8E%87/"/>
    
  </entry>
  
  <entry>
    <title>Image Super-Resolution via Deep Recursive Residual Network论文阅读笔记</title>
    <link href="http://alexzou14.github.io/2020/04/07/DRRN/"/>
    <id>http://alexzou14.github.io/2020/04/07/DRRN/</id>
    <published>2020-04-07T10:47:09.000Z</published>
    <updated>2020-04-07T10:50:52.829Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="https://cdn.jsdelivr.net/npm/aplayer@1.10/dist/APlayer.min.css"><script src="https://cdn.jsdelivr.net/npm/aplayer@1.10/dist/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="https://cdn.jsdelivr.net/npm/meting@1.2/dist/Meting.min.js"></script><h3 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h3><p>因为卷积神经网络可以有效的学习低分辨率到高分辨率图像的非线性映射，所以在当时基于卷积神经网络的模型在单一图像超分辨率的问题上取得了巨大成功。同时CNN模型要耗费大量的运算资源，网络参数非常多。因此本文提出了一种非常深的残差网络，使用递归学习来控制参数数目和增加深度残差学习减少网络的训练难度。</p><hr><h3 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h3><p>由于SISR的目标是从LR图像上还原其中高频信息，所以这个问题应用很广，在医疗成像，安全监控等。在近期，CNN方法广泛的应用与解决各种不定问题，SISR问题自从SRCNN将CNN方法引入到图像超分辨率问题中，CNN方法在超分领域取得了很大的进步。作者回顾了当时众多方法的优缺点，SRCNN的计算成本高，后面就有人提出了ESPCN用亚像素卷积恢复图像，提高了图像恢复的效率。SRCNN的网络很浅，得到的结论不全对，后面Kim提出的VDSR利用比较深的网络和跳跃连接来恢复图像。后面为了减少网络参数，提出了DRCN利用递归来减少网络的参数。现在该领域存在的问题就是非常深的网络需要非常多的参数，大模型网络需要很大的存储空间，对移动系统的适应度很差。<br>作者受到以上方法的启发，提出了一种深度残差递归网络（DRRN）。<br>DRRN的创新点：</p><ol><li>在VDSR中只用了输入到输出端的全局残差（GRL），可以有效的降低训练深网络的难度，在此基础之上作者又添加了一个局部残差（LRL），有效的减少图像经深网络处理后细节的丢失。</li><li>和DRCN相比，作者提出了一个由多个残差单元组成的残差块进行参数共享，并且设计了一个多路径结构的递归块来解决梯度爆炸。</li></ol><hr><h3 id="Related-work"><a href="#Related-work" class="headerlink" title="Related work"></a>Related work</h3><blockquote><p><img src="http://sotavision.cn/showdoc/server/../Public/Uploads/2020-04-07/5e8c50dca8b27.png" alt="" loading="lazy"></p></blockquote><h4 id="SResNet"><a href="#SResNet" class="headerlink" title="SResNet"></a>SResNet</h4><p>ResNet中的残差学习框架简化了深度网络的训练，残差单元公式化为：<br>\[ F(\text x)=H(\text x)-\text x, \hat{\text x}=U(\text x)=\sigma(F(\text x,W)+h(\text x)) \]<br>h(x)为恒等映射(identity mapping)，F为残差函数，残差网络的核心就是去学习增加的残差函数F，这样的设计让网络更加容易训练并且不会产生过拟合</p><h4 id="VDSR"><a href="#VDSR" class="headerlink" title="VDSR"></a>VDSR</h4><p>VDSR中在输入的低分辨率图片和输出的高分辨图片之间引入了GRL，VDSR主要有三个特点</p><ul><li>VDSR在残差分支中使用了20个权重层，这能让网络的reception field增大</li><li>GRL(Global Residual Learning)能让VDSR快速收敛</li><li>通过尺度扩展，单个VDSR网络对不同尺度的图像具有较强的鲁棒性</li></ul><h4 id="DRCN"><a href="#DRCN" class="headerlink" title="DRCN"></a>DRCN</h4><p>添加更多的权层会引入更多的参数，其中模型可能会过拟合，并且可能会导致模型更大难以存储和复现，为了解决这些问题，作者在网络中引入了一个递归层，每层的递归层都是监督式的，包含三个部分:</p><ul><li>第一部分为embedding net，对输入中的特征进行提取，</li><li>第二层为为inference net，相当于特征的非线性变换， 将T个递归堆叠在一个递归层中，在这些递归之间共享权重</li><li>第三层为reconstruction net，即特征图重建</li></ul><hr><h3 id="Proposed-Method"><a href="#Proposed-Method" class="headerlink" title="Proposed Method"></a>Proposed Method</h3><h4 id="Network-structure"><a href="#Network-structure" class="headerlink" title="Network structure"></a>Network structure</h4><blockquote><p><img src="http://sotavision.cn/showdoc/server/../Public/Uploads/2020-04-07/5e8c50ce1fa5a.png" alt="" loading="lazy"></p></blockquote><p>作者在DRRN中将GRL和递归网络结合在一起使用，将多个残差单元堆积在一起，如下两幅图所示，在ResNet中，不同的残差单元对identity branch使用不同的输入，但是在DRRN中，使用了多路径结构，所有残差单元共享相同的identity branch的输入</p><blockquote><p><img src="http://sotavision.cn/showdoc/server/../Public/Uploads/2020-04-07/5e8c51101c2e9.png" alt="" loading="lazy"></p></blockquote><h4 id="Residual-Unit"><a href="#Residual-Unit" class="headerlink" title="Residual Unit"></a>Residual Unit</h4><blockquote><p><img src="http://sotavision.cn/showdoc/server/../Public/Uploads/2020-04-07/5e8c515dc7328.png" alt="" loading="lazy"></p></blockquote><p>在DRRN的残差单元将激活层放到了权重层的前面，通过别人的论文可知，这样的设计让网络更加容易训练,而每个残差单元之间的残差路径有助于学习高度复杂的有限元结构，同一路径有助于训练过程中的梯度反向传播，与ResNet中的链模式相比，这种模式更有利于学习，不容易过拟合，具体公式如下， 由于残差单元被递归学习，权重w在一个递归区中共享，但是在不同的递归区内不同。<br>\[ H^u=F(H^{u-1},W^u)+H^{u-1} \]<br>\[ H^u= G(H^{u-1})=F(H^{u-1},W)+H^{0} \]</p><h4 id="Recursive-Block"><a href="#Recursive-Block" class="headerlink" title="Recursive Block"></a>Recursive Block</h4><p>DRRN的网络结构如下图所示，通过叠加几个递归块，然后用卷积层重建LR和HR图像之间的残差。然后将剩余图像从输入LR image添加到全局标识映射中。整个DRRN网络结构如下图所示。实际上，VDSR可以看作是DRRN的一个特例。，当残差单元为0时，DRRN变为VDSR</p><blockquote><p><img src="http://sotavision.cn/showdoc/server/../Public/Uploads/2020-04-07/5e8c533c2b4c1.png" alt="" loading="lazy"></p></blockquote><p>DRRN网络中递归块的个数和残差单元数最终影响网络的性能，所以第u个残差单元可以表示为：<br>\[ H_b^u=G(H^{u-1})=F(H_b^{u-1},W_b)+H_b^0 \]<br>第b个递归块的输出可以表达为：<br>\[ \text x_b=H_b^UG^{(U)}(f_b(\text x_{b-1}))=G(G(\cdots(G(f_b(\text x_{b-1}))\cdots)) \]</p><h4 id="Network-Structure"><a href="#Network-Structure" class="headerlink" title="Network Structure"></a>Network Structure</h4><p>DRRN总的网络层数为：\( d=(1+2\times U)\times B +1 \)，R代表递归块,则输出的表达式为：<br>\[ \text y=D(\text x)=f_{Rec}(R_B(R_{B-1}(\cdots(R_1(\text x))\cdots)))+\text x \]</p><h4 id="Loss-function"><a href="#Loss-function" class="headerlink" title="Loss function"></a>Loss function</h4><p>作者在这里使用了MSE的loss function：<br>\[ L(\Theta)=\dfrac{1}{2N}\displaystyle\sum_{i=1}^N ||\tilde{\text x}^{(i)}-D(\text x^{i})||^2 \]</p><hr><h3 id="Experiment"><a href="#Experiment" class="headerlink" title="Experiment"></a>Experiment</h3><h4 id="Experiments-Details"><a href="#Experiments-Details" class="headerlink" title="Experiments Details"></a>Experiments Details</h4><p>实验细节可以参考官方代码：<a href="https://github.com/tyshiwo/DRRN" target="_blank" rel="noopener">https://github.com/tyshiwo/DRRN</a> CVPR17.<br>数据增强：图像翻转，旋转（90°、180°、270°），旋转后的图像水平翻转，数据量增加7倍，运用不同的放大倍数图像进行同一个模型训练，训练图像大小为31x31。</p><p>训练策略：SGD min-batch 128, momentum 0.9，初始学习率为0.1，每10轮学习率下降一半，同时使用梯度自动裁切技术。</p><p>卷积核相关：卷积核大小为3x3，个数为128</p><h4 id="Study-of-B-and-U"><a href="#Study-of-B-and-U" class="headerlink" title="Study of B and U"></a>Study of B and U</h4><blockquote><p><img src="http://sotavision.cn/showdoc/server/../Public/Uploads/2020-04-07/5e8c586b4e1ee.png" alt="" loading="lazy"></p></blockquote><p>B1U25可以使用更少的参数实现最先进的结果。</p><h4 id="Comparisons-with-state-of-the-arts"><a href="#Comparisons-with-state-of-the-arts" class="headerlink" title="Comparisons with state-of-the-arts"></a>Comparisons with state-of-the-arts</h4><p>利用52层的网络结构，利用递残差网络结构有两种实现方式：B1U25(k=297K), B17U1(k=7375K)(k表示参数数量)，与其他模型的比较结果如下PSNR和SSIM：</p><blockquote><p><img src="http://sotavision.cn/showdoc/server/../Public/Uploads/2020-04-07/5e8c591a3a9a0.png" alt="" loading="lazy"></p></blockquote><p>本文还利用了一种信息保真度(Information Fidelity Criterion IFC )的评价指标：</p><blockquote><p><img src="http://sotavision.cn/showdoc/server/../Public/Uploads/2020-04-07/5e8c593338956.png" alt="" loading="lazy"></p></blockquote><hr><h3 id="Conclusions"><a href="#Conclusions" class="headerlink" title="Conclusions"></a>Conclusions</h3><ol><li>针对单图像超分辨率的深度递归网络（DRRN）。</li><li>提出了一个增强的残差单元结构在一个递归块中递归学习</li><li>提出了局部残差和全局残差共同作用于网络，权值共享等提高网络的性能。</li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;https://cdn.jsdelivr.net/npm/aplayer@1.10/dist/APlayer.min.css&quot;&gt;&lt;script 
      
    
    </summary>
    
    
      <category term="论文笔记" scheme="http://alexzou14.github.io/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="深度学习" scheme="http://alexzou14.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="笔记" scheme="http://alexzou14.github.io/tags/%E7%AC%94%E8%AE%B0/"/>
    
      <category term="超分辨率" scheme="http://alexzou14.github.io/tags/%E8%B6%85%E5%88%86%E8%BE%A8%E7%8E%87/"/>
    
  </entry>
  
  <entry>
    <title>EnhanceNet：Single ImageSuper-Resolution through Automated Texture Synthesis论文阅读笔记</title>
    <link href="http://alexzou14.github.io/2020/04/06/EhanceNet/"/>
    <id>http://alexzou14.github.io/2020/04/06/EhanceNet/</id>
    <published>2020-04-06T09:43:53.000Z</published>
    <updated>2020-04-06T09:57:27.146Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="https://cdn.jsdelivr.net/npm/aplayer@1.10/dist/APlayer.min.css"><script src="https://cdn.jsdelivr.net/npm/aplayer@1.10/dist/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="https://cdn.jsdelivr.net/npm/meting@1.2/dist/Meting.min.js"></script><p>文章地址：<a href="https://arxiv.org/pdf/1612.07919.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1612.07919.pdf</a></p><h3 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h3><p>在当时，图像超分辨率的性能评判指标是通过PSNR来测量的，但是PSNR与人感知相关性差，PSNR过小会造成图像过于平滑缺少纹理看上去不够自然。作者提出了一种结合纹理损失的自动纹理合成的新颖应用，该损失专注于创建逼真的纹理，而不是针对训练过程中像素精度的地面真实图像再现进行优化。 通过在对抗训练环境中使用前馈全卷积神经网络，我们可以在高放大倍率下显着提高图像质量。 在大量数据集上进行的广泛实验证明了我们方法的有效性，在定量和定性基准方面都产生了最新的结果。</p><hr><h3 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h3><p>超分辨率问题的不定性：对HR图像进行降采样时，大量不同的HR图像会产生相同的LR图像。这导致SISR成为高度复杂的问题。一个关键问题是大量降采样因子导致高频信息丢失，从而使超分辨图像中的纹理区域变得模糊，过分平滑且外观不自然。如下图所示：</p><blockquote><p><img src="http://sotavision.cn/showdoc/server/../Public/Uploads/2020-03-30/5e8155f778b87.png" alt="" loading="lazy"></p></blockquote><p>原因是，选择了当前最先进的方法所采用的目标函数：大多数系统将HR Ground Truth 图像与其从HR真实图像重建之间的像素平均均方误差（MSE）降至最低。 然而，LR观察与人类对图像质量的感知之间的相关性很差。 虽然易于最小化，但最佳的MSE估计器返回许多可能解的平均值，这使得SISR结果看起来不自然且难以置信。 在超分辨率的背景下，这种均值回归问题是众所周知的事实，但是，对自然图像的高维多峰分布进行建模仍然是一个具有挑战性的问题。</p><blockquote><p><img src="http://sotavision.cn/showdoc/server/../Public/Uploads/2020-03-30/5e81576a3af3b.png" alt="" loading="lazy"></p></blockquote><p>作者因此提出了一种新的文理合成网络改进方案，结合了对抗训练的感知损失。</p><hr><h3 id="Related-work"><a href="#Related-work" class="headerlink" title="Related work"></a>Related work</h3><p>作者在这部分简要的说明了早期传统方法容易产生模糊和伪影。当时比较受欢迎的方法有基于外部实例的方法和董超老师提出来的深度学习方法。因为这些模型都是通过优化最小均方误差（MSE）得到的，所以这些结果都趋于模糊和缺少高频细节的。一种感知损失的提出，虽然PSNR更低但是也锐化了输出结果。对抗网络就是当时可以得到比较好的锐化结果，但是同时这个网络也会是的生成图像添加一些不必要的伪影。作者就是在这些工作基础上展开的。</p><hr><h3 id="Proposed-Method"><a href="#Proposed-Method" class="headerlink" title="Proposed Method"></a>Proposed Method</h3><h4 id="Single-image-super-resolution"><a href="#Single-image-super-resolution" class="headerlink" title="Single image super-resolution"></a>Single image super-resolution</h4><p>一张高分辨率图片\( I_{HR}\in[0,1]^{\alpha w\times \alpha h\times c} \)下采样为低分辨率图像：<br>\( I_{LR}=d_{\alpha}(I_{HR})\in [0,1]^{w\times h\times c} \)<br>使用下采样操作表示为：<br>\( d_{\alpha}:[0,1]^{\alpha w\times \alpha h\times c}\rightarrow [0,1]^{w\times h\times c} \)<br>固定比例因子\( \alpha &gt;1 \)，图像宽度\(w\)，高度\(h\)和颜色通道\(c\)。 SISR的任务是提供一个从\(I_{LR}\)估计\(I_{HR}\)的近似逆\(f\approx d^{-1}\)：<br>由于下采样操作d是non-injective 的，并且存在大量可能的图像\(I_{est}\)，而\(d(I_{est})=I_{LR}\)保持不变，因此该问题很复杂。<br>最近的学习方法旨在通过使当前估计和ground truth图像之间的欧几里得损失\(\left |{I_{est}-I_{HR}}\right|_2^2\)最小化，通过多层神经网络来近似\(f\)。 尽管这些模型通过PSNR可以得出出色的结果，但所得图像往往看起来模糊并且缺少原始图像中存在的高频纹理。 这是SISR模棱两可的直接结果：由于下采样会从输入图像中去除高频信息，因此没有方法希望以像素为单位重现所有精细的细节。 因此，即使是最先进的模型也要学习在那些区域中生成所有可能纹理的均值，以使输出图像的欧几里得损失最小化。</p><h4 id="Network-structure"><a href="#Network-structure" class="headerlink" title="Network structure"></a>Network structure</h4><p>网络结构采用全卷积的方式，使得输入图像可以是任意尺寸。受到VGG网络的启发，卷积核全部采用3*3的尺寸，在保持一定量参数的情况下构建更深的网络。网络的输入是低分辨率图像，在网络末端采用最近邻的方法上采样达到高分辨率图像的尺寸，这样有利于降低计算复杂度，参数的初始化采用Xavier，网络的学习目标是超分辨率图像与输入的低分辨率图像线性插值的差，整个网络架构如下图所示</p><blockquote><p><img src="http://sotavision.cn/showdoc/server/../Public/Uploads/2020-03-30/5e81610a96e33.png" alt="" loading="lazy"></p></blockquote><p>超分辨率重建网络+感知网络+对抗网络(判别器)<br>第一不用bicubic interpolation,原因是，过多的引入冗余，而且计算量大。第二，不用transpose convolution ,原因是，过多的引入冗余，同时感受野增加了。另外，transconvolution容易产生checkerboard artifacts，需要加入而外的惩罚loss。<br>超分辨率重建网络=ResNet block(下采样path)+nearest upsampling(上采样路径).</p><h4 id="Loss-function"><a href="#Loss-function" class="headerlink" title="Loss function"></a>Loss function</h4><h5 id="Pixel-wise-loss-in-the-image-space"><a href="#Pixel-wise-loss-in-the-image-space" class="headerlink" title="Pixel-wise loss in the image-space"></a>Pixel-wise loss in the image-space</h5><div>    \( L_E=\left\|I_{est}-I_{HR}\right\|_2^2 \)</div>其中 <div>    \(\left\|I\right\|_2^2=\dfrac{1}{whc}\displaystyle\sum_{w,h,c}(I_{w,h,c})^2\)</div>pixel-wise loss强调的是两幅图像之间每个对应像素的匹配，这与人眼的感知结果有所区别。通过pixel-wise loss训练的图片通常会较为平滑，缺少高频信息。即使输出图片具有较高的PSNR，视觉效果也并没有很突出。<h5 id="Perceptual-loss-in-feature-space"><a href="#Perceptual-loss-in-feature-space" class="headerlink" title="Perceptual loss in feature space"></a>Perceptual loss in feature space</h5><p>这个损失的计算是把\(I_{est}\)和\(I_{HR}\)送入一个函数\(\phi\)计算在\(\phi\)映射下的L2损失：<br>\(L_P=\left|\phi (I_{est})-\phi(I_{HR})\right|_2^2\)<br>我们避免了要求网络输出图像与原始高分辨率图像pixel-wise上的一致，而是鼓励两幅图具有相似的特征。<br>一般\(\phi\)可以用预训练好的VGG19在第二和第五池化层的输出。</p><h5 id="Texture-matching-loss"><a href="#Texture-matching-loss" class="headerlink" title="Texture matching loss"></a>Texture matching loss</h5><p>该损失的表示为：\(L_P=\left|G(\phi (I_{est}))-G(\phi(I_{HR}))\right|_2^2\)其中G为gram matrix:\(G(F)=FF^T\)<br>这个和分割迁移的损失类似，用在这的主要目的是生成和HR图像相似的局部纹路。经验是16*16生成的效果最好。</p><h5 id="Adversarial-training"><a href="#Adversarial-training" class="headerlink" title="Adversarial training"></a>Adversarial training</h5><p>对抗训练是一种最新技术，已被证明是产生逼真的图像的有用机制。 在原始设置中，对生成网络G进行了训练， 同时，判别网络D被训练以区分来自数据集的真实图像x和生成的样本G(z)。这种方法导致了minimax游戏，其中训练了生成器以使其loss最小化:\(L_A=-\log (D(G(z)))\)<br>鉴别器loss最小化:\(L_D=-\log (D(x))-\log (1-D(G(z)))\)</p><blockquote><p><img src="http://sotavision.cn/showdoc/server/../Public/Uploads/2020-03-30/5e8166cd4e466.png" alt="" loading="lazy"></p></blockquote><hr><h3 id="Experiment"><a href="#Experiment" class="headerlink" title="Experiment"></a>Experiment</h3><h4 id="Experiments-Details"><a href="#Experiments-Details" class="headerlink" title="Experiments Details"></a>Experiments Details</h4><p>具体实验细节待复现</p><h4 id="Effect-of-different-losses"><a href="#Effect-of-different-losses" class="headerlink" title="Effect of different losses"></a>Effect of different losses</h4><p>我们使用表2中列举的Loss组合进行训练，并将结果展示在表3中。更多的结果在补充中。</p><blockquote><p><img src="http://sotavision.cn/showdoc/server/../Public/Uploads/2020-03-30/5e81674cef1ed.png" alt="" loading="lazy"><br><img src="http://sotavision.cn/showdoc/server/../Public/Uploads/2020-03-30/5e81678163846.png" alt="" loading="lazy"></p></blockquote><h4 id="Evaluation-of-perceptual-quality"><a href="#Evaluation-of-perceptual-quality" class="headerlink" title="Evaluation of perceptual quality"></a>Evaluation of perceptual quality</h4><p>在ImgeNet数据集上对用户进行问卷调查，其中91.0%选择由ENet-PAT产生的图像</p><hr><h3 id="Conclusions"><a href="#Conclusions" class="headerlink" title="Conclusions"></a>Conclusions</h3><p>本文主要贡献：</p><ul><li>我们提出了一种体系结构，该体系结构能够通过欧几里得损失训练或对抗性训练，感知损失和新提出的用于超分辨率的纹理转移损失的新颖组合，通过定量和定性测量产生最新结果 。</li></ul><p>存在的限制性：</p><ul><li>由于SISR是一个严重的问题，因此仍然存在一些局限性。 虽然由ENet-PAT生成的图像看上去逼真，但它们与像素像素的真实图像不匹配。 此外，对抗训练有时会在输出中产生伪影，这些伪影会大大减少，但不会增加纹理损失，因此无法完全消除。</li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;https://cdn.jsdelivr.net/npm/aplayer@1.10/dist/APlayer.min.css&quot;&gt;&lt;script 
      
    
    </summary>
    
    
      <category term="论文笔记" scheme="http://alexzou14.github.io/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="深度学习" scheme="http://alexzou14.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="笔记" scheme="http://alexzou14.github.io/tags/%E7%AC%94%E8%AE%B0/"/>
    
      <category term="超分辨率" scheme="http://alexzou14.github.io/tags/%E8%B6%85%E5%88%86%E8%BE%A8%E7%8E%87/"/>
    
  </entry>
  
  <entry>
    <title>Enhanced Deep Residual Networks for Single Image Super-Resolution论文阅读笔记</title>
    <link href="http://alexzou14.github.io/2020/04/06/EDSR/"/>
    <id>http://alexzou14.github.io/2020/04/06/EDSR/</id>
    <published>2020-04-06T09:41:06.000Z</published>
    <updated>2020-04-06T09:43:06.824Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="https://cdn.jsdelivr.net/npm/aplayer@1.10/dist/APlayer.min.css"><script src="https://cdn.jsdelivr.net/npm/aplayer@1.10/dist/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="https://cdn.jsdelivr.net/npm/meting@1.2/dist/Meting.min.js"></script><p>论文地址：<a href="https://arxiv.org/pdf/1707.02921.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1707.02921.pdf</a></p><h3 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h3><p> 随着深度卷积神经网络（DCNN）的发展，最近在图像超分辨率方面的研究也取得了进展。尤其，残差学习技术表现出很好的性能。在这篇文章中，作者介绍了：</p><ol><li>提出一种增强的深度超分辨率网络（enhanced deep super-resolution，简称 EDSR），其性能超过当前最先进的超分辨率（SR）方法。</li><li>作者提出的模型通过删除常规残差网络中不必要的模块进行优化，实现了显著的性能提高。</li><li>提出一种新的多尺度深度超分辨率系统（multi-scale deep super-resolution，简称MDSR）和训练方法，可以将单个模型中不同的放大因子（upscaling factors）重建为高分辨率图像。</li></ol><hr><h3 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h3><p>作者首先介绍了DeepNet的局限性：</p><ol><li>模型的重构性能对较小的体系结构变化很敏感，相同模型基于不同的初始化和训练技巧可以得到不同等级的性能表现。</li><li>大多数现行的SR算法将不同的scale视为独立的问题，并没有利用SR间不同scale的关联，所以每一种scale都需要单独训练。</li></ol><p>VDSR模型是可以同时解决不同scale问题的，这说明尺度特定的模型的训练是有冗余的，但是VDSR的LR是需要通过bicubic成HR尺寸的，没有直接在LR空间进行计算，损失了LR空间的图像细节并增大了计算复杂度。<br>为了解决这个问题呢，作者主要采用了以下两点：</p><ol><li>移除了不必要的模型，主要就是移除了BN</li><li>设计了带有一个单一主分支的基准（多尺度 —— multi-scale）模块 ，含有 B = 16 的残差模块，所以大部分参数都可以在不同尺度间进行共享，</li></ol><hr><h3 id="Related-work"><a href="#Related-work" class="headerlink" title="Related work"></a>Related work</h3><p>这里作者回顾了超分辨率研究的方法，从传统方法到神经网络方法，然后重点说明了深度学习方法在近几年的发展。董超老师提出的SRCNN将深度学习方法引入到超分领域中，让后不断的发展，VDSR引入了跳层连接有效的缓解了梯度爆炸问题等等。<br>作者还提出MSE或者L2 loss可能并不能保证在PSNR和SSIM上有更好的性能，所以这里采用了L1 Loss函数。</p><hr><h3 id="Proposed-Method"><a href="#Proposed-Method" class="headerlink" title="Proposed Method"></a>Proposed Method</h3><h4 id="EDSR"><a href="#EDSR" class="headerlink" title="EDSR"></a>EDSR</h4><p>提出了一个增强版的结构简单的残差网络模型：</p><blockquote><p><img src="http://sotavision.cn/showdoc/server/../Public/Uploads/2020-03-31/5e82ec74e94c1.png" alt="" loading="lazy"></p></blockquote><p>去掉BN后，相同的计算资源下，EDSR就可以堆叠更多的网络层或者使每层提取更多的特征，从而得到更好的性能表现。但是增加特征图的数量将使训练过程不稳定，作者提出了一个residual scaling来解决该问题。在每个residual block的最后一个卷积层后添加一个constant scaling layer，可以帮助稳定训练（尤其是在应用很多的filter时）。</p><blockquote><p><img src="http://sotavision.cn/showdoc/server/../Public/Uploads/2020-03-31/5e82ed24cf1e5.png" alt="" loading="lazy"></p></blockquote><p>当训练缩放因子为×3，×4的EDSR时，作者用预训练过的×2的网络来初始化模型参数。这个策略加速了训练并且提升了最后的性能表现。对于×4，使用预训练×2的模型训练过程中会收敛很快。</p><blockquote><p><img src="http://sotavision.cn/showdoc/server/../Public/Uploads/2020-03-31/5e82ee74cee60.png" alt="" loading="lazy"></p></blockquote><h4 id="MDSR"><a href="#MDSR" class="headerlink" title="MDSR"></a>MDSR</h4><p>在MDSR网络头部设置了一个预处理模块，用来减少输入图像不同尺寸的差异，每个预处理模块包含两个卷积核为5×5的残差块。在MDSR的末尾，不同的scale-upsampling模型平行设置来处理各种尺度的SR预测。</p><p>MDSR withB = 80 and F = 64.</p><blockquote><p><img src="http://sotavision.cn/showdoc/server/../Public/Uploads/2020-03-31/5e82ef178192c.png" alt="" loading="lazy"></p></blockquote><hr><h3 id="Experiment"><a href="#Experiment" class="headerlink" title="Experiment"></a>Experiment</h3><h4 id="Experiments-Details"><a href="#Experiments-Details" class="headerlink" title="Experiments Details"></a>Experiments Details</h4><ul><li>实验数据用的是比赛提供的DIV2K，它有800张训练图像，100张验证图像，100张测试图像。每张都是2k分辨率。</li><li>预处理是，每张图像减去DIV2K的总平均值。另外在训练时，损失函数用L1而不是L2，源码用torch7封装。</li><li>提一下MDSR的训练，是*2 *3 *4三中的尺度随机混合作为训练集，在更新梯度时，只有对应尺度的那部分参数更新。</li></ul><h4 id="Geometric-Self-ensemble"><a href="#Geometric-Self-ensemble" class="headerlink" title="Geometric Self-ensemble"></a>Geometric Self-ensemble</h4><p>这是一个很神奇的方法，测试时，把图像90度旋转以及翻转，总共有8种不同的图像，分别进网络然后变换回原始位置，8张图像再取平均。这个方法可以使测试结果有略微提高。Note that geometric self-ensemble is valid only for symmetric downsampling methods such as bicubic downsampling.</p><h4 id="Comparisons-with-state-of-the-arts"><a href="#Comparisons-with-state-of-the-arts" class="headerlink" title="Comparisons with state-of-the-arts"></a>Comparisons with state-of-the-arts</h4><blockquote><p><img src="http://sotavision.cn/showdoc/server/../Public/Uploads/2020-03-31/5e82f02b03e36.png" alt="" loading="lazy"><br><img src="http://sotavision.cn/showdoc/server/../Public/Uploads/2020-03-31/5e82f0647f1ae.png" alt="" loading="lazy"></p></blockquote><hr><h3 id="Conclusions"><a href="#Conclusions" class="headerlink" title="Conclusions"></a>Conclusions</h3><ol><li>在Residual block中删除了BN，卷积完成后也不Relu。并控制了残差规模来降低训练复杂度。</li><li>MDSR具有尺度相关的模块和共享的主网络，能够在统一的框架内有效地处理各种尺度的超分辨率问题</li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;https://cdn.jsdelivr.net/npm/aplayer@1.10/dist/APlayer.min.css&quot;&gt;&lt;script 
      
    
    </summary>
    
    
      <category term="论文笔记" scheme="http://alexzou14.github.io/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="深度学习" scheme="http://alexzou14.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="笔记" scheme="http://alexzou14.github.io/tags/%E7%AC%94%E8%AE%B0/"/>
    
      <category term="超分辨率" scheme="http://alexzou14.github.io/tags/%E8%B6%85%E5%88%86%E8%BE%A8%E7%8E%87/"/>
    
  </entry>
  
  <entry>
    <title>Image Restoration Using Convolutional Auto-encoders with Symmetric Skip Connections论文阅读笔记</title>
    <link href="http://alexzou14.github.io/2020/04/06/RED/"/>
    <id>http://alexzou14.github.io/2020/04/06/RED/</id>
    <published>2020-04-06T09:31:55.000Z</published>
    <updated>2020-04-06T09:36:03.663Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="https://cdn.jsdelivr.net/npm/aplayer@1.10/dist/APlayer.min.css"><script src="https://cdn.jsdelivr.net/npm/aplayer@1.10/dist/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="https://cdn.jsdelivr.net/npm/meting@1.2/dist/Meting.min.js"></script><h3 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h3><p>本文提出了一种非常深的深卷积自动编码网络。因为卷积层在消除图像失真的同时捕获了图像内容的抽象细节，反卷积层具有向上采样特征图和恢复图像细节的能力，所以这个网络有很多的卷积和反卷积层构成，学习了从LR图像到HR图像端到端的映射。由于更深的网络很难训练存在梯度爆炸的问题，所以本文采用了将卷积层和反卷积层使用跳跃连接。卷积层和反卷积层的跳跃连接有两个优点:</p><ol><li>这种设计允许信号直接反向传播到底层，从而解决了梯度爆炸的问题，使得训练深度网络更加容易。提高了性能</li><li>这样的设计有利于恢复更干净的图像。</li></ol><p>同时这个模型可以用于图像去噪，去除JPEG压缩伪影和图像嵌入等问题。</p><hr><h3 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h3><p>图像恢复的任务是从其损坏的观察中恢复干净的图像，这已知是一种不适定的逆问题。 通过适应不同类型的腐败分布，相同的数学模型适用于图像去噪和超分辨率等问题。当时，深度神经网络已经在图像处理和计算机视觉任务中表现出优越的性能，从高级识别， 语义分割到低级去噪，超分辨率，去模糊，修复和恢复来自压缩图像的原始图像。 尽管深度神经网络取得了进展，但一些研究问题仍有待解决。 例如，更深层次的网络通常可以实现更好的性能吗？ 可以设计一个能够处理不同级别降置的深度模型吗？<br>观察DNN最近在图像处理任务上的优越性能，我们提出了一种基于卷积神经网络（CNN）的图像恢复框架。我们观察到，为了获得良好的恢复性能，训练非常深的模型是有益的。同时，我们表明，由于大容量网络的优势，在处理多个不同级别的损坏时，单个网络可以实现非常有前途的性能。具体而言，所提出的框架学习从损坏的图像到干净的图像的端到端完全卷积映射。该网络由多层卷积和反卷积运算符组成。由于更深层次的网络往往更难以训练，我们建议将卷积层和反卷积层对称地连接到跳层连接，训练过程收敛得更快，更有可能获得高质量的局部最优。我们的主要贡献总结如下。</p><ol><li>提出了一个非常深入的图像恢复网络结构，卷积层作为特征处理编码，反卷积层解码恢复图像。</li><li>提出了在卷积层与反卷积层之间增加跳跃连接层，跳跃连接有助于将梯度反向传播到底层，并将图像细节传播到顶层。</li><li>将这个网络应用在其他各种应用场景中取得了良好的恢复性能。</li></ol><hr><h3 id="Related-work"><a href="#Related-work" class="headerlink" title="Related work"></a>Related work</h3><p>关工作文献中的图像修复已经做了大量工作。诸如Total variation，BM3D算法和基于字典学习的方法等传统方法在图像去噪和超分辨率等图像恢复主题上表现出非常好的性能。由于图像恢复通常是一个不适定的问题，正规化的使用已被证明是必不可少的。堆积去噪自动编码器是最着名的DNN模型之一，可用于图像恢复。组合稀疏编码和DNN预训练与去噪自动编码器，用于低级视觉任务，如图像去噪和修复。其他基于神经网络的方法，如多层感知器和CNN用于图像去噪，以及DNN用于图像或视频超分辨率和压缩伪影减少这些年来一直在积极研究。Burger等人提出了一种基于补丁的算法，用简单的多层感知器学习。他们还得出结论，对于大型网络，大型训练数据，神经网络可以实现最先进的图像去噪性能。 Jain和Seung 提出了一个完全卷积的CNN用于去噪。他们发现CNN提供与小波和马尔可夫随机场（MRF）方法相当甚至更优越的性能。崔等人在多尺度上对输入图像采用非局部自相似（NLSS）搜索，然后以逐层方式使用协同局部自编码器进行超分辨率。董超老师等人提出直接学习低/高分辨率图像之间的端到端映射。王等人认为，传统稀疏编码所代表的领域专业知识可以结合起来，以实现进一步改进的结果。 DNN方法的一个优点是这些方法纯粹是数据驱动的，并且没有关于噪声分布的假设。</p><hr><h3 id="Proposed-Method"><a href="#Proposed-Method" class="headerlink" title="Proposed Method"></a>Proposed Method</h3><h4 id="Network-structure"><a href="#Network-structure" class="headerlink" title="Network structure"></a>Network structure</h4><p>网络整体结构，如下图：</p><blockquote><p><img src="http://sotavision.cn/showdoc/server/../Public/Uploads/2020-04-06/5e8ae477ba16d.png" alt="" loading="lazy"></p></blockquote><p>该框架完全是卷积和反卷积的。在每次卷积和反卷积之后添加校正层。卷积层充当特征提取器，它保留图像中对象的主要组件，同时消除损坏。然后组合去卷积层以恢复图像内容的细节。反卷积层的输出是输入图像的“干净”版本。此外，跳过连接也从卷积层添加到其对应的镜像反卷积层。<br>卷积特征映射以元素方式传递给解卷积特征映射并与解卷积特征映射求和，并在校正后传递到下一层。对于低级别的图像恢复问题，我们更喜欢在网络中不使用池化或解除池化，因为通常池化会丢弃对这些任务至关重要的有用图像细节。</p><h4 id="Deconvolution-decoder"><a href="#Deconvolution-decoder" class="headerlink" title="Deconvolution decoder"></a>Deconvolution decoder</h4><p>卷积：特征提取，随卷积进行，图像特征被提取，同时噪声的效果被降低，经过多层卷积后，图像的特征被提取出来，也降低了噪声的影响。</p><p>反卷积：针对特征的上采样，完成由图像特征到图像的转换，由于利用的是过滤后的噪声后的图像特征，因此达到了降噪、图像修复的目的。</p><p>文中通过实验说明利用反卷积结构而不用padding 和upsampling的原因，如下图，反卷积对图像细节有补偿作用。</p><blockquote><p><img src="http://sotavision.cn/showdoc/server/../Public/Uploads/2020-04-06/5e8af334c374c.png" alt="" loading="lazy"></p></blockquote><h4 id="Skip-connections"><a href="#Skip-connections" class="headerlink" title="Skip connections"></a>Skip connections</h4><p>作用：</p><p>1）保留更多的图像细节，协助反卷积层完成图像的恢复工作；</p><p>2）反向传播过程中的梯度反向，减少梯度消失，加快模型训练，文章有一些公式推导说明跳层连接对梯度变化的好处，详见论文；</p><p>利用图像修复实验说明跳层作用：</p><blockquote><p><img src="http://sotavision.cn/showdoc/server/../Public/Uploads/2020-04-06/5e8af364b2498.png" alt="" loading="lazy"><br><img src="http://sotavision.cn/showdoc/server/../Public/Uploads/2020-04-06/5e8af36f42064.png" alt="" loading="lazy"></p></blockquote><h4 id="Training"><a href="#Training" class="headerlink" title="Training"></a>Training</h4><p>局部模块结构：</p><blockquote><p><img src="http://sotavision.cn/showdoc/server/../Public/Uploads/2020-04-06/5e8af2f3cbf08.png" alt="" loading="lazy"></p></blockquote><p>卷积和反卷积层可以表示为：\( F(X)=\max(0,W_k \ast X+B_k) \)其中\( * \)代表卷积和反卷积操作。<br>ReLU的激活函数可以表达为：\( F(X_1,X_2)=\max(0,X_1+X_2) \)</p><p>使用的loss function为MSE：\( L(\Theta)=\dfrac{1}{N}\displaystyle\sum_{i=1}^N||F(X^i;\Theta)-Y^i||_F^2 \)<br>使用了SGD的方法来进行训练。</p><hr><h3 id="Experiment"><a href="#Experiment" class="headerlink" title="Experiment"></a>Experiment</h3><h4 id="Experiments-Details"><a href="#Experiments-Details" class="headerlink" title="Experiments Details"></a>Experiments Details</h4><p>参考code: <a href="https://github.com/SSinyu/RED_CNN" target="_blank" rel="noopener">https://github.com/SSinyu/RED_CNN</a></p><h4 id="Network-analysis"><a href="#Network-analysis" class="headerlink" title="Network analysis"></a>Network analysis</h4><p><strong>filter number</strong>:<br>滤波器大小为3x3,patch大小为50x50,跳层层数为2，不同的滤波个数，32,64,128，结果如下：</p><blockquote><p><img src="http://sotavision.cn/showdoc/server/../Public/Uploads/2020-04-06/5e8af5a0db0b2.png" alt="" loading="lazy"></p></blockquote><p><strong>filter size</strong>:<br>滤波器个数为64,patch大小为50x50,跳层层数为2，不同的滤波器大小为，3x3,5x5,7x7,9x9，结果如下：</p><blockquote><p><img src="http://sotavision.cn/showdoc/server/../Public/Uploads/2020-04-06/5e8af5d533455.png" alt="" loading="lazy"></p></blockquote><p><strong>training patch size</strong>:<br>滤波器个数为64,滤波器大小为3x3 ,跳层层数为2，不同的patch size，25x25, 50x50,75x75,100x100结果如下：</p><blockquote><p><img src="http://sotavision.cn/showdoc/server/../Public/Uploads/2020-04-06/5e8af618ac327.png" alt="" loading="lazy"></p></blockquote><p><strong>step size pf skip connections</strong>:<br>滤波器个数为64,滤波器大小为3x3 ,跳层层数为2,4,7，不同的patch size为50x50结果如下：</p><blockquote><p><img src="http://sotavision.cn/showdoc/server/../Public/Uploads/2020-04-06/5e8af63939f59.png" alt="" loading="lazy"></p></blockquote><p>结论：滤波器个数越多，大小越大，训练图像越大，跳层越少性能越好；</p><h4 id="Comparisons-with-state-of-the-arts"><a href="#Comparisons-with-state-of-the-arts" class="headerlink" title="Comparisons with state-of-the-arts"></a>Comparisons with state-of-the-arts</h4><p>文章中进行了图像去噪、超解像、JPEG deblocking、Non-blind deblurring、图像修复实验，列举一下超解像相关结果：</p><blockquote><p><img src="http://sotavision.cn/showdoc/server/../Public/Uploads/2020-04-06/5e8af66598c25.png" alt="" loading="lazy"></p></blockquote><p>与VDSR和DRCN进行了比较：</p><blockquote><p><img src="http://sotavision.cn/showdoc/server/../Public/Uploads/2020-04-06/5e8af686692bb.png" alt="" loading="lazy"></p></blockquote><hr><h3 id="Conclusions"><a href="#Conclusions" class="headerlink" title="Conclusions"></a>Conclusions</h3><ol><li>作者提出了一个非常深入的图像恢复网络结构，卷积层作为特征处理编码，反卷积层解码恢复图像。</li><li>作者提出了在卷积层与反卷积层之间增加跳跃连接层，跳跃连接有助于将梯度反向传播到底层，并将图像细节传播到顶层。</li><li>将这个网络应用在其他各种应用场景中取得了良好的恢复性能。</li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;https://cdn.jsdelivr.net/npm/aplayer@1.10/dist/APlayer.min.css&quot;&gt;&lt;script 
      
    
    </summary>
    
    
      <category term="论文笔记" scheme="http://alexzou14.github.io/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="深度学习" scheme="http://alexzou14.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="笔记" scheme="http://alexzou14.github.io/tags/%E7%AC%94%E8%AE%B0/"/>
    
      <category term="超分辨率" scheme="http://alexzou14.github.io/tags/%E8%B6%85%E5%88%86%E8%BE%A8%E7%8E%87/"/>
    
  </entry>
  
  <entry>
    <title>Deeply-Recursive Convolutional Network for Image Super-Resolution论文阅读笔记</title>
    <link href="http://alexzou14.github.io/2020/04/05/DRCN/"/>
    <id>http://alexzou14.github.io/2020/04/05/DRCN/</id>
    <published>2020-04-05T15:41:36.000Z</published>
    <updated>2020-04-05T16:54:36.505Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="https://cdn.jsdelivr.net/npm/aplayer@1.10/dist/APlayer.min.css"><script src="https://cdn.jsdelivr.net/npm/aplayer@1.10/dist/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="https://cdn.jsdelivr.net/npm/meting@1.2/dist/Meting.min.js"></script><h3 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h3><p>本文提出了使用深度递归卷积网络（DRCN）的图像超分辨率方法（SR）。 DRCN有一个非常深的递归层（最多16个递归）。增加递归深度可以提高性能，而不会为附加卷积引入新参数。由于网络特别深，所以存在梯度爆炸的问题，无法用标准梯度下降来学习。作者就利用率跳跃连接和递归监督的方式解决了这一问题。</p><hr><h3 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h3><p>对于图像超分辨率（SR），卷积网络的感受野决定了可以利用的上下文信息的量来推断丢失的高频分量。由于SR是一个不定逆向问题，收集和分析更多的邻域像素可对恢复下采样后可能丢失的信息提供更多线索。各种计算机视觉任务的深度卷积网络（DCN）通常使用非常大的感受野，在扩展感受野的许多方法中，增加网络深度是一种可能的方式。利用大于1×1的过滤器大小的卷积（转换）层或者减少中间表示的维度的池层可以使用的。 这两种方法都有缺点：层引入更多的参数和池层。网络层通常丢弃一些像素级别的信息。<br>对于图像恢复问题，如超分辨率和去噪，图像细节非常重要。因此，这些问题的大多数深度学习方法不能使用池化技术。通过添加新的网络层来增加深度，基本上会引入更多的参数。可能会出现两个问题。首先，很可能会过度拟合，需要更多的训练数据，使模型变得越来越大。<br>为了解决这些问题，作者使用深度递归卷积网络（DRCN）。DRCN根据需要重复地使用相同的卷积层参数。参数的数量不会增加，而执行了更多的递归。作者发现用广泛使用的随机梯度下降法优化的DRCN不容易收敛。这是由于爆炸/消失的梯度。提出了一个高性能的模型结构，能够捕捉像素长程的依赖，在保持较小模型的情况下，有更宽的感受野。<br>作者提出两种方法来缓解训练的难度：对所有递归进行监督，使用从输入到重建层的跳跃连接。</p><hr><h3 id="Related-work"><a href="#Related-work" class="headerlink" title="Related work"></a>Related work</h3><h4 id="Single-Image-Super-Resolution"><a href="#Single-Image-Super-Resolution" class="headerlink" title="Single-Image Super-Resolution"></a>Single-Image Super-Resolution</h4><p>早期单一图像超分辨率早期使用插值的方法，但是效果很差，后面又提出了统计先验和内部patch方法，最近基于学习的方法SRCN的提出，证明了端到端的方法在SR问题上的可行性。</p><h4 id="Recursive-Neural-Network-in-Computer-Vision"><a href="#Recursive-Neural-Network-in-Computer-Vision" class="headerlink" title="Recursive Neural Network in Computer Vision"></a>Recursive Neural Network in Computer Vision</h4><p>递归神经网络在语义分割和特征提取上有很多的应用，但是在图像超分上还是没有应用。</p><hr><h3 id="Proposed-Method"><a href="#Proposed-Method" class="headerlink" title="Proposed Method"></a>Proposed Method</h3><h4 id="Basic-Model"><a href="#Basic-Model" class="headerlink" title="Basic Model"></a>Basic Model</h4><p>整个网络分三个部分，每个部分都只有一层隐层，只有inference 网络是递归的。<br>结构如下图：</p><blockquote><p><img src="http://sotavision.cn/showdoc/server/../Public/Uploads/2020-04-05/5e89e70041905.png" alt="" loading="lazy"></p></blockquote><p><strong>Embedding network</strong>:用于从RGB和灰度图像产生feature map，相当于特征提取,可以用一下表达式表示：<br>\( H_{-1}=\max (0,W_{-1} \ast \text{x}+b_{-1}) \)<br>\( H_{0}=\max (0,W_{0}\ast H_{-1}+b_{0}) \)<br>\( f_{1}(\text x)=H_0 \)<br>其中 \( f_1(\text{x}) \) 表示特征提取的输出。<br><strong>Inference network</strong>:解决超分辨率，相当于特征的非线性变换,其中 \( g(H)=\max(0,W \ast H+b) \) ，递归可以表示为：<br>\( H_d=g(H_{d-1})=\max(0,W \ast H_{d-1}+b) \),<br>故递归层输出为：\( f_2(H)=(g\circ g\circ g\circ \cdots \circ)g(H)=g^D(H) \)<br><strong>reconstruction network</strong>:用于把多个通道的转成三个通道的图片。重建层可以表达为：<br>$$H_{D+1}=\max(0,W_{D+1} \ast H_D+b_{D+1})$$<br>\( \hat{y}=\max(0,W_{D+2} \ast H_{D+1}+b_{D+2}) \)，\( f_3(H)=\hat{y} \)</p><p>这个模型的优点就是简单强大，缺点就是递归层数多的时候很难训练，会产生梯度爆炸。</p><h4 id="Advanced-Model"><a href="#Advanced-Model" class="headerlink" title="Advanced Model"></a>Advanced Model</h4><p>进阶网络结构如下：</p><blockquote><p><img src="http://sotavision.cn/showdoc/server/../Public/Uploads/2020-04-05/5e89ed8fe90af.png" alt="" loading="lazy"></p></blockquote><p><strong>Recursive-Supervision</strong>：<br>为了解决梯度爆炸和寻找最佳递归次数的问题，提出了一种进一步的模型，所以递归上增加一个监督。监督可以表示为: </p><div>\[ \hat{ y}_d = f_3(\text{x},g^{(d)}(f_1(\text x))) \]</div>，其中 <div>\[ f_3(\text x, H_d)=\text x+f_3(H_d) \]</div>，<p>然后最终输出可以表达为: \( \hat{y}= \displaystyle\sum_{d=1}^D {w_d \cdot \hat{\text y}_d} \)</p><p><strong>skip-connection</strong>:使用该技巧的原因是超分辨率重建，低分辨率图像和网络输出的高分辨率图像有很高的相关性，而随着递归网络深度加深学习不到这种线性映射，所以通过该技巧可以使用大部分低分辨率信息用于重建高分辨率图像。</p><p>其中的\( H_1 \)到是\( H_D \)个共享参数的卷积层。DRCN将每一层的卷积结果都通过同一个Reconstruction Net得到一个重建结果，从而共得到D个重建结果，再把它们加权平均得到最终的输出。另外，受到ResNet的启发，DRCN通过skip connection将输入图像与\( H_d \)的输出相加后再作为Reconstruction Net的输入，相当于使Inference Net去学习高分辨率图像与低分辨率图像的差，即恢复图像的高频部分。</p><h4 id="Loss-function"><a href="#Loss-function" class="headerlink" title="Loss function"></a>Loss function</h4><p>中间层的loss function为：</p><div>    \[ l_1(\theta)= \displaystyle\sum_{d=1}^D \displaystyle\sum_{i=1}^N{ \dfrac{1}{2DN}|| \text y^{(i)}-\hat{\text y}_d^{(i)}||^2} \]</div>最后一层的loss function为：<div>    \[ l_2(\theta)= \displaystyle\sum_{i=1}^N{ \dfrac{1}{2N}|| \text y^{(i)}- \displaystyle\sum_{d=1}^D w_d \cdot \hat{\text y}_d^{(i)}||^2} \]</div><p>最终的loss function为：\( L(\theta)=\alpha l_1(\theta)+(1-\alpha)l_2(\theta)+\beta||\theta||^2 \)其中\( \alpha \)和\( \beta \)为可以调参数。</p><hr><h3 id="Experiment"><a href="#Experiment" class="headerlink" title="Experiment"></a>Experiment</h3><h4 id="Experiments-Details"><a href="#Experiments-Details" class="headerlink" title="Experiments Details"></a>Experiments Details</h4><p>实现细节待复现</p><h4 id="Study-of-Deep-Recursions"><a href="#Study-of-Deep-Recursions" class="headerlink" title="Study of Deep Recursions"></a>Study of Deep Recursions</h4><p>作者发现recursion d越大，psnr越高</p><blockquote><p><img src="http://sotavision.cn/showdoc/server/../Public/Uploads/2020-04-05/5e89f8ca3719c.png" alt="" loading="lazy"></p></blockquote><p>并且发现如果不是recursion的unit全部连到final output，而是单一的某一层，效果会变差，证明了recursion ensemble的效果</p><blockquote><p><img src="http://sotavision.cn/showdoc/server/../Public/Uploads/2020-04-05/5e89f945719f7.png" alt="" loading="lazy"></p></blockquote><h4 id="Comparisons-with-state-of-the-arts"><a href="#Comparisons-with-state-of-the-arts" class="headerlink" title="Comparisons with state-of-the-arts"></a>Comparisons with state-of-the-arts</h4><blockquote><p><img src="http://sotavision.cn/showdoc/server/../Public/Uploads/2020-04-05/5e89f97d92e5b.png" alt="" loading="lazy"></p></blockquote><hr><h3 id="Conclusions"><a href="#Conclusions" class="headerlink" title="Conclusions"></a>Conclusions</h3><ol><li>作者提出了一种使用深度递归卷积网络的超分辨率方法，有效地共享权重参数</li><li>使用递归监督和跳跃连接解决梯度爆炸和最佳递归层数问题</li><li>作者的方法在当时超分问题取得了较好的效果</li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;https://cdn.jsdelivr.net/npm/aplayer@1.10/dist/APlayer.min.css&quot;&gt;&lt;script 
      
    
    </summary>
    
    
      <category term="论文笔记" scheme="http://alexzou14.github.io/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="深度学习" scheme="http://alexzou14.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="笔记" scheme="http://alexzou14.github.io/tags/%E7%AC%94%E8%AE%B0/"/>
    
      <category term="超分辨率" scheme="http://alexzou14.github.io/tags/%E8%B6%85%E5%88%86%E8%BE%A8%E7%8E%87/"/>
    
  </entry>
  
  <entry>
    <title>FSRNet：End-to-End Learning Face Super-Resolution with Facial Priors论文阅读笔记</title>
    <link href="http://alexzou14.github.io/2020/04/02/FSRNet/"/>
    <id>http://alexzou14.github.io/2020/04/02/FSRNet/</id>
    <published>2020-04-02T09:03:11.000Z</published>
    <updated>2020-04-02T09:34:32.089Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="https://cdn.jsdelivr.net/npm/aplayer@1.10/dist/APlayer.min.css"><script src="https://cdn.jsdelivr.net/npm/aplayer@1.10/dist/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="https://cdn.jsdelivr.net/npm/meting@1.2/dist/Meting.min.js"></script><h3 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h3><p>人脸超分辨率是图像超分辨率的一个特殊任务，特别的人脸先验信息可以更好的被利用，还原出更好的超清图像。作者发现，现有的方法在恢复非常低的超分辨率人脸图像存在问题，恢复不清晰的情况。因此提出了一个端到端的训练方式和充分利用人脸的几何先验知识来组成的一个网络。该网络先构建了一个粗SR网络来恢复出一个粗高分辨率图片，再将粗HR图像送去两个分支网络中，一个是细SR编码，另一个是先验信息评估，细SR编码用来提取特征图，先验信息评估用来评价估计解析图。再将得到的特征图和先验信息送入细SR编码还原HR图像。为了生成更真实的图像，作者还提出了一个FSRGAN将对抗损失引入FSRNet中。</p><hr><h3 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h3><p>面部超分辨率（SR）是特定的一类图像超分辨率问题。目前大多数人脸图像超分辨算法是由通用的图像超分辨算法加以适当修改得到的。之前的面部超分辨率方法大多是由通用的图像超分辨率算法加以适当修改得到的，大多数没有加人脸先验知识，所以导致恢复效果不好。并且用于人脸超分辨率的图像不是端到端的训练。这篇文章受到人脸几何先验和端到端的训练的启发，提出了一个端到端的深度可训练面部超分辨网络，充分利用人脸图像的几何先验信息，即面部landmark的heatmap和人脸解析图，来对低分辨率人脸图像进行超分辨率。<br>本文主要贡献有：</p><ul><li>第一个提出用人脸几何先验的知识进行端到端学习的人脸超分辨率方法。</li><li>同时引入了两种几何先验，face landmark 和面部解析</li><li>提出的FSRnet在模糊未对齐和非常低的分辨率的图像，通过8倍放大，是目前最好的水平。同时用FSRnetGAN网络可以进一步生成更加逼真的images。</li><li>对于人脸超分辨率，人脸对齐和面部解析 最为新的评价标准。进一步证明，该方法可以解决传统的视觉感知度量方法的不一致性。</li></ul><p>选择形状作为先验的两种考虑：首先，当分辨率从高到低时，形状比纹理保存得更好，因此更有可能被提取出来以提高超分辨率。形状的表示要比纹理的表示好一些 。人脸解析 是不同人脸组成的分割估计。</p><blockquote><p><img src="http://sotavision.cn/showdoc/server/../Public/Uploads/2020-03-26/5e7c114a3c0a8.png" alt="FSRNet1" loading="lazy"></p></blockquote><hr><h3 id="Related-work"><a href="#Related-work" class="headerlink" title="Related work"></a>Related work</h3><h4 id="Facial-Prior-Knowledge"><a href="#Facial-Prior-Knowledge" class="headerlink" title="Facial Prior Knowledge"></a>Facial Prior Knowledge</h4><p>有很多使用了人脸先验信息的人脸超分方法，都是为了更好的从低分辨率图像恢复成高分辨率图像。但是早期的方法在低放缩倍率下估计人脸先验是非常困难的。随着近期深度卷积网络在人脸超分上的引用成功， 宋等人提出了一种两阶段的方法，首先生成面部COMP由CNNs合成，然后通过成分增强方法合成细粒度的面部结构。与上述方法不同的是，我们的FSRNet完全LEV 擦除面部地标热图和解析地图的端到端培训方式。</p><h4 id="End-to-end-Training"><a href="#End-to-end-Training" class="headerlink" title="End-to-end Training"></a>End-to-end Training</h4><p>端到端的训练模式被广泛运用在一般的图像超分问题中，DRRN的提出有效的解决了网络参数数量和网络准确率的问题。与上述仅依靠深层模型力量的方法不同，我们的FSRNet不仅是一个端到端的可训练神经网络，而且结合了来自人脸先验的丰富信息，更好的将低分辨率人脸图像还原成高分辨率图像。</p><hr><h3 id="Proposed-Method"><a href="#Proposed-Method" class="headerlink" title="Proposed Method"></a>Proposed Method</h3><h4 id="Network-structure"><a href="#Network-structure" class="headerlink" title="Network structure"></a>Network structure</h4><p>网络结构如下图：</p><blockquote><p><img src="http://sotavision.cn/showdoc/server/../Public/Uploads/2020-03-26/5e7c16555126e.png" alt="FSRNet2" loading="lazy"></p></blockquote><p>FSRNet是由四个部分组成：coarse SR network, fine SR encoder, prior estimation network和fine SR decoder</p><p>粗SR网络将输入的LR图像初步还原成一个粗SR图像,这个粗SR网络可以表达成：$$y_c=C(\text{x})$$<br><strong>先验信息估计网络</strong>：从最近成功的叠加热图回归在人体姿势估计中受到启发，文章提出在先验信息估计网络中使用一个 HourGlass 结构来估计面部landmark 的 heatmap 和解析图。因为这两个先验信息都可以表示 2D的人脸形状，所以在先验信息估计网络中，特征在两个任务之间是共享的， 除了最后一层。为了有效整合各种尺度的特征并保留不同尺度的空间信息，HourGlass block 在对称层之间使用 skip-connection 机制。最后，共享的 HG 特征连接到两个分离的 1×1 卷积层来生成 landmark heatmap和解析图。<br><strong>精细的SR编码器</strong>：受到 ResNet 在超分辨任务中的成功的启发，文章使用 residual block 进行特征提取。考虑到计算的开销，先验信息的特征会降采样到 64×64。为了使得特征尺寸一致，编码器首先经过一个 3×3，stride为 2 的卷积层来把特征图降采样到 64×64。然后再使用 ResNet 结构提取图像特征。</p><p>对应的先验评估网络P和细SR编码网络F分别可以表示成：<br>$$\text{p}=P(\text{y}_c)$$,$$\text{f}=F(\text{y}_c)$$<br><strong>精细的SR解码器</strong>：解码器把先验信息和图像特征组合为输入，首先将先验特征 p 和图像特征 f 进行concatenate，作为输入。然后通过 3×3 的卷积层把特征图的通道数减少为 64。然后一个 4×4 的反卷积层被用来把特征图的 size 上采样到 128×128。然后使用 3 个 residual block 来对特征进行解码。最后的 3×3 卷积层被用来得到最终的 HR 图像。</p><p>将coarse SRimages送入特征提取和先验估计网络中:<br>$$\text{y}=D(\text{f},\text{p})$$</p><h4 id="Loss-function"><a href="#Loss-function" class="headerlink" title="Loss function"></a>Loss function</h4><p>给定训练集：\( \{\text{x}^{(i)},\tilde{\text{y}}^{(i)},\tilde{\text{p}}^{(i)}\}_{i=1}^N \),FSRNet的损失函数为( \( \tilde{\text{y}},\tilde{\text{p}} \)为ground truth):</p><p>$$L_F(\Theta)=\dfrac{1}{2N}\sum_{i=1}^N { {\left|{\tilde{\text{y}}^{(i)}-\text{y}_c^{(i)}}\right|^2+\left|{\tilde{\text{y}}^{(i)}-\text{y}^{(i)}}\right|^2+\lambda\left|{\tilde{\text{p}}^{(i)}-\text{p}^{(i)}}\right|^2} } $$</p><h4 id="FSRGAN"><a href="#FSRGAN" class="headerlink" title="FSRGAN"></a>FSRGAN</h4><p>FSRGAN网络结构如下图：</p><blockquote><p><img src="http://sotavision.cn/showdoc/server/../Public/Uploads/2020-03-26/5e7c1ea4770b2.png" alt="FSRNet3" loading="lazy"></p></blockquote><p>GAN LOSS为：$$L_C(\text{F,C})=\mathbb{E}[\log\text{C}(\tilde{\text{y}},\text{x})]+\mathbb{E}[\log(1-\text{C}({\text{F(x)}},\text{x}))]$$<br>感知损失为：$$L_P=\left|{\phi(\text{y})-\phi(\tilde\text{y})}\right|^2$$</p><hr><h3 id="Experiment"><a href="#Experiment" class="headerlink" title="Experiment"></a>Experiment</h3><h4 id="Implementation-Details"><a href="#Implementation-Details" class="headerlink" title="Implementation Details"></a>Implementation Details</h4><p>实验详情可以参考代码：<a href="https://github.com/cydiachen/FSRNET_pytorch" target="_blank" rel="noopener">https://github.com/cydiachen/FSRNET_pytorch</a></p><h4 id="Prior-Knowledge-for-Face-Super-Resolution"><a href="#Prior-Knowledge-for-Face-Super-Resolution" class="headerlink" title="Prior Knowledge for Face Super-Resolution"></a>Prior Knowledge for Face Super-Resolution</h4><blockquote><p><img src="http://sotavision.cn/showdoc/server/../Public/Uploads/2020-03-26/5e7c20908f199.png" alt="FSRNet4" loading="lazy"><br><img src="http://sotavision.cn/showdoc/server/../Public/Uploads/2020-03-26/5e7c20a18f698.png" alt="FSRNet5" loading="lazy"></p></blockquote><p>把先验信息估计网络移除以后，构建了一个 Baseline 网络。基于 Baseline 网络，引入 ground truth 人脸先验信息（landmark heatmap 和解析图）到拼接层，得到一个新的网络。<br>结论：</p><ul><li>解析图比 landmark heatmap 含有更多人脸图像超分辨的信息，带来的提升更大；</li><li>全局的解析图比局部的解析图更有用；</li><li>landmark 数量增加所带来的提升很小</li></ul><h4 id="Network-analysis"><a href="#Network-analysis" class="headerlink" title="Network analysis"></a>Network analysis</h4><h5 id="Effects-of-Estimated-Priors"><a href="#Effects-of-Estimated-Priors" class="headerlink" title="Effects of Estimated Priors"></a>Effects of Estimated Priors</h5><blockquote><p><img src="http://sotavision.cn/showdoc/server/../Public/Uploads/2020-03-26/5e7c2210976f2.png" alt="FSRNet6" loading="lazy"></p></blockquote><p>Baseline_v1：完全不包含先验信息<br>Baseline_v2：包含先验信息，但不进行监督训练<br>结论：</p><ol><li>即使不进行监督训练，先验信息也能帮助到SR任务，可能是因为先验信息提供了更多的高频信息。</li><li>越多先验信息，越好。</li><li>最佳性能为25.85dB，但是使用ground truth信息时，能达到26.55dB。说明估计得到的先验信息并不完美，更好的先验信息估计网络可能会得到更好的结果。</li></ol><h5 id="Effects-of-Hourglass-Numbers"><a href="#Effects-of-Hourglass-Numbers" class="headerlink" title="Effects of Hourglass Numbers"></a>Effects of Hourglass Numbers</h5><p>强大的先验信息预测网络会得到更好的结果，所以探究Hourglass数量h对网络性能的影响。分别取1，2，4，结果为25.69，25.87，25.95。<br>不同的Hourglass数量对landmark估计的影响：</p><blockquote><p><img src="http://sotavision.cn/showdoc/server/../Public/Uploads/2020-03-26/5e7c2276370ed.png" alt="FSRNet7" loading="lazy"></p></blockquote><p>可以看到 h 数量增加时，先验信息估计网络结构越深，学习能力越强，性能越好。</p><h4 id="Comparisons-with-state-of-the-arts"><a href="#Comparisons-with-state-of-the-arts" class="headerlink" title="Comparisons with state-of-the-arts"></a>Comparisons with state-of-the-arts</h4><blockquote><p><img src="http://sotavision.cn/showdoc/server/../Public/Uploads/2020-03-26/5e7c22b5ad9fa.png" alt="FSRNet8" loading="lazy"></p></blockquote><p>放大8倍后的性能比较，虽然FSRGAN的两项指标（PSNR/SSIM）都不如FSRNet，但是从视觉效果上看更加真实。这也与目前的一个共识相对应：基于生成对抗网络的模型可以恢复视觉上合理的图像，但是在一些指标上（PSNR , SSIM）的值会低。而基于MSE的深度模型会生成平滑的图像，但是有高的PSNR/SSIIM。</p><hr><h3 id="Conclusions"><a href="#Conclusions" class="headerlink" title="Conclusions"></a>Conclusions</h3><ol><li>本文提出了深度端到端的可训练的人脸超分辨网络FSRNet</li><li>FSRNet的关键在于先验信息估计网络，这个网络不仅有助于改善PSNR/SSIM，还提供从非常低分辨率的图像精确估计几何先验信息（landmark heatmap和解析图）的解决方案。</li><li>实验结果表明FSRNet比当前的SOTA的方法要更好，即使在未对齐的人脸图像上。</li></ol><p>未来的工作可以有以下几个方面：</p><ol><li>设计一个更好的先验信息估计网络。</li><li>迭代地学习精细的SR网络。</li><li>调研其他有用的脸部先验信息。 </li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;https://cdn.jsdelivr.net/npm/aplayer@1.10/dist/APlayer.min.css&quot;&gt;&lt;script 
      
    
    </summary>
    
    
      <category term="论文笔记" scheme="http://alexzou14.github.io/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="深度学习" scheme="http://alexzou14.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="笔记" scheme="http://alexzou14.github.io/tags/%E7%AC%94%E8%AE%B0/"/>
    
      <category term="超分辨率" scheme="http://alexzou14.github.io/tags/%E8%B6%85%E5%88%86%E8%BE%A8%E7%8E%87/"/>
    
  </entry>
  
</feed>
