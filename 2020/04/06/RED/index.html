<!DOCTYPE html><html lang="zh-CN"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0"><meta name="description" content="Image Restoration Using Convolutional Auto-encoders with Symmetric Skip Connections论文阅读笔记"><meta name="keywords" content="深度学习,笔记,超分辨率"><meta name="author" content="秩同道合"><meta name="copyright" content="秩同道合"><meta name="theme-color" content="#0078E7"><title>Image Restoration Using Convolutional Auto-encoders with Symmetric Skip Connections论文阅读笔记 | 秩同道合的小站</title><link rel="shortcut icon" href="https://cdn.jsdelivr.net/gh/AlexZou14/CDN/img/favicon.ico"><link rel="preload" href="/css/hexo-theme-yun.css" as="style"><link rel="preload" href="/js/utils.js" as="script"><link rel="preload" href="/js/hexo-theme-yun.js" as="script"><link rel="prefetch" href="/js/sidebar.js" as="script"><link rel="preconnect" href="https://cdn.jsdelivr.net" crossorigin><link rel="stylesheet" href="/css/hexo-theme-yun.css"><link rel="alternate" href="/atom.xml" title="秩同道合的小站"><script id="yun-config">
    let Yun = window.Yun || {};
    let CONFIG = {"root":"/","title":"秩同道合的小站","version":"0.3.1","anonymous_image":"https://cdn.jsdelivr.net/gh/YunYouJun/cdn/img/avatar/none.jpg","say":{"api":"https://v1.hitokoto.cn","hitokoto":true},"valine":{"el":"#valine-container","verify":false,"notify":false,"appId":"eCgP91hRSX8OtvCIR4MgLfcl-gzGzoHsz","appKey":"N5gVT8kUx5O0wMvc47SU040Y","serverURLs":null,"placeholder":"大佬求指教&nbsp_(:з」∠)_ （填写邮箱可以收到回复通知～）","avatar":null,"meta":["nick","mail","link"],"pageSize":10,"lang":"zh-cn","visitor":true},"fireworks":{"colors":["102, 167, 221","62, 131, 225","33, 78, 194"]}};
  </script><script src="//at.alicdn.com/t/font_1140697_pa6cswvjpq.js" async></script><meta name="generator" content="Hexo 4.2.0"><link rel="alternate" href="/atom.xml" title="秩同道合的小站" type="application/atom+xml">
</head><body><script defer src="https://cdn.jsdelivr.net/npm/animejs@latest/anime.min.js"></script><script defer src="/js/ui/fireworks.js"></script><canvas class="fireworks"></canvas><div class="container"><a class="sidebar-toggle sidebar-toggle-fixed hty-icon-button"><div class="hamburger hamburger--spin" type="button"><span class="hamburger-box"><span class="hamburger-inner"></span></span></div></a><aside class="sidebar"><ul class="sidebar-nav"><li class="sidebar-nav-item sidebar-nav-toc sidebar-nav-active hty-icon-button" data-target="post-toc-wrap" title="文章目录"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-list-ordered"></use></svg></li><li class="sidebar-nav-item sidebar-nav-overview hty-icon-button" data-target="site-overview-wrap" title="站点概览"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-passport-line"></use></svg></li></ul><div class="sidebar-panel" id="site-overview-wrap"><div class="site-info fix-top"><a class="site-author-avatar" href="/about" title="秩同道合"><img loading="lazy" src="https://cdn.jsdelivr.net/gh/AlexZou14/CDN/img/touxiang.jpg" alt="秩同道合"></a><div class="site-author-name"><a href="/about">秩同道合</a></div><a class="site-name" href="/about/site.html">秩同道合的小站</a><sub class="site-subtitle">寻找志趣相投的伙伴！</sub><div class="site-desciption">我和你，以及我们的秩相同所以我们才等价！</div></div><nav class="site-state"><a class="site-state-item hty-icon-button icon-home" href="/" title="首页"><span class="site-state-item-icon"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-home-4-line"></use></svg></span></a><div class="site-state-item site-state-posts"><a href="/archives" title="归档"><span class="site-state-item-icon"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-archive-line"></use></svg></span><span class="site-state-item-count">25</span></a></div><div class="site-state-item site-state-categories"><a href="/categories" title="分类"><span class="site-state-item-icon"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-folder-2-line"></use></svg></span><span class="site-state-item-count">1</span></a></div><div class="site-state-item site-state-tags"><a href="/tags" title="标签"><span class="site-state-item-icon"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-price-tag-3-line"></use></svg></span><span class="site-state-item-count">3</span></a></div><a class="site-state-item hty-icon-button" href="https://github.com/AlexZou14" target="_blank" rel="noopener" title="reward.comment"><span class="site-state-item-icon"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-hand-coin-line"></use></svg></span></a></nav><hr style="margin-bottom:0.5rem"><div class="links-of-author"><a class="links-of-author-item hty-icon-button" rel="noopener" href="/atom.xml" title="RSS" target="_blank" style="color:orange"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-rss-line"></use></svg></a><a class="links-of-author-item hty-icon-button" rel="noopener" href="https://github.com/AlexZou14" title="GitHub" target="_blank" style="color:#181717"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-github-line"></use></svg></a><a class="links-of-author-item hty-icon-button" rel="noopener" href="mailto:1120375574@qq.com" title="E-Mail" target="_blank" style="color:#8E71C1"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-mail-line"></use></svg></a><a class="links-of-author-item hty-icon-button" rel="noopener" href="https://space.bilibili.com/19164044" title="哔哩哔哩" target="_blank" style="color:#FF8EB3"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-bilibili-line"></use></svg></a></div><hr style="margin:0.5rem 1rem"><div class="links"><a class="links-item hty-icon-button" href="http://sotavision.cn" target="_blank" rel="noopener" title="工作组" style="color:#000000"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-send-plane-2-line"></use></svg></a></div></div><script defer src="/js/sidebar.js"></script><div class="sidebar-panel sidebar-panel-active" id="post-toc-wrap"><div class="post-toc"><div class="post-toc-progress"><div class="progress-bar"></div><div class="progress-info"><span class="progress-notice">您已经读了</span><span class="progress-num">0</span><span class="progress-percentage">%</span></div></div><div class="post-toc-content"><ol class="toc"><li class="toc-item toc-level-3"><a class="toc-link" href="#Abstract"><span class="toc-number">1.</span> <span class="toc-text">Abstract</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Introduction"><span class="toc-number">2.</span> <span class="toc-text">Introduction</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Related-work"><span class="toc-number">3.</span> <span class="toc-text">Related work</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Proposed-Method"><span class="toc-number">4.</span> <span class="toc-text">Proposed Method</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Network-structure"><span class="toc-number">4.1.</span> <span class="toc-text">Network structure</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Deconvolution-decoder"><span class="toc-number">4.2.</span> <span class="toc-text">Deconvolution decoder</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Skip-connections"><span class="toc-number">4.3.</span> <span class="toc-text">Skip connections</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Training"><span class="toc-number">4.4.</span> <span class="toc-text">Training</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Experiment"><span class="toc-number">5.</span> <span class="toc-text">Experiment</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Experiments-Details"><span class="toc-number">5.1.</span> <span class="toc-text">Experiments Details</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Network-analysis"><span class="toc-number">5.2.</span> <span class="toc-text">Network analysis</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Comparisons-with-state-of-the-arts"><span class="toc-number">5.3.</span> <span class="toc-text">Comparisons with state-of-the-arts</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Conclusions"><span class="toc-number">6.</span> <span class="toc-text">Conclusions</span></a></li></ol></div></div></div></aside><main class="sidebar-translate" id="content"><div id="post"><article class="post-block" itemscope itemtype="http://schema.org/Article"><link itemprop="mainEntityOfPage" href="http://alexzou14.github.io/2020/04/06/RED/"><span hidden itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="name" content="秩同道合"><meta itemprop="description" content="Image Restoration Using Convolutional Auto-encoders with Symmetric Skip Connections论文阅读笔记"></span><span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="秩同道合的小站"></span><header class="post-header"><h1 class="post-title" itemprop="name headline">Image Restoration Using Convolutional Auto-encoders with Symmetric Skip Connections论文阅读笔记</h1><div class="post-meta"><span class="post-time"><span class="post-meta-item-icon"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-calendar-line"></use></svg></span> <time title="创建时间：2020-04-06 17:31:55" itemprop="dateCreated datePublished" datetime="2020-04-06T17:31:55+08:00">2020-04-06</time></span><div class="post-count"><span class="post-symbolcount"><span class="post-meta-item-icon" title="本文字数"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-file-word-line"></use></svg></span> <span title="本文字数">6.9k</span><span class="post-meta-divider">-</span><span class="post-meta-item-icon" title="阅读时长"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-timer-line"></use></svg></span> <span title="阅读时长">14 分钟</span></span></div><span class="leancloud_visitors" id="/2020/04/06/RED/" data-flag-title="Image Restoration Using Convolutional Auto-encoders with Symmetric Skip Connections论文阅读笔记"><span class="post-meta-item-icon" title="阅读次数"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-eye-line"></use></svg> <span class="leancloud-visitors-count"></span></span></span><div class="post-classify"><span class="post-category"><span class="post-meta-item-icon"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-folder-line"></use></svg></span> <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a class="category" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/" itemprop="url" rel="index"><span itemprop="text">论文笔记</span></a></span></span><span class="post-tag"><span class="post-meta-divider">-</span><a class="tag" href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"><span class="post-meta-item-icon"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-price-tag-3-line"></use></svg></span><span class="tag-name">深度学习</span></a><a class="tag" href="/tags/%E7%AC%94%E8%AE%B0/"><span class="post-meta-item-icon"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-price-tag-3-line"></use></svg></span><span class="tag-name">笔记</span></a><a class="tag" href="/tags/%E8%B6%85%E5%88%86%E8%BE%A8%E7%8E%87/"><span class="post-meta-item-icon"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-price-tag-3-line"></use></svg></span><span class="tag-name">超分辨率</span></a></span></div></div></header><section class="post-body" itemprop="articleBody"><div class="post-content post-markdown"><h3 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h3><p>本文提出了一种非常深的深卷积自动编码网络。因为卷积层在消除图像失真的同时捕获了图像内容的抽象细节，反卷积层具有向上采样特征图和恢复图像细节的能力，所以这个网络有很多的卷积和反卷积层构成，学习了从LR图像到HR图像端到端的映射。由于更深的网络很难训练存在梯度爆炸的问题，所以本文采用了将卷积层和反卷积层使用跳跃连接。卷积层和反卷积层的跳跃连接有两个优点:</p>
<ol>
<li>这种设计允许信号直接反向传播到底层，从而解决了梯度爆炸的问题，使得训练深度网络更加容易。提高了性能</li>
<li>这样的设计有利于恢复更干净的图像。</li>
</ol>
<p>同时这个模型可以用于图像去噪，去除JPEG压缩伪影和图像嵌入等问题。</p>
<hr>
<h3 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h3><p>图像恢复的任务是从其损坏的观察中恢复干净的图像，这已知是一种不适定的逆问题。 通过适应不同类型的腐败分布，相同的数学模型适用于图像去噪和超分辨率等问题。当时，深度神经网络已经在图像处理和计算机视觉任务中表现出优越的性能，从高级识别， 语义分割到低级去噪，超分辨率，去模糊，修复和恢复来自压缩图像的原始图像。 尽管深度神经网络取得了进展，但一些研究问题仍有待解决。 例如，更深层次的网络通常可以实现更好的性能吗？ 可以设计一个能够处理不同级别降置的深度模型吗？<br>观察DNN最近在图像处理任务上的优越性能，我们提出了一种基于卷积神经网络（CNN）的图像恢复框架。我们观察到，为了获得良好的恢复性能，训练非常深的模型是有益的。同时，我们表明，由于大容量网络的优势，在处理多个不同级别的损坏时，单个网络可以实现非常有前途的性能。具体而言，所提出的框架学习从损坏的图像到干净的图像的端到端完全卷积映射。该网络由多层卷积和反卷积运算符组成。由于更深层次的网络往往更难以训练，我们建议将卷积层和反卷积层对称地连接到跳层连接，训练过程收敛得更快，更有可能获得高质量的局部最优。我们的主要贡献总结如下。</p>
<ol>
<li>提出了一个非常深入的图像恢复网络结构，卷积层作为特征处理编码，反卷积层解码恢复图像。</li>
<li>提出了在卷积层与反卷积层之间增加跳跃连接层，跳跃连接有助于将梯度反向传播到底层，并将图像细节传播到顶层。</li>
<li>将这个网络应用在其他各种应用场景中取得了良好的恢复性能。</li>
</ol>
<hr>
<h3 id="Related-work"><a href="#Related-work" class="headerlink" title="Related work"></a>Related work</h3><p>关工作文献中的图像修复已经做了大量工作。诸如Total variation，BM3D算法和基于字典学习的方法等传统方法在图像去噪和超分辨率等图像恢复主题上表现出非常好的性能。由于图像恢复通常是一个不适定的问题，正规化的使用已被证明是必不可少的。堆积去噪自动编码器是最着名的DNN模型之一，可用于图像恢复。组合稀疏编码和DNN预训练与去噪自动编码器，用于低级视觉任务，如图像去噪和修复。其他基于神经网络的方法，如多层感知器和CNN用于图像去噪，以及DNN用于图像或视频超分辨率和压缩伪影减少这些年来一直在积极研究。Burger等人提出了一种基于补丁的算法，用简单的多层感知器学习。他们还得出结论，对于大型网络，大型训练数据，神经网络可以实现最先进的图像去噪性能。 Jain和Seung 提出了一个完全卷积的CNN用于去噪。他们发现CNN提供与小波和马尔可夫随机场（MRF）方法相当甚至更优越的性能。崔等人在多尺度上对输入图像采用非局部自相似（NLSS）搜索，然后以逐层方式使用协同局部自编码器进行超分辨率。董超老师等人提出直接学习低/高分辨率图像之间的端到端映射。王等人认为，传统稀疏编码所代表的领域专业知识可以结合起来，以实现进一步改进的结果。 DNN方法的一个优点是这些方法纯粹是数据驱动的，并且没有关于噪声分布的假设。</p>
<hr>
<h3 id="Proposed-Method"><a href="#Proposed-Method" class="headerlink" title="Proposed Method"></a>Proposed Method</h3><h4 id="Network-structure"><a href="#Network-structure" class="headerlink" title="Network structure"></a>Network structure</h4><p>网络整体结构，如下图：</p>
<blockquote>
<p><img src="http://sotavision.cn/showdoc/server/../Public/Uploads/2020-04-06/5e8ae477ba16d.png" alt="" loading="lazy"></p>
</blockquote>
<p>该框架完全是卷积和反卷积的。在每次卷积和反卷积之后添加校正层。卷积层充当特征提取器，它保留图像中对象的主要组件，同时消除损坏。然后组合去卷积层以恢复图像内容的细节。反卷积层的输出是输入图像的“干净”版本。此外，跳过连接也从卷积层添加到其对应的镜像反卷积层。<br>卷积特征映射以元素方式传递给解卷积特征映射并与解卷积特征映射求和，并在校正后传递到下一层。对于低级别的图像恢复问题，我们更喜欢在网络中不使用池化或解除池化，因为通常池化会丢弃对这些任务至关重要的有用图像细节。</p>
<h4 id="Deconvolution-decoder"><a href="#Deconvolution-decoder" class="headerlink" title="Deconvolution decoder"></a>Deconvolution decoder</h4><p>卷积：特征提取，随卷积进行，图像特征被提取，同时噪声的效果被降低，经过多层卷积后，图像的特征被提取出来，也降低了噪声的影响。</p>
<p>反卷积：针对特征的上采样，完成由图像特征到图像的转换，由于利用的是过滤后的噪声后的图像特征，因此达到了降噪、图像修复的目的。</p>
<p>文中通过实验说明利用反卷积结构而不用padding 和upsampling的原因，如下图，反卷积对图像细节有补偿作用。</p>
<blockquote>
<p><img src="http://sotavision.cn/showdoc/server/../Public/Uploads/2020-04-06/5e8af334c374c.png" alt="" loading="lazy"></p>
</blockquote>
<h4 id="Skip-connections"><a href="#Skip-connections" class="headerlink" title="Skip connections"></a>Skip connections</h4><p>作用：</p>
<p>1）保留更多的图像细节，协助反卷积层完成图像的恢复工作；</p>
<p>2）反向传播过程中的梯度反向，减少梯度消失，加快模型训练，文章有一些公式推导说明跳层连接对梯度变化的好处，详见论文；</p>
<p>利用图像修复实验说明跳层作用：</p>
<blockquote>
<p><img src="http://sotavision.cn/showdoc/server/../Public/Uploads/2020-04-06/5e8af364b2498.png" alt="" loading="lazy"><br><img src="http://sotavision.cn/showdoc/server/../Public/Uploads/2020-04-06/5e8af36f42064.png" alt="" loading="lazy"></p>
</blockquote>
<h4 id="Training"><a href="#Training" class="headerlink" title="Training"></a>Training</h4><p>局部模块结构：</p>
<blockquote>
<p><img src="http://sotavision.cn/showdoc/server/../Public/Uploads/2020-04-06/5e8af2f3cbf08.png" alt="" loading="lazy"></p>
</blockquote>
<p>卷积和反卷积层可以表示为：\( F(X)=\max(0,W_k \ast X+B_k) \)其中\( * \)代表卷积和反卷积操作。<br>ReLU的激活函数可以表达为：\( F(X_1,X_2)=\max(0,X_1+X_2) \)</p>
<p>使用的loss function为MSE：\( L(\Theta)=\dfrac{1}{N}\displaystyle\sum_{i=1}^N||F(X^i;\Theta)-Y^i||_F^2 \)<br>使用了SGD的方法来进行训练。</p>
<hr>
<h3 id="Experiment"><a href="#Experiment" class="headerlink" title="Experiment"></a>Experiment</h3><h4 id="Experiments-Details"><a href="#Experiments-Details" class="headerlink" title="Experiments Details"></a>Experiments Details</h4><p>参考code: <a href="https://github.com/SSinyu/RED_CNN" target="_blank" rel="noopener">https://github.com/SSinyu/RED_CNN</a></p>
<h4 id="Network-analysis"><a href="#Network-analysis" class="headerlink" title="Network analysis"></a>Network analysis</h4><p><strong>filter number</strong>:<br>滤波器大小为3x3,patch大小为50x50,跳层层数为2，不同的滤波个数，32,64,128，结果如下：</p>
<blockquote>
<p><img src="http://sotavision.cn/showdoc/server/../Public/Uploads/2020-04-06/5e8af5a0db0b2.png" alt="" loading="lazy"></p>
</blockquote>
<p><strong>filter size</strong>:<br>滤波器个数为64,patch大小为50x50,跳层层数为2，不同的滤波器大小为，3x3,5x5,7x7,9x9，结果如下：</p>
<blockquote>
<p><img src="http://sotavision.cn/showdoc/server/../Public/Uploads/2020-04-06/5e8af5d533455.png" alt="" loading="lazy"></p>
</blockquote>
<p><strong>training patch size</strong>:<br>滤波器个数为64,滤波器大小为3x3 ,跳层层数为2，不同的patch size，25x25, 50x50,75x75,100x100结果如下：</p>
<blockquote>
<p><img src="http://sotavision.cn/showdoc/server/../Public/Uploads/2020-04-06/5e8af618ac327.png" alt="" loading="lazy"></p>
</blockquote>
<p><strong>step size pf skip connections</strong>:<br>滤波器个数为64,滤波器大小为3x3 ,跳层层数为2,4,7，不同的patch size为50x50结果如下：</p>
<blockquote>
<p><img src="http://sotavision.cn/showdoc/server/../Public/Uploads/2020-04-06/5e8af63939f59.png" alt="" loading="lazy"></p>
</blockquote>
<p>结论：滤波器个数越多，大小越大，训练图像越大，跳层越少性能越好；</p>
<h4 id="Comparisons-with-state-of-the-arts"><a href="#Comparisons-with-state-of-the-arts" class="headerlink" title="Comparisons with state-of-the-arts"></a>Comparisons with state-of-the-arts</h4><p>文章中进行了图像去噪、超解像、JPEG deblocking、Non-blind deblurring、图像修复实验，列举一下超解像相关结果：</p>
<blockquote>
<p><img src="http://sotavision.cn/showdoc/server/../Public/Uploads/2020-04-06/5e8af66598c25.png" alt="" loading="lazy"></p>
</blockquote>
<p>与VDSR和DRCN进行了比较：</p>
<blockquote>
<p><img src="http://sotavision.cn/showdoc/server/../Public/Uploads/2020-04-06/5e8af686692bb.png" alt="" loading="lazy"></p>
</blockquote>
<hr>
<h3 id="Conclusions"><a href="#Conclusions" class="headerlink" title="Conclusions"></a>Conclusions</h3><ol>
<li>作者提出了一个非常深入的图像恢复网络结构，卷积层作为特征处理编码，反卷积层解码恢复图像。</li>
<li>作者提出了在卷积层与反卷积层之间增加跳跃连接层，跳跃连接有助于将梯度反向传播到底层，并将图像细节传播到顶层。</li>
<li>将这个网络应用在其他各种应用场景中取得了良好的恢复性能。</li>
</ol>
</div><ul class="post-copyright"><li class="post-copyright-author"><strong>本文作者：</strong>秩同道合</li><li class="post-copyright-link"><strong>本文链接：</strong><a href="http://alexzou14.github.io/2020/04/06/RED/" title="Image Restoration Using Convolutional Auto-encoders with Symmetric Skip Connections论文阅读笔记">http://alexzou14.github.io/2020/04/06/RED/</a></li><li class="post-copyright-license"><strong>版权声明：</strong>本博客所有文章除特别声明外，均默认采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh" target="_blank" rel="noopener" title="CC BY-NC-SA 4.0 "><svg class="icon"><use xlink:href="#icon-creative-commons-line"></use></svg><svg class="icon"><use xlink:href="#icon-creative-commons-by-line"></use></svg><svg class="icon"><use xlink:href="#icon-creative-commons-nc-line"></use></svg><svg class="icon"><use xlink:href="#icon-creative-commons-sa-line"></use></svg></a> 许可协议。</li></ul></section></article><div class="post-nav"><div class="post-nav-item"><a class="post-nav-prev" href="/2020/04/06/EDSR/" rel="prev" title="Enhanced Deep Residual Networks for Single Image Super-Resolution论文阅读笔记"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-arrow-left-s-line"></use></svg><span class="post-nav-text">Enhanced Deep Residual Networks for Single Image Super-Resolution论文阅读笔记</span></a></div><div class="post-nav-item"><a class="post-nav-next" href="/2020/04/05/DRCN/" rel="next" title="Deeply-Recursive Convolutional Network for Image Super-Resolution论文阅读笔记"><span class="post-nav-text">Deeply-Recursive Convolutional Network for Image Super-Resolution论文阅读笔记</span><svg class="icon" aria-hidden="true"><use xlink:href="#icon-arrow-right-s-line"></use></svg></a></div></div><div id="comment"><div class="comment-tooltip text-center"><span>若您无 GitHub 账号，可直接在下方匿名评论。</span><br><span>若您想及时得到回复提醒，建议跳转 GitHub Issues 评论。</span><br><span>若没有本文 Issue，您可以使用 Comment 模版新建。</span><br><a class="hty-button hty-button--raised" id="github-issues" href="https://github.com/AlexZou14/alexzou14.github.io/issues?q=is:issue+Image Restoration Using Convolutional Auto-encoders with Symmetric Skip Connections论文阅读笔记" target="_blank" rel="noopener">GitHub Issues</a></div><div class="comment-container" id="valine-container"></div></div><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.css"><script defer src="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.js"></script><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex@latest/dist/contrib/copy-tex.min.css"><script defer src="https://cdn.jsdelivr.net/npm/katex@latest/dist/contrib/copy-tex.min.js"></script><script defer src="https://cdn.jsdelivr.net/npm/katex@latest/dist/contrib/auto-render.min.js" onload="renderMathInElement(document.body);"></script></div></main><footer class="sidebar-translate" id="footer"><div class="copyright"><span>&copy; 2019 – 2021 </span><span class="with-love" id="animate"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-cloud-line"></use></svg></span><span class="author"> 秩同道合</span></div><div class="powered"><span>由 <a href="https://hexo.io" target="_blank" rel="noopener">Hexo</a> 驱动 v4.2.0</span><span class="footer-separator">|</span><span>主题 - <a rel="noopener" href="https://github.com/YunYouJun/hexo-theme-yun" target="_blank"><span>Yun</span></a> v0.3.1</span></div><script defer src="https://cdn.jsdelivr.net/npm/valine@latest/dist/Valine.min.js"></script><script>function initValine() {
  new Valine(CONFIG.valine);
}
document.addEventListener("DOMContentLoaded", function() {
  initValine();
});</script></footer><a class="hty-icon-button" id="goUp" aria-label="back-to-top" href="#"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-arrow-up-s-line"></use></svg></a></div><script defer src="/js/utils.js"></script><script defer src="/js/hexo-theme-yun.js"></script><script src="https://cdn.jsdelivr.net/npm/live2d-widget@^3.1.3/lib/L2Dwidget.min.js"></script><script>L2Dwidget.init({"pluginRootPath":"live2dw/","pluginJsPath":"lib/","pluginModelPath":"assets/","tagMode":false,"debug":false,"model":{"jsonPath":"https://cdn.jsdelivr.net/npm/live2d-widget-model-hijiki@1.0.5/assets/hijiki.model.json"},"display":{"position":"right","width":200,"height":400},"mobile":{"show":true},"react":{"opacity":0.8},"dialog":{"enable":true,"hitokoto":true},"log":false});</script></body></html>