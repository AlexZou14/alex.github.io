<!DOCTYPE html><html lang="zh-CN"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0"><meta name="description" content="EnhanceNet：Single ImageSuper-Resolution through Automated Texture Synthesis论文阅读笔记"><meta name="keywords" content="深度学习,笔记,超分辨率"><meta name="author" content="秩同道合"><meta name="copyright" content="秩同道合"><meta name="theme-color" content="#0078E7"><title>EnhanceNet：Single ImageSuper-Resolution through Automated Texture Synthesis论文阅读笔记 | 秩同道合的小站</title><link rel="shortcut icon" href="https://cdn.jsdelivr.net/gh/AlexZou14/CDN/img/favicon.ico"><link rel="preload" href="/css/hexo-theme-yun.css" as="style"><link rel="preload" href="/js/utils.js" as="script"><link rel="preload" href="/js/hexo-theme-yun.js" as="script"><link rel="prefetch" href="/js/sidebar.js" as="script"><link rel="preconnect" href="https://cdn.jsdelivr.net" crossorigin><link rel="stylesheet" href="/css/hexo-theme-yun.css"><link rel="alternate" href="/atom.xml" title="秩同道合的小站"><script id="yun-config">
    let Yun = window.Yun || {};
    let CONFIG = {"root":"/","title":"秩同道合的小站","version":"0.3.1","anonymous_image":"https://cdn.jsdelivr.net/gh/YunYouJun/cdn/img/avatar/none.jpg","say":{"api":"https://v1.hitokoto.cn","hitokoto":true},"valine":{"el":"#valine-container","verify":false,"notify":false,"appId":"eCgP91hRSX8OtvCIR4MgLfcl-gzGzoHsz","appKey":"N5gVT8kUx5O0wMvc47SU040Y","serverURLs":null,"placeholder":"大佬求指教&nbsp_(:з」∠)_ （填写邮箱可以收到回复通知～）","avatar":null,"meta":["nick","mail","link"],"pageSize":10,"lang":"zh-cn","visitor":true},"fireworks":{"colors":["102, 167, 221","62, 131, 225","33, 78, 194"]}};
  </script><script src="//at.alicdn.com/t/font_1140697_pa6cswvjpq.js" async></script><meta name="generator" content="Hexo 4.2.0"><link rel="alternate" href="/atom.xml" title="秩同道合的小站" type="application/atom+xml">
</head><body><script defer src="https://cdn.jsdelivr.net/npm/animejs@latest/anime.min.js"></script><script defer src="/js/ui/fireworks.js"></script><canvas class="fireworks"></canvas><div class="container"><a class="sidebar-toggle sidebar-toggle-fixed hty-icon-button"><div class="hamburger hamburger--spin" type="button"><span class="hamburger-box"><span class="hamburger-inner"></span></span></div></a><aside class="sidebar"><ul class="sidebar-nav"><li class="sidebar-nav-item sidebar-nav-toc sidebar-nav-active hty-icon-button" data-target="post-toc-wrap" title="文章目录"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-list-ordered"></use></svg></li><li class="sidebar-nav-item sidebar-nav-overview hty-icon-button" data-target="site-overview-wrap" title="站点概览"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-passport-line"></use></svg></li></ul><div class="sidebar-panel" id="site-overview-wrap"><div class="site-info fix-top"><a class="site-author-avatar" href="/about" title="秩同道合"><img loading="lazy" src="https://cdn.jsdelivr.net/gh/AlexZou14/CDN/img/touxiang.jpg" alt="秩同道合"></a><div class="site-author-name"><a href="/about">秩同道合</a></div><a class="site-name" href="/about/site.html">秩同道合的小站</a><sub class="site-subtitle">寻找志趣相投的伙伴！</sub><div class="site-desciption">我和你，以及我们的秩相同所以我们才等价！</div></div><nav class="site-state"><a class="site-state-item hty-icon-button icon-home" href="/" title="首页"><span class="site-state-item-icon"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-home-4-line"></use></svg></span></a><div class="site-state-item site-state-posts"><a href="/archives" title="归档"><span class="site-state-item-icon"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-archive-line"></use></svg></span><span class="site-state-item-count">29</span></a></div><div class="site-state-item site-state-categories"><a href="/categories" title="分类"><span class="site-state-item-icon"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-folder-2-line"></use></svg></span><span class="site-state-item-count">2</span></a></div><div class="site-state-item site-state-tags"><a href="/tags" title="标签"><span class="site-state-item-icon"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-price-tag-3-line"></use></svg></span><span class="site-state-item-count">5</span></a></div><a class="site-state-item hty-icon-button" href="https://github.com/AlexZou14" target="_blank" rel="noopener" title="reward.comment"><span class="site-state-item-icon"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-hand-coin-line"></use></svg></span></a></nav><hr style="margin-bottom:0.5rem"><div class="links-of-author"><a class="links-of-author-item hty-icon-button" rel="noopener" href="/atom.xml" title="RSS" target="_blank" style="color:orange"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-rss-line"></use></svg></a><a class="links-of-author-item hty-icon-button" rel="noopener" href="https://github.com/AlexZou14" title="GitHub" target="_blank" style="color:#181717"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-github-line"></use></svg></a><a class="links-of-author-item hty-icon-button" rel="noopener" href="mailto:1120375574@qq.com" title="E-Mail" target="_blank" style="color:#8E71C1"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-mail-line"></use></svg></a><a class="links-of-author-item hty-icon-button" rel="noopener" href="https://space.bilibili.com/19164044" title="哔哩哔哩" target="_blank" style="color:#FF8EB3"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-bilibili-line"></use></svg></a></div><hr style="margin:0.5rem 1rem"><div class="links"><a class="links-item hty-icon-button" href="http://sotavision.cn" target="_blank" rel="noopener" title="工作组" style="color:#000000"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-send-plane-2-line"></use></svg></a></div></div><script defer src="/js/sidebar.js"></script><div class="sidebar-panel sidebar-panel-active" id="post-toc-wrap"><div class="post-toc"><div class="post-toc-progress"><div class="progress-bar"></div><div class="progress-info"><span class="progress-notice">您已经读了</span><span class="progress-num">0</span><span class="progress-percentage">%</span></div></div><div class="post-toc-content"><ol class="toc"><li class="toc-item toc-level-3"><a class="toc-link" href="#Abstract"><span class="toc-number">1.</span> <span class="toc-text">Abstract</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Introduction"><span class="toc-number">2.</span> <span class="toc-text">Introduction</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Related-work"><span class="toc-number">3.</span> <span class="toc-text">Related work</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Proposed-Method"><span class="toc-number">4.</span> <span class="toc-text">Proposed Method</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Single-image-super-resolution"><span class="toc-number">4.1.</span> <span class="toc-text">Single image super-resolution</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Network-structure"><span class="toc-number">4.2.</span> <span class="toc-text">Network structure</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Loss-function"><span class="toc-number">4.3.</span> <span class="toc-text">Loss function</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#Pixel-wise-loss-in-the-image-space"><span class="toc-number">4.3.1.</span> <span class="toc-text">Pixel-wise loss in the image-space</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#Perceptual-loss-in-feature-space"><span class="toc-number">4.3.2.</span> <span class="toc-text">Perceptual loss in feature space</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#Texture-matching-loss"><span class="toc-number">4.3.3.</span> <span class="toc-text">Texture matching loss</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#Adversarial-training"><span class="toc-number">4.3.4.</span> <span class="toc-text">Adversarial training</span></a></li></ol></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Experiment"><span class="toc-number">5.</span> <span class="toc-text">Experiment</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Experiments-Details"><span class="toc-number">5.1.</span> <span class="toc-text">Experiments Details</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Effect-of-different-losses"><span class="toc-number">5.2.</span> <span class="toc-text">Effect of different losses</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Evaluation-of-perceptual-quality"><span class="toc-number">5.3.</span> <span class="toc-text">Evaluation of perceptual quality</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Conclusions"><span class="toc-number">6.</span> <span class="toc-text">Conclusions</span></a></li></ol></div></div></div></aside><main class="sidebar-translate" id="content"><div id="post"><article class="post-block" itemscope itemtype="http://schema.org/Article"><link itemprop="mainEntityOfPage" href="http://alexzou14.github.io/2020/04/06/EhanceNet/"><span hidden itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="name" content="秩同道合"><meta itemprop="description" content="EnhanceNet：Single ImageSuper-Resolution through Automated Texture Synthesis论文阅读笔记"></span><span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="秩同道合的小站"></span><header class="post-header"><h1 class="post-title" itemprop="name headline">EnhanceNet：Single ImageSuper-Resolution through Automated Texture Synthesis论文阅读笔记</h1><div class="post-meta"><span class="post-time"><span class="post-meta-item-icon"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-calendar-line"></use></svg></span> <time title="创建时间：2020-04-06 17:43:53" itemprop="dateCreated datePublished" datetime="2020-04-06T17:43:53+08:00">2020-04-06</time></span><div class="post-count"><span class="post-symbolcount"><span class="post-meta-item-icon" title="本文字数"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-file-word-line"></use></svg></span> <span title="本文字数">6.9k</span><span class="post-meta-divider">-</span><span class="post-meta-item-icon" title="阅读时长"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-timer-line"></use></svg></span> <span title="阅读时长">14 分钟</span></span></div><span class="leancloud_visitors" id="/2020/04/06/EhanceNet/" data-flag-title="EnhanceNet：Single ImageSuper-Resolution through Automated Texture Synthesis论文阅读笔记"><span class="post-meta-item-icon" title="阅读次数"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-eye-line"></use></svg> <span class="leancloud-visitors-count"></span></span></span><div class="post-classify"><span class="post-category"><span class="post-meta-item-icon"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-folder-line"></use></svg></span> <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a class="category" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/" itemprop="url" rel="index"><span itemprop="text">论文笔记</span></a></span></span><span class="post-tag"><span class="post-meta-divider">-</span><a class="tag" href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"><span class="post-meta-item-icon"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-price-tag-3-line"></use></svg></span><span class="tag-name">深度学习</span></a><a class="tag" href="/tags/%E7%AC%94%E8%AE%B0/"><span class="post-meta-item-icon"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-price-tag-3-line"></use></svg></span><span class="tag-name">笔记</span></a><a class="tag" href="/tags/%E8%B6%85%E5%88%86%E8%BE%A8%E7%8E%87/"><span class="post-meta-item-icon"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-price-tag-3-line"></use></svg></span><span class="tag-name">超分辨率</span></a></span></div></div></header><section class="post-body" itemprop="articleBody"><div class="post-content post-markdown"><p>文章地址：<a href="https://arxiv.org/pdf/1612.07919.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1612.07919.pdf</a></p>
<h3 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h3><p>在当时，图像超分辨率的性能评判指标是通过PSNR来测量的，但是PSNR与人感知相关性差，PSNR过小会造成图像过于平滑缺少纹理看上去不够自然。作者提出了一种结合纹理损失的自动纹理合成的新颖应用，该损失专注于创建逼真的纹理，而不是针对训练过程中像素精度的地面真实图像再现进行优化。 通过在对抗训练环境中使用前馈全卷积神经网络，我们可以在高放大倍率下显着提高图像质量。 在大量数据集上进行的广泛实验证明了我们方法的有效性，在定量和定性基准方面都产生了最新的结果。</p>
<hr>
<h3 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h3><p>超分辨率问题的不定性：对HR图像进行降采样时，大量不同的HR图像会产生相同的LR图像。这导致SISR成为高度复杂的问题。一个关键问题是大量降采样因子导致高频信息丢失，从而使超分辨图像中的纹理区域变得模糊，过分平滑且外观不自然。如下图所示：</p>
<blockquote>
<p><img src="http://sotavision.cn/showdoc/server/../Public/Uploads/2020-03-30/5e8155f778b87.png" alt="" loading="lazy"></p>
</blockquote>
<p>原因是，选择了当前最先进的方法所采用的目标函数：大多数系统将HR Ground Truth 图像与其从HR真实图像重建之间的像素平均均方误差（MSE）降至最低。 然而，LR观察与人类对图像质量的感知之间的相关性很差。 虽然易于最小化，但最佳的MSE估计器返回许多可能解的平均值，这使得SISR结果看起来不自然且难以置信。 在超分辨率的背景下，这种均值回归问题是众所周知的事实，但是，对自然图像的高维多峰分布进行建模仍然是一个具有挑战性的问题。</p>
<blockquote>
<p><img src="http://sotavision.cn/showdoc/server/../Public/Uploads/2020-03-30/5e81576a3af3b.png" alt="" loading="lazy"></p>
</blockquote>
<p>作者因此提出了一种新的文理合成网络改进方案，结合了对抗训练的感知损失。</p>
<hr>
<h3 id="Related-work"><a href="#Related-work" class="headerlink" title="Related work"></a>Related work</h3><p>作者在这部分简要的说明了早期传统方法容易产生模糊和伪影。当时比较受欢迎的方法有基于外部实例的方法和董超老师提出来的深度学习方法。因为这些模型都是通过优化最小均方误差（MSE）得到的，所以这些结果都趋于模糊和缺少高频细节的。一种感知损失的提出，虽然PSNR更低但是也锐化了输出结果。对抗网络就是当时可以得到比较好的锐化结果，但是同时这个网络也会是的生成图像添加一些不必要的伪影。作者就是在这些工作基础上展开的。</p>
<hr>
<h3 id="Proposed-Method"><a href="#Proposed-Method" class="headerlink" title="Proposed Method"></a>Proposed Method</h3><h4 id="Single-image-super-resolution"><a href="#Single-image-super-resolution" class="headerlink" title="Single image super-resolution"></a>Single image super-resolution</h4><p>一张高分辨率图片\( I_{HR}\in[0,1]^{\alpha w\times \alpha h\times c} \)下采样为低分辨率图像：<br>\( I_{LR}=d_{\alpha}(I_{HR})\in [0,1]^{w\times h\times c} \)<br>使用下采样操作表示为：<br>\( d_{\alpha}:[0,1]^{\alpha w\times \alpha h\times c}\rightarrow [0,1]^{w\times h\times c} \)<br>固定比例因子\( \alpha &gt;1 \)，图像宽度\(w\)，高度\(h\)和颜色通道\(c\)。 SISR的任务是提供一个从\(I_{LR}\)估计\(I_{HR}\)的近似逆\(f\approx d^{-1}\)：<br>由于下采样操作d是non-injective 的，并且存在大量可能的图像\(I_{est}\)，而\(d(I_{est})=I_{LR}\)保持不变，因此该问题很复杂。<br>最近的学习方法旨在通过使当前估计和ground truth图像之间的欧几里得损失\(\left |{I_{est}-I_{HR}}\right|_2^2\)最小化，通过多层神经网络来近似\(f\)。 尽管这些模型通过PSNR可以得出出色的结果，但所得图像往往看起来模糊并且缺少原始图像中存在的高频纹理。 这是SISR模棱两可的直接结果：由于下采样会从输入图像中去除高频信息，因此没有方法希望以像素为单位重现所有精细的细节。 因此，即使是最先进的模型也要学习在那些区域中生成所有可能纹理的均值，以使输出图像的欧几里得损失最小化。</p>
<h4 id="Network-structure"><a href="#Network-structure" class="headerlink" title="Network structure"></a>Network structure</h4><p>网络结构采用全卷积的方式，使得输入图像可以是任意尺寸。受到VGG网络的启发，卷积核全部采用3*3的尺寸，在保持一定量参数的情况下构建更深的网络。网络的输入是低分辨率图像，在网络末端采用最近邻的方法上采样达到高分辨率图像的尺寸，这样有利于降低计算复杂度，参数的初始化采用Xavier，网络的学习目标是超分辨率图像与输入的低分辨率图像线性插值的差，整个网络架构如下图所示</p>
<blockquote>
<p><img src="http://sotavision.cn/showdoc/server/../Public/Uploads/2020-03-30/5e81610a96e33.png" alt="" loading="lazy"></p>
</blockquote>
<p>超分辨率重建网络+感知网络+对抗网络(判别器)<br>第一不用bicubic interpolation,原因是，过多的引入冗余，而且计算量大。第二，不用transpose convolution ,原因是，过多的引入冗余，同时感受野增加了。另外，transconvolution容易产生checkerboard artifacts，需要加入而外的惩罚loss。<br>超分辨率重建网络=ResNet block(下采样path)+nearest upsampling(上采样路径).</p>
<h4 id="Loss-function"><a href="#Loss-function" class="headerlink" title="Loss function"></a>Loss function</h4><h5 id="Pixel-wise-loss-in-the-image-space"><a href="#Pixel-wise-loss-in-the-image-space" class="headerlink" title="Pixel-wise loss in the image-space"></a>Pixel-wise loss in the image-space</h5><div>
    \( L_E=\left\|I_{est}-I_{HR}\right\|_2^2 \)
</div>
其中 
<div>
    \(\left\|I\right\|_2^2=\dfrac{1}{whc}\displaystyle\sum_{w,h,c}(I_{w,h,c})^2\)
</div>
pixel-wise loss强调的是两幅图像之间每个对应像素的匹配，这与人眼的感知结果有所区别。通过pixel-wise loss训练的图片通常会较为平滑，缺少高频信息。即使输出图片具有较高的PSNR，视觉效果也并没有很突出。

<h5 id="Perceptual-loss-in-feature-space"><a href="#Perceptual-loss-in-feature-space" class="headerlink" title="Perceptual loss in feature space"></a>Perceptual loss in feature space</h5><p>这个损失的计算是把\(I_{est}\)和\(I_{HR}\)送入一个函数\(\phi\)计算在\(\phi\)映射下的L2损失：<br>\(L_P=\left|\phi (I_{est})-\phi(I_{HR})\right|_2^2\)<br>我们避免了要求网络输出图像与原始高分辨率图像pixel-wise上的一致，而是鼓励两幅图具有相似的特征。<br>一般\(\phi\)可以用预训练好的VGG19在第二和第五池化层的输出。</p>
<h5 id="Texture-matching-loss"><a href="#Texture-matching-loss" class="headerlink" title="Texture matching loss"></a>Texture matching loss</h5><p>该损失的表示为：\(L_P=\left|G(\phi (I_{est}))-G(\phi(I_{HR}))\right|_2^2\)其中G为gram matrix:\(G(F)=FF^T\)<br>这个和分割迁移的损失类似，用在这的主要目的是生成和HR图像相似的局部纹路。经验是16*16生成的效果最好。</p>
<h5 id="Adversarial-training"><a href="#Adversarial-training" class="headerlink" title="Adversarial training"></a>Adversarial training</h5><p>对抗训练是一种最新技术，已被证明是产生逼真的图像的有用机制。 在原始设置中，对生成网络G进行了训练， 同时，判别网络D被训练以区分来自数据集的真实图像x和生成的样本G(z)。这种方法导致了minimax游戏，其中训练了生成器以使其loss最小化:\(L_A=-\log (D(G(z)))\)<br>鉴别器loss最小化:\(L_D=-\log (D(x))-\log (1-D(G(z)))\)</p>
<blockquote>
<p><img src="http://sotavision.cn/showdoc/server/../Public/Uploads/2020-03-30/5e8166cd4e466.png" alt="" loading="lazy"></p>
</blockquote>
<hr>
<h3 id="Experiment"><a href="#Experiment" class="headerlink" title="Experiment"></a>Experiment</h3><h4 id="Experiments-Details"><a href="#Experiments-Details" class="headerlink" title="Experiments Details"></a>Experiments Details</h4><p>具体实验细节待复现</p>
<h4 id="Effect-of-different-losses"><a href="#Effect-of-different-losses" class="headerlink" title="Effect of different losses"></a>Effect of different losses</h4><p>我们使用表2中列举的Loss组合进行训练，并将结果展示在表3中。更多的结果在补充中。</p>
<blockquote>
<p><img src="http://sotavision.cn/showdoc/server/../Public/Uploads/2020-03-30/5e81674cef1ed.png" alt="" loading="lazy"><br><img src="http://sotavision.cn/showdoc/server/../Public/Uploads/2020-03-30/5e81678163846.png" alt="" loading="lazy"></p>
</blockquote>
<h4 id="Evaluation-of-perceptual-quality"><a href="#Evaluation-of-perceptual-quality" class="headerlink" title="Evaluation of perceptual quality"></a>Evaluation of perceptual quality</h4><p>在ImgeNet数据集上对用户进行问卷调查，其中91.0%选择由ENet-PAT产生的图像</p>
<hr>
<h3 id="Conclusions"><a href="#Conclusions" class="headerlink" title="Conclusions"></a>Conclusions</h3><p>本文主要贡献：</p>
<ul>
<li>我们提出了一种体系结构，该体系结构能够通过欧几里得损失训练或对抗性训练，感知损失和新提出的用于超分辨率的纹理转移损失的新颖组合，通过定量和定性测量产生最新结果 。</li>
</ul>
<p>存在的限制性：</p>
<ul>
<li>由于SISR是一个严重的问题，因此仍然存在一些局限性。 虽然由ENet-PAT生成的图像看上去逼真，但它们与像素像素的真实图像不匹配。 此外，对抗训练有时会在输出中产生伪影，这些伪影会大大减少，但不会增加纹理损失，因此无法完全消除。</li>
</ul>
</div><ul class="post-copyright"><li class="post-copyright-author"><strong>本文作者：</strong>秩同道合</li><li class="post-copyright-link"><strong>本文链接：</strong><a href="http://alexzou14.github.io/2020/04/06/EhanceNet/" title="EnhanceNet：Single ImageSuper-Resolution through Automated Texture Synthesis论文阅读笔记">http://alexzou14.github.io/2020/04/06/EhanceNet/</a></li><li class="post-copyright-license"><strong>版权声明：</strong>本博客所有文章除特别声明外，均默认采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh" target="_blank" rel="noopener" title="CC BY-NC-SA 4.0 "><svg class="icon"><use xlink:href="#icon-creative-commons-line"></use></svg><svg class="icon"><use xlink:href="#icon-creative-commons-by-line"></use></svg><svg class="icon"><use xlink:href="#icon-creative-commons-nc-line"></use></svg><svg class="icon"><use xlink:href="#icon-creative-commons-sa-line"></use></svg></a> 许可协议。</li></ul></section></article><div class="post-nav"><div class="post-nav-item"><a class="post-nav-prev" href="/2020/04/07/DRRN/" rel="prev" title="Image Super-Resolution via Deep Recursive Residual Network论文阅读笔记"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-arrow-left-s-line"></use></svg><span class="post-nav-text">Image Super-Resolution via Deep Recursive Residual Network论文阅读笔记</span></a></div><div class="post-nav-item"><a class="post-nav-next" href="/2020/04/06/EDSR/" rel="next" title="Enhanced Deep Residual Networks for Single Image Super-Resolution论文阅读笔记"><span class="post-nav-text">Enhanced Deep Residual Networks for Single Image Super-Resolution论文阅读笔记</span><svg class="icon" aria-hidden="true"><use xlink:href="#icon-arrow-right-s-line"></use></svg></a></div></div><div id="comment"><div class="comment-tooltip text-center"><span>若您无 GitHub 账号，可直接在下方匿名评论。</span><br><span>若您想及时得到回复提醒，建议跳转 GitHub Issues 评论。</span><br><span>若没有本文 Issue，您可以使用 Comment 模版新建。</span><br><a class="hty-button hty-button--raised" id="github-issues" href="https://github.com/AlexZou14/alexzou14.github.io/issues?q=is:issue+EnhanceNet：Single ImageSuper-Resolution through Automated Texture Synthesis论文阅读笔记" target="_blank" rel="noopener">GitHub Issues</a></div><div class="comment-container" id="valine-container"></div></div><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.css"><script defer src="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.js"></script><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex@latest/dist/contrib/copy-tex.min.css"><script defer src="https://cdn.jsdelivr.net/npm/katex@latest/dist/contrib/copy-tex.min.js"></script><script defer src="https://cdn.jsdelivr.net/npm/katex@latest/dist/contrib/auto-render.min.js" onload="renderMathInElement(document.body);"></script></div></main><footer class="sidebar-translate" id="footer"><div class="copyright"><span>&copy; 2019 – 2021 </span><span class="with-love" id="animate"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-cloud-line"></use></svg></span><span class="author"> 秩同道合</span></div><div class="powered"><span>由 <a href="https://hexo.io" target="_blank" rel="noopener">Hexo</a> 驱动 v4.2.0</span><span class="footer-separator">|</span><span>主题 - <a rel="noopener" href="https://github.com/YunYouJun/hexo-theme-yun" target="_blank"><span>Yun</span></a> v0.3.1</span></div><script defer src="https://cdn.jsdelivr.net/npm/valine@latest/dist/Valine.min.js"></script><script>function initValine() {
  new Valine(CONFIG.valine);
}
document.addEventListener("DOMContentLoaded", function() {
  initValine();
});</script></footer><a class="hty-icon-button" id="goUp" aria-label="back-to-top" href="#"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-arrow-up-s-line"></use></svg></a></div><script defer src="/js/utils.js"></script><script defer src="/js/hexo-theme-yun.js"></script><script src="https://cdn.jsdelivr.net/npm/live2d-widget@^3.1.3/lib/L2Dwidget.min.js"></script><script>L2Dwidget.init({"pluginRootPath":"live2dw/","pluginJsPath":"lib/","pluginModelPath":"assets/","tagMode":false,"debug":false,"model":{"jsonPath":"https://cdn.jsdelivr.net/npm/live2d-widget-model-hijiki@1.0.5/assets/hijiki.model.json"},"display":{"position":"right","width":200,"height":400},"mobile":{"show":true},"react":{"opacity":0.8},"dialog":{"enable":true,"hitokoto":true},"log":false});</script></body></html>