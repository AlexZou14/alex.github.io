<!DOCTYPE html><html lang="zh-CN"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0"><meta name="description" content="Guided Super-Resolution as Pixel-to-Pixel Transformation论文阅读笔记"><meta name="keywords" content="深度学习,笔记,超分辨率"><meta name="author" content="秩同道合"><meta name="copyright" content="秩同道合"><meta name="theme-color" content="#0078E7"><title>Guided Super-Resolution as Pixel-to-Pixel Transformation论文阅读笔记 | 秩同道合的小站</title><link rel="shortcut icon" href="https://cdn.jsdelivr.net/gh/AlexZou14/CDN/img/favicon.ico"><link rel="preload" href="/css/hexo-theme-yun.css" as="style"><link rel="preload" href="/js/utils.js" as="script"><link rel="preload" href="/js/hexo-theme-yun.js" as="script"><link rel="prefetch" href="/js/sidebar.js" as="script"><link rel="preconnect" href="https://cdn.jsdelivr.net" crossorigin><link rel="stylesheet" href="/css/hexo-theme-yun.css"><link rel="alternate" href="/atom.xml" title="秩同道合的小站"><script id="yun-config">
    let Yun = window.Yun || {};
    let CONFIG = {"root":"/","title":"秩同道合的小站","version":"0.3.1","anonymous_image":"https://cdn.jsdelivr.net/gh/YunYouJun/cdn/img/avatar/none.jpg","say":{"api":"https://v1.hitokoto.cn","hitokoto":true},"valine":{"el":"#valine-container","verify":false,"notify":false,"appId":"eCgP91hRSX8OtvCIR4MgLfcl-gzGzoHsz","appKey":"N5gVT8kUx5O0wMvc47SU040Y","serverURLs":null,"placeholder":"大佬求指教&nbsp_(:з」∠)_ （填写邮箱可以收到回复通知～）","avatar":null,"meta":["nick","mail","link"],"pageSize":10,"lang":"zh-cn","visitor":true},"fireworks":{"colors":["102, 167, 221","62, 131, 225","33, 78, 194"]}};
  </script><script src="//at.alicdn.com/t/font_1140697_pa6cswvjpq.js" async></script><meta name="generator" content="Hexo 4.2.0"><link rel="alternate" href="/atom.xml" title="秩同道合的小站" type="application/atom+xml">
</head><body><script defer src="https://cdn.jsdelivr.net/npm/animejs@latest/anime.min.js"></script><script defer src="/js/ui/fireworks.js"></script><canvas class="fireworks"></canvas><div class="container"><a class="sidebar-toggle sidebar-toggle-fixed hty-icon-button"><div class="hamburger hamburger--spin" type="button"><span class="hamburger-box"><span class="hamburger-inner"></span></span></div></a><aside class="sidebar"><ul class="sidebar-nav"><li class="sidebar-nav-item sidebar-nav-toc sidebar-nav-active hty-icon-button" data-target="post-toc-wrap" title="文章目录"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-list-ordered"></use></svg></li><li class="sidebar-nav-item sidebar-nav-overview hty-icon-button" data-target="site-overview-wrap" title="站点概览"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-passport-line"></use></svg></li></ul><div class="sidebar-panel" id="site-overview-wrap"><div class="site-info fix-top"><a class="site-author-avatar" href="/about" title="秩同道合"><img loading="lazy" src="https://cdn.jsdelivr.net/gh/AlexZou14/CDN/img/touxiang.jpg" alt="秩同道合"></a><div class="site-author-name"><a href="/about">秩同道合</a></div><a class="site-name" href="/about/site.html">秩同道合的小站</a><sub class="site-subtitle">寻找志趣相投的伙伴！</sub><div class="site-desciption">我和你，以及我们的秩相同所以我们才等价！</div></div><nav class="site-state"><a class="site-state-item hty-icon-button icon-home" href="/" title="首页"><span class="site-state-item-icon"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-home-4-line"></use></svg></span></a><div class="site-state-item site-state-posts"><a href="/archives" title="归档"><span class="site-state-item-icon"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-archive-line"></use></svg></span><span class="site-state-item-count">26</span></a></div><div class="site-state-item site-state-categories"><a href="/categories" title="分类"><span class="site-state-item-icon"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-folder-2-line"></use></svg></span><span class="site-state-item-count">1</span></a></div><div class="site-state-item site-state-tags"><a href="/tags" title="标签"><span class="site-state-item-icon"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-price-tag-3-line"></use></svg></span><span class="site-state-item-count">3</span></a></div><a class="site-state-item hty-icon-button" href="https://github.com/AlexZou14" target="_blank" rel="noopener" title="reward.comment"><span class="site-state-item-icon"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-hand-coin-line"></use></svg></span></a></nav><hr style="margin-bottom:0.5rem"><div class="links-of-author"><a class="links-of-author-item hty-icon-button" rel="noopener" href="/atom.xml" title="RSS" target="_blank" style="color:orange"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-rss-line"></use></svg></a><a class="links-of-author-item hty-icon-button" rel="noopener" href="https://github.com/AlexZou14" title="GitHub" target="_blank" style="color:#181717"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-github-line"></use></svg></a><a class="links-of-author-item hty-icon-button" rel="noopener" href="mailto:1120375574@qq.com" title="E-Mail" target="_blank" style="color:#8E71C1"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-mail-line"></use></svg></a><a class="links-of-author-item hty-icon-button" rel="noopener" href="https://space.bilibili.com/19164044" title="哔哩哔哩" target="_blank" style="color:#FF8EB3"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-bilibili-line"></use></svg></a></div><hr style="margin:0.5rem 1rem"><div class="links"><a class="links-item hty-icon-button" href="http://sotavision.cn" target="_blank" rel="noopener" title="工作组" style="color:#000000"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-send-plane-2-line"></use></svg></a></div></div><script defer src="/js/sidebar.js"></script><div class="sidebar-panel sidebar-panel-active" id="post-toc-wrap"><div class="post-toc"><div class="post-toc-progress"><div class="progress-bar"></div><div class="progress-info"><span class="progress-notice">您已经读了</span><span class="progress-num">0</span><span class="progress-percentage">%</span></div></div><div class="post-toc-content"><ol class="toc"><li class="toc-item toc-level-3"><a class="toc-link" href="#Abstract"><span class="toc-number">1.</span> <span class="toc-text">Abstract</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Introduction"><span class="toc-number">2.</span> <span class="toc-text">Introduction</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Related-work"><span class="toc-number">3.</span> <span class="toc-text">Related work</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Guided-filtering"><span class="toc-number">3.1.</span> <span class="toc-text">Guided filtering</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Guided-super-resolution"><span class="toc-number">3.2.</span> <span class="toc-text">Guided super-resolution</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Learned-guided-super-resolution"><span class="toc-number">3.3.</span> <span class="toc-text">Learned guided super-resolution</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Proposed-Method"><span class="toc-number">4.</span> <span class="toc-text">Proposed Method</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Notation-and-Preliminaries"><span class="toc-number">4.1.</span> <span class="toc-text">Notation and Preliminaries</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Proposed-Solution"><span class="toc-number">4.2.</span> <span class="toc-text">Proposed Solution</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Experimental-Results"><span class="toc-number">5.</span> <span class="toc-text">Experimental Results</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Evaluation-Settings"><span class="toc-number">5.1.</span> <span class="toc-text">Evaluation Settings</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Analysis"><span class="toc-number">5.2.</span> <span class="toc-text">Analysis</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Comparisons-with-state-of-the-arts"><span class="toc-number">5.3.</span> <span class="toc-text">Comparisons with state-of-the-arts</span></a></li></ol></li></ol></div></div></div></aside><main class="sidebar-translate" id="content"><div id="post"><article class="post-block" itemscope itemtype="http://schema.org/Article"><link itemprop="mainEntityOfPage" href="http://alexzou14.github.io/2020/04/08/GuideSR/"><span hidden itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="name" content="秩同道合"><meta itemprop="description" content="Guided Super-Resolution as Pixel-to-Pixel Transformation论文阅读笔记"></span><span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="秩同道合的小站"></span><header class="post-header"><h1 class="post-title" itemprop="name headline">Guided Super-Resolution as Pixel-to-Pixel Transformation论文阅读笔记</h1><div class="post-meta"><span class="post-time"><span class="post-meta-item-icon"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-calendar-line"></use></svg></span> <time title="创建时间：2020-04-08 11:11:46" itemprop="dateCreated datePublished" datetime="2020-04-08T11:11:46+08:00">2020-04-08</time></span><div class="post-count"><span class="post-symbolcount"><span class="post-meta-item-icon" title="本文字数"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-file-word-line"></use></svg></span> <span title="本文字数">6.4k</span><span class="post-meta-divider">-</span><span class="post-meta-item-icon" title="阅读时长"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-timer-line"></use></svg></span> <span title="阅读时长">13 分钟</span></span></div><span class="leancloud_visitors" id="/2020/04/08/GuideSR/" data-flag-title="Guided Super-Resolution as Pixel-to-Pixel Transformation论文阅读笔记"><span class="post-meta-item-icon" title="阅读次数"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-eye-line"></use></svg> <span class="leancloud-visitors-count"></span></span></span><div class="post-classify"><span class="post-category"><span class="post-meta-item-icon"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-folder-line"></use></svg></span> <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a class="category" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/" itemprop="url" rel="index"><span itemprop="text">论文笔记</span></a></span></span><span class="post-tag"><span class="post-meta-divider">-</span><a class="tag" href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"><span class="post-meta-item-icon"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-price-tag-3-line"></use></svg></span><span class="tag-name">深度学习</span></a><a class="tag" href="/tags/%E7%AC%94%E8%AE%B0/"><span class="post-meta-item-icon"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-price-tag-3-line"></use></svg></span><span class="tag-name">笔记</span></a><a class="tag" href="/tags/%E8%B6%85%E5%88%86%E8%BE%A8%E7%8E%87/"><span class="post-meta-item-icon"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-price-tag-3-line"></use></svg></span><span class="tag-name">超分辨率</span></a></span></div></div></header><section class="post-body" itemprop="articleBody"><div class="post-content post-markdown"><h3 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h3><p>本文提出了引导超分的概念。输入是某种模态的低分辨率图像数据，如用ToF相机获取的深度图像。目标输出是输入图像的高分辨率版本。其中还有一个概念叫做Guided Image——引导图像，是输入图像在另一种模态下的高分辨率图像，如使用传统相机获取的深度图像。传统建模的思想是：将低分辨率图像进行上采样，从引导图像中获取高频信息补充到上采样后的输入图像。本文作者利用了机器学习的方法将这个问题转化成了引导图像到输入图像模态的pixel-to-pixel的映射，和风格迁移有着很大的相似之处。Pixel-to-pixel mapping是通过多层感知机进行的实现，通过最小化源图像和下采样目标图像之间的差异来学习其权重，即为我们所要求解的映射。本文提出的算法只需要对映射函数进行正则化，避免了对输出结果的直接处理，而且是无监督学习的算法。</p>
<hr>
<h3 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h3><p>很多计算机视觉任务都可以看作引导超分辨率的实例，例如，环境测绘，机器人视觉等等。引导超分辨率可以看成输入一张低分辨率图，再出入一张相似的高清图来还原目标图像得到高分辨率图。如下图所示：</p>
<blockquote>
<p><img src="http://sotavision.cn/showdoc/server/../Public/Uploads/2020-03-23/5e7815665ee78.png" alt="" loading="lazy"></p>
</blockquote>
<p>换一个角度看，引导超分辨又可以看作是引导滤波器的推广形式，引导滤波器将源图像映射到目标图像  通过计算每个像素处的相同大小，依赖于源和引导图像中的局部邻域，从而得到源图像的高清版本。作者提出了另一种关于超分辨率的解释，源图像和引导图像的作用被交换，即从一个图像的像素级映射 到另一个而不改变分辨率，并通过要求其下采样版本与源映像匹配来约束输出。<br>文中认为，这种导超分辨率的观点有两个非常实际的优点：</p>
<ul>
<li>从所需分辨率开始，只使用1×1个核，不同的输入位置不会混合，这避免了模糊。</li>
<li>通过对所有像素使用相同的映射函数，并在其参数之前放置收缩，这样就会得到不需要正则化输出图像的适定问题</li>
</ul>
<p>本文的贡献是</p>
<ul>
<li>一种新的引导超分辨率公式，即无监督学习从像素到像素的转换受低分辨率光源限制。</li>
<li>我们对两个任务进行了实验：深度图的超分辨率和树高图的超分辨率。它们表明我们的方法在高上采样因子上明显优于其他的超分辨率方法</li>
</ul>
<hr>
<h3 id="Related-work"><a href="#Related-work" class="headerlink" title="Related work"></a>Related work</h3><h4 id="Guided-filtering"><a href="#Guided-filtering" class="headerlink" title="Guided filtering"></a>Guided filtering</h4><p>关于引导滤波，一般的原理是通过应用一个滤波器来增强源图像，该滤波器的输出不仅取决于源图像的局部邻域，而且还取决于从引导图像中的相同邻域导出的权重。引导滤波已经被用于各种各样的图像处理应用中，从去噪或着色等低级任务一直到立体匹配等。</p>
<h4 id="Guided-super-resolution"><a href="#Guided-super-resolution" class="headerlink" title="Guided super-resolution"></a>Guided super-resolution</h4><p>对于超分辨深度以及诸如色调映射和图像着色等低层操作，已经探索了将引导滤波扩展到超分辨问题。这些方法中有基于上述局部滤波原理的局部方法和将上采样任务描述为全局能量最小化的全局方法。</p>
<h4 id="Learned-guided-super-resolution"><a href="#Learned-guided-super-resolution" class="headerlink" title="Learned guided super-resolution"></a>Learned guided super-resolution</h4><p>这些方法迄今为止都是非监督的。还有一系列的工作，从例子中学习如何向上采样源图像同时将高频细节从引导图像传输到目标输出上。<br>这中数据处理方法的优点有：从真实图像数据中学习如何最佳融合源图像和引导图像可能比手工启发式方法更好。<br>与所有监督学习的缺点一样：</p>
<ol>
<li>必须获取足够数量的训练数据</li>
<li>通过这样设计的算法可能在训练数据上过拟合，但是在真实场景下表现不好</li>
</ol>
<p>高分辨率“引导”图像通过标准语义分割网络生成“目标”分割映射，使用一个损失函数鼓励目标具有相同的目标。 标签分布作为低分辨率的源图。</p>
<hr>
<h3 id="Proposed-Method"><a href="#Proposed-Method" class="headerlink" title="Proposed Method"></a>Proposed Method</h3><blockquote>
<p><img src="http://sotavision.cn/showdoc/server/../Public/Uploads/2020-03-23/5e7825da3d75f.png" alt="" loading="lazy"></p>
</blockquote>
<h4 id="Notation-and-Preliminaries"><a href="#Notation-and-Preliminaries" class="headerlink" title="Notation and Preliminaries"></a>Notation and Preliminaries</h4><p>首先对符号做出如下的定义：Source Image记作S（S.size=M×M），Guided Image记作G（G.size=N×N×C），目标图像记作T（T.size=N×N）。N和M之间的数量关系是通过尺度为D的上采样算子决定的，即N=D·M。S中的第m个像素记作\(S_m\),\(S_m\)是由T中的D×D范围内的block即b(m)经过某种非线性均值方式获得到的，即<br>\[ s_m=\dfrac{1}{D^2} \sum_{n\in b(m)}t_n = {\left\langle t_n \right\rangle}_{b(m)} \]<br>目标是Given S and G，esitimate\(\hat{T}\)</p>
<h4 id="Proposed-Solution"><a href="#Proposed-Solution" class="headerlink" title="Proposed Solution"></a>Proposed Solution</h4><p>设Guided Image和Target Image之间的映射为参数记作\(\theta\)的函数\(f_{\theta}\), 二者满足如下的关系\(\hat{t}_ n =f_{\theta}(g_n)\)，s和t之间的距离度量采用1-Norm，目标函数建立如下:<br>\[ \hat{\theta}=\arg\min_{\theta}\sum_{m}\left|{s_m - \left\langle{f_{\theta}(g_n)}\right\rangle_{b(m)}}\right| \]<br>这里是一个很典型的参数估计问题：\(f_{\theta}\)的形式为多层卷积感知器，参数记作\(\theta\) 。为了避免ill-posed problem求解出过拟合的参数\(\theta\)，作者引入了对\(\theta\)的 2-Norm 正则项，如下所示。参数\(\lambda\)控制了正则化的强度。<br>\[ \hat{\theta}=\arg\min_{\theta}\sum_{m}\left|{s_m - \left\langle{f_{\theta}(g_n)}\right\rangle_{b(m)}}\right|+\lambda\left|{\theta}\right|^2 \]<br>最后为了避免由于色彩、角点等其它特征造成的映射歧义，\(f_{\theta}\)函数引入了坐标变量 \(x_{n}\)。坐标和图像数据分别进行训练，将最终的结果merge到一起来进行参数优化。形式如下式所示:<br>\[ \hat{\theta}=\arg\min_{\theta}\sum_{m}\left|{s_m - \left\langle{f_{\theta}(g_n,x_n)}\right\rangle_{b(m)}}\right|+\lambda\left|{\theta}\right|^2 \]<br>通过这种架构，还可以通过设置各个超参数λg(=0.001)，λx(=0.0001)，λhead(=0.0001)来不同地对每个分支进行处理。目标函数采用了梯度下降法进行求解。</p>
<blockquote>
<p><img src="http://sotavision.cn/showdoc/server/../Public/Uploads/2020-03-23/5e782eec854c0.png" alt="" loading="lazy"></p>
</blockquote>
<hr>
<h3 id="Experimental-Results"><a href="#Experimental-Results" class="headerlink" title="Experimental Results"></a>Experimental Results</h3><p>论文代码：<a href="https://github.com/riccardodelutio/PixTransform" target="_blank" rel="noopener">https://github.com/riccardodelutio/PixTransform</a></p>
<h4 id="Evaluation-Settings"><a href="#Evaluation-Settings" class="headerlink" title="Evaluation Settings"></a>Evaluation Settings</h4><p>在所有实验中，作者将目标分辨率设置为256×256像素。 在不同的上采样因子（即×4，×8，×16和×32）下评估算法，分别对应于64×64、32×32、16×16和8×8的源分辨率。 所对比的方法：BiCubic、Guided Filter、Fast Bilateral Solver、Static-Dynamic Solver、MSG-Net。对比指标：Percentage of Bad Pixels（PBP），均方误差，平均绝对误差。<br>\[PBP_{\delta} = \dfrac{1}{N^2}\sum_n[|\hat{t_n}-t_n|&gt;\delta]\]<br>这里没有使用SSIM和PSNR的原因，个人分析认为：Guided SR不同于重建，因此不存在质量评价的这一标准，只需要考量和Ground Truth之间的差异即可。</p>
<h4 id="Analysis"><a href="#Analysis" class="headerlink" title="Analysis"></a>Analysis</h4><blockquote>
<p><img src="http://sotavision.cn/showdoc/server/../Public/Uploads/2020-03-23/5e7830c4e71b9.png" alt="" loading="lazy"></p>
</blockquote>
<p>这里展示了不同的映射函数的结果</p>
<h4 id="Comparisons-with-state-of-the-arts"><a href="#Comparisons-with-state-of-the-arts" class="headerlink" title="Comparisons with state-of-the-arts"></a>Comparisons with state-of-the-arts</h4><p><img src="http://sotavision.cn/showdoc/server/../Public/Uploads/2020-03-23/5e78312f76337.png" alt="" loading="lazy"></p>
<p>由上面显示的结果可看出，这个方法在处理分割图像超分上是目前最优的</p>
<hr>
<p>###Conclusions</p>
<ol>
<li>提出了一种新的、无监督的引导超分辨率方法。关键思想是将问题看作是高分辨率引导图像向低分辨率源图像域的像素级变换。</li>
<li>即使在高上采样因子上该方法也能够恢复非常精细的结构和非常锋利的边缘</li>
<li>利用了机器学习的方法将这个问题转化成了引导图像到输入图像模态的pixel-to-pixel的映射</li>
</ol>
</div><ul class="post-copyright"><li class="post-copyright-author"><strong>本文作者：</strong>秩同道合</li><li class="post-copyright-link"><strong>本文链接：</strong><a href="http://alexzou14.github.io/2020/04/08/GuideSR/" title="Guided Super-Resolution as Pixel-to-Pixel Transformation论文阅读笔记">http://alexzou14.github.io/2020/04/08/GuideSR/</a></li><li class="post-copyright-license"><strong>版权声明：</strong>本博客所有文章除特别声明外，均默认采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh" target="_blank" rel="noopener" title="CC BY-NC-SA 4.0 "><svg class="icon"><use xlink:href="#icon-creative-commons-line"></use></svg><svg class="icon"><use xlink:href="#icon-creative-commons-by-line"></use></svg><svg class="icon"><use xlink:href="#icon-creative-commons-nc-line"></use></svg><svg class="icon"><use xlink:href="#icon-creative-commons-sa-line"></use></svg></a> 许可协议。</li></ul></section></article><div class="post-nav"><div class="post-nav-item"><a class="post-nav-prev" href="/2020/04/08/SRFBN/" rel="prev" title="Feedback Network for Image Super-Resolution论文阅读笔记"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-arrow-left-s-line"></use></svg><span class="post-nav-text">Feedback Network for Image Super-Resolution论文阅读笔记</span></a></div><div class="post-nav-item"><a class="post-nav-next" href="/2020/04/08/LapSRN/" rel="next" title="Deep Laplacian Pyramid Networks for Fast and Accurate Super-Resolution论文阅读笔记"><span class="post-nav-text">Deep Laplacian Pyramid Networks for Fast and Accurate Super-Resolution论文阅读笔记</span><svg class="icon" aria-hidden="true"><use xlink:href="#icon-arrow-right-s-line"></use></svg></a></div></div><div id="comment"><div class="comment-tooltip text-center"><span>若您无 GitHub 账号，可直接在下方匿名评论。</span><br><span>若您想及时得到回复提醒，建议跳转 GitHub Issues 评论。</span><br><span>若没有本文 Issue，您可以使用 Comment 模版新建。</span><br><a class="hty-button hty-button--raised" id="github-issues" href="https://github.com/AlexZou14/alexzou14.github.io/issues?q=is:issue+Guided Super-Resolution as Pixel-to-Pixel Transformation论文阅读笔记" target="_blank" rel="noopener">GitHub Issues</a></div><div class="comment-container" id="valine-container"></div></div><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.css"><script defer src="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.js"></script><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex@latest/dist/contrib/copy-tex.min.css"><script defer src="https://cdn.jsdelivr.net/npm/katex@latest/dist/contrib/copy-tex.min.js"></script><script defer src="https://cdn.jsdelivr.net/npm/katex@latest/dist/contrib/auto-render.min.js" onload="renderMathInElement(document.body);"></script></div></main><footer class="sidebar-translate" id="footer"><div class="copyright"><span>&copy; 2019 – 2021 </span><span class="with-love" id="animate"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-cloud-line"></use></svg></span><span class="author"> 秩同道合</span></div><div class="powered"><span>由 <a href="https://hexo.io" target="_blank" rel="noopener">Hexo</a> 驱动 v4.2.0</span><span class="footer-separator">|</span><span>主题 - <a rel="noopener" href="https://github.com/YunYouJun/hexo-theme-yun" target="_blank"><span>Yun</span></a> v0.3.1</span></div><script defer src="https://cdn.jsdelivr.net/npm/valine@latest/dist/Valine.min.js"></script><script>function initValine() {
  new Valine(CONFIG.valine);
}
document.addEventListener("DOMContentLoaded", function() {
  initValine();
});</script></footer><a class="hty-icon-button" id="goUp" aria-label="back-to-top" href="#"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-arrow-up-s-line"></use></svg></a></div><script defer src="/js/utils.js"></script><script defer src="/js/hexo-theme-yun.js"></script><script src="https://cdn.jsdelivr.net/npm/live2d-widget@^3.1.3/lib/L2Dwidget.min.js"></script><script>L2Dwidget.init({"pluginRootPath":"live2dw/","pluginJsPath":"lib/","pluginModelPath":"assets/","tagMode":false,"debug":false,"model":{"jsonPath":"https://cdn.jsdelivr.net/npm/live2d-widget-model-hijiki@1.0.5/assets/hijiki.model.json"},"display":{"position":"right","width":200,"height":400},"mobile":{"show":true},"react":{"opacity":0.8},"dialog":{"enable":true,"hitokoto":true},"log":false});</script></body></html>