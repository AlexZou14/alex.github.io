<!DOCTYPE html><html lang="zh-CN"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0"><meta name="description" content="Image Super-Resolution Using Deep Convolutional Networks 论文阅读笔记"><meta name="keywords" content="深度学习,笔记,超分辨率"><meta name="author" content="秩同道合"><meta name="copyright" content="秩同道合"><meta name="theme-color" content="#0078E7"><title>Image Super-Resolution Using Deep Convolutional Networks 论文阅读笔记 | 秩同道合的小站</title><link rel="shortcut icon" href="https://cdn.jsdelivr.net/gh/AlexZou14/CDN/img/favicon.ico"><link rel="preload" href="/css/hexo-theme-yun.css" as="style"><link rel="preload" href="/js/utils.js" as="script"><link rel="preload" href="/js/hexo-theme-yun.js" as="script"><link rel="prefetch" href="/js/sidebar.js" as="script"><link rel="preconnect" href="https://cdn.jsdelivr.net" crossorigin><link rel="stylesheet" href="/css/hexo-theme-yun.css"><link rel="alternate" href="/atom.xml" title="秩同道合的小站"><script id="yun-config">
    let Yun = window.Yun || {};
    let CONFIG = {"root":"/","title":"秩同道合的小站","version":"0.3.1","anonymous_image":"https://cdn.jsdelivr.net/gh/YunYouJun/cdn/img/avatar/none.jpg","say":{"api":"https://v1.hitokoto.cn","hitokoto":true},"valine":{"el":"#valine-container","verify":false,"notify":false,"appId":"eCgP91hRSX8OtvCIR4MgLfcl-gzGzoHsz","appKey":"N5gVT8kUx5O0wMvc47SU040Y","serverURLs":null,"placeholder":"大佬求指教&nbsp_(:з」∠)_ （填写邮箱可以收到回复通知～）","avatar":null,"meta":["nick","mail","link"],"pageSize":10,"lang":"zh-cn","visitor":true},"fireworks":{"colors":["102, 167, 221","62, 131, 225","33, 78, 194"]}};
  </script><script src="//at.alicdn.com/t/font_1140697_pa6cswvjpq.js" async></script><meta name="generator" content="Hexo 4.2.0"><link rel="alternate" href="/atom.xml" title="秩同道合的小站" type="application/atom+xml">
</head><body><script defer src="https://cdn.jsdelivr.net/npm/animejs@latest/anime.min.js"></script><script defer src="/js/ui/fireworks.js"></script><canvas class="fireworks"></canvas><div class="container"><a class="sidebar-toggle sidebar-toggle-fixed hty-icon-button"><div class="hamburger hamburger--spin" type="button"><span class="hamburger-box"><span class="hamburger-inner"></span></span></div></a><aside class="sidebar"><ul class="sidebar-nav"><li class="sidebar-nav-item sidebar-nav-toc sidebar-nav-active hty-icon-button" data-target="post-toc-wrap" title="文章目录"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-list-ordered"></use></svg></li><li class="sidebar-nav-item sidebar-nav-overview hty-icon-button" data-target="site-overview-wrap" title="站点概览"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-passport-line"></use></svg></li></ul><div class="sidebar-panel" id="site-overview-wrap"><div class="site-info fix-top"><a class="site-author-avatar" href="/about" title="秩同道合"><img loading="lazy" src="https://cdn.jsdelivr.net/gh/AlexZou14/CDN/img/touxiang.jpg" alt="秩同道合"></a><div class="site-author-name"><a href="/about">秩同道合</a></div><a class="site-name" href="/about/site.html">秩同道合的小站</a><sub class="site-subtitle">寻找志趣相投的伙伴！</sub><div class="site-desciption">我和你，以及我们的秩相同所以我们才等价！</div></div><nav class="site-state"><a class="site-state-item hty-icon-button icon-home" href="/" title="首页"><span class="site-state-item-icon"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-home-4-line"></use></svg></span></a><div class="site-state-item site-state-posts"><a href="/archives" title="归档"><span class="site-state-item-icon"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-archive-line"></use></svg></span><span class="site-state-item-count">24</span></a></div><div class="site-state-item site-state-categories"><a href="/categories" title="分类"><span class="site-state-item-icon"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-folder-2-line"></use></svg></span><span class="site-state-item-count">1</span></a></div><div class="site-state-item site-state-tags"><a href="/tags" title="标签"><span class="site-state-item-icon"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-price-tag-3-line"></use></svg></span><span class="site-state-item-count">3</span></a></div><a class="site-state-item hty-icon-button" href="https://github.com/AlexZou14" target="_blank" rel="noopener" title="reward.comment"><span class="site-state-item-icon"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-hand-coin-line"></use></svg></span></a></nav><hr style="margin-bottom:0.5rem"><div class="links-of-author"><a class="links-of-author-item hty-icon-button" rel="noopener" href="/atom.xml" title="RSS" target="_blank" style="color:orange"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-rss-line"></use></svg></a><a class="links-of-author-item hty-icon-button" rel="noopener" href="https://github.com/AlexZou14" title="GitHub" target="_blank" style="color:#181717"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-github-line"></use></svg></a><a class="links-of-author-item hty-icon-button" rel="noopener" href="mailto:1120375574@qq.com" title="E-Mail" target="_blank" style="color:#8E71C1"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-mail-line"></use></svg></a><a class="links-of-author-item hty-icon-button" rel="noopener" href="https://space.bilibili.com/19164044" title="哔哩哔哩" target="_blank" style="color:#FF8EB3"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-bilibili-line"></use></svg></a></div><hr style="margin:0.5rem 1rem"><div class="links"><a class="links-item hty-icon-button" href="http://sotavision.cn" target="_blank" rel="noopener" title="工作组" style="color:#000000"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-send-plane-2-line"></use></svg></a></div></div><script defer src="/js/sidebar.js"></script><div class="sidebar-panel sidebar-panel-active" id="post-toc-wrap"><div class="post-toc"><div class="post-toc-progress"><div class="progress-bar"></div><div class="progress-info"><span class="progress-notice">您已经读了</span><span class="progress-num">0</span><span class="progress-percentage">%</span></div></div><div class="post-toc-content"><ol class="toc"><li class="toc-item toc-level-3"><a class="toc-link" href="#Abstract"><span class="toc-number">1.</span> <span class="toc-text">Abstract</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Introduction"><span class="toc-number">2.</span> <span class="toc-text">Introduction</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Related-work"><span class="toc-number">3.</span> <span class="toc-text">Related work</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Image-Super-Resolution"><span class="toc-number">3.1.</span> <span class="toc-text">Image Super-Resolution</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Convolutional-Neural-Networks"><span class="toc-number">3.2.</span> <span class="toc-text">Convolutional Neural Networks</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Deep-Learning-for-Image-Restoration"><span class="toc-number">3.3.</span> <span class="toc-text">Deep Learning for Image Restoration</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Proposed-Method"><span class="toc-number">4.</span> <span class="toc-text">Proposed Method</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Network-structure"><span class="toc-number">4.1.</span> <span class="toc-text">Network structure</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Patch-extraction-and-representation"><span class="toc-number">4.2.</span> <span class="toc-text">Patch extraction and representation</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Non-linear-mapping"><span class="toc-number">4.3.</span> <span class="toc-text">Non-linear mapping</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Reconstruction"><span class="toc-number">4.4.</span> <span class="toc-text">Reconstruction</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Relationship-to-Sparse-Coding-Based-Methods"><span class="toc-number">4.5.</span> <span class="toc-text">Relationship to Sparse-Coding-Based Methods</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#loss-function"><span class="toc-number">4.6.</span> <span class="toc-text">loss function</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Experiment"><span class="toc-number">5.</span> <span class="toc-text">Experiment</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Experiments-Details"><span class="toc-number">5.1.</span> <span class="toc-text">Experiments Details</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Network-analysis"><span class="toc-number">5.2.</span> <span class="toc-text">Network analysis</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#Filter-number"><span class="toc-number">5.2.1.</span> <span class="toc-text">Filter number</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#Filter-size"><span class="toc-number">5.2.2.</span> <span class="toc-text">Filter size</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#Number-of-layers"><span class="toc-number">5.2.3.</span> <span class="toc-text">Number of layers</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Comparisons-with-state-of-the-arts"><span class="toc-number">5.3.</span> <span class="toc-text">Comparisons with state-of-the-arts</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Conclusions"><span class="toc-number">6.</span> <span class="toc-text">Conclusions</span></a></li></ol></div></div></div></aside><main class="sidebar-translate" id="content"><div id="post"><article class="post-block" itemscope itemtype="http://schema.org/Article"><link itemprop="mainEntityOfPage" href="http://alexzou14.github.io/2020/04/30/SRCNN/"><span hidden itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="name" content="秩同道合"><meta itemprop="description" content="Image Super-Resolution Using Deep Convolutional Networks 论文阅读笔记"></span><span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="秩同道合的小站"></span><header class="post-header"><h1 class="post-title" itemprop="name headline">Image Super-Resolution Using Deep Convolutional Networks 论文阅读笔记</h1><div class="post-meta"><span class="post-time"><span class="post-meta-item-icon"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-calendar-line"></use></svg></span> <time title="创建时间：2020-04-30 11:55:28" itemprop="dateCreated datePublished" datetime="2020-04-30T11:55:28+08:00">2020-04-30</time></span><div class="post-count"><span class="post-symbolcount"><span class="post-meta-item-icon" title="本文字数"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-file-word-line"></use></svg></span> <span title="本文字数">7.4k</span><span class="post-meta-divider">-</span><span class="post-meta-item-icon" title="阅读时长"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-timer-line"></use></svg></span> <span title="阅读时长">15 分钟</span></span></div><span class="leancloud_visitors" id="/2020/04/30/SRCNN/" data-flag-title="Image Super-Resolution Using Deep Convolutional Networks 论文阅读笔记"><span class="post-meta-item-icon" title="阅读次数"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-eye-line"></use></svg> <span class="leancloud-visitors-count"></span></span></span><div class="post-classify"><span class="post-category"><span class="post-meta-item-icon"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-folder-line"></use></svg></span> <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a class="category" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/" itemprop="url" rel="index"><span itemprop="text">论文笔记</span></a></span></span><span class="post-tag"><span class="post-meta-divider">-</span><a class="tag" href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"><span class="post-meta-item-icon"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-price-tag-3-line"></use></svg></span><span class="tag-name">深度学习</span></a><a class="tag" href="/tags/%E7%AC%94%E8%AE%B0/"><span class="post-meta-item-icon"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-price-tag-3-line"></use></svg></span><span class="tag-name">笔记</span></a><a class="tag" href="/tags/%E8%B6%85%E5%88%86%E8%BE%A8%E7%8E%87/"><span class="post-meta-item-icon"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-price-tag-3-line"></use></svg></span><span class="tag-name">超分辨率</span></a></span></div></div></header><section class="post-body" itemprop="articleBody"><div class="post-content post-markdown"><h3 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h3><p>作者提出了一种基于深度学习的单影像超分辨率重建方法。我们直接以端对端的方式学习高分辨率影像和低分辨率影像之间的mapping。Mapping可以用一个深度卷积神经网络来表示，通过输入低分辨率的影像输出高分辨率的影像。</p>
<hr>
<h3 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h3><p>当时，单幅图像的超分辨率重建大多都是基于样本学习的，如稀疏编码就是典型的方法之一。这种方法一般先对图像进行特征提取，然后编码成一个低分辨率字典，稀疏系数传到高分辨率字典中重建高分辨率部分，然后将这些部分汇聚作为输出。以往的SR方法都关注学习和优化字典或者建立模型，很少去优化或者考虑统一的优化框架。<br>为了解决上述问题，本文中提出了一种深度卷积神经网络（SRCNN），即一种LR到HR的端对端映射，具有如下性质：</p>
<ol>
<li>结构简单，与其他现有方法相比具有优越的正确性，对比结果如下：</li>
<li>滤波器和层的数量适中，即使在CPU上运行速度也比较快，因为它是一个前馈网络，而且在使用时不用管优化问题；</li>
<li>实验证明，该网络的复原质量可以在大的数据集或者大的模型中进一步提高。</li>
</ol>
<p>本文的主要贡献：</p>
<ol>
<li>我们提出了一个卷积神经网络用于图像超分辨率重建，这个网络直接学习LR到HR图像之间端对端映射，几乎没有优化后的前后期处理。</li>
<li>将深度学习的SR方法与基于传统的稀疏编码相结合，为网络结构的设计提供指导。</li>
<li>深度学习在超分辨率问题上能取得较好的质量和速度。</li>
</ol>
<hr>
<h3 id="Related-work"><a href="#Related-work" class="headerlink" title="Related work"></a>Related work</h3><h4 id="Image-Super-Resolution"><a href="#Image-Super-Resolution" class="headerlink" title="Image Super-Resolution"></a>Image Super-Resolution</h4><p>指通过低分辨率图像或图像序列恢复出高分辨率图像。高分辨率图像意味着图像具有更多的细节信息、更细腻的画质,，这些细节在高清电视、医学成像、遥感卫星成像等领域有着重要的应用价值。</p>
<h4 id="Convolutional-Neural-Networks"><a href="#Convolutional-Neural-Networks" class="headerlink" title="Convolutional Neural Networks"></a>Convolutional Neural Networks</h4><p>许多因素是CNN风靡的关键原因：</p>
<ol>
<li>在现在的GPU高效的训练 </li>
<li>矫正线性单元RELU的提出使得在同样实现高精度的同时收敛更快；</li>
<li>对于训练大模型获取的丰富数据集更加简单。我们的方法同样收益与这些成就。</li>
</ol>
<h4 id="Deep-Learning-for-Image-Restoration"><a href="#Deep-Learning-for-Image-Restoration" class="headerlink" title="Deep Learning for Image Restoration"></a>Deep Learning for Image Restoration</h4><p>有很多的深度学习方法用于影像的恢复。多层感知机（MLP）所有的图层都是全连接的（与卷积相反），应用于自然影像的降噪和后模糊降噪。</p>
<hr>
<h3 id="Proposed-Method"><a href="#Proposed-Method" class="headerlink" title="Proposed Method"></a>Proposed Method</h3><h4 id="Network-structure"><a href="#Network-structure" class="headerlink" title="Network structure"></a>Network structure</h4><blockquote>
<p><img src="http://sotavision.cn/showdoc/server/../Public/Uploads/2020-01-22/5e27ffd852093.png" alt="" loading="lazy"></p>
</blockquote>
<p>使用双三次插值将单幅低分辨率图像变成我们想要的大小，假设这个内插值的图像为Y,我们的目标是从Y中恢复图像F(Y)使之尽可能与高分辨率图像X相似，为了便于区分，我们仍然把Y称为低分辨率图像，尽管它与X大小相同，我们希望学习到这个映射函数F，需要以下三部：</p>
<h4 id="Patch-extraction-and-representation"><a href="#Patch-extraction-and-representation" class="headerlink" title="Patch extraction and representation"></a>Patch extraction and representation</h4><p>这个操作将一个高维向量映射到另一个高维向量，每一个映射向量表示一个高分辨率patch,这些向量组成另一个特征映射。<br>第一层定义为函数\(F_1\)：<br>\[F_1(Y) = \max(0,W_1 \times Y + B_1)\]<br>其中，\(W_1\)和\(B_1\)分别代表滤波器和偏差，\(W_1\)的大小为\(c \times f_1 \times f_1 \times n_1\), c是输入图像的通道数，\(f_1\)是滤波器的空间大小，\(n_1\)是滤波器的数量。从直观上看，\(W_1\)使用\(n_1\)个卷积，每个卷积核大小为\(c \times f_1 \times f_1\)。输出是\(n_1\)给特征映射。\(B_1\)是一个\(n_1\)维的向量，每个元素都与一个滤波器有关，在滤波器响应中使用Rectiﬁed Linear Unit (ReLU,\(\max(0,x)\)) </p>
<h4 id="Non-linear-mapping"><a href="#Non-linear-mapping" class="headerlink" title="Non-linear mapping"></a>Non-linear mapping</h4><p>这个操作将一个高维向量映射到另一个高维向量，每一个映射向量表示一个高分辨率patch,这些向量组成另一个特征映射。<br>第二步将\(n_1\)维的向量映射到\(n_2\)维，这相当于使用\(n_2\)个\(1 \times 1\)的滤波器，第二层的操作如下：<br>\[F_2(Y) = max(0,W_2 \times F_1(Y) + B_2)\]<br>其中，\(W_2\)的大小为\(n_1*1*1*n_2\)，\(B_2\)是\(n_2\)维的向量，每个输出的\(n_2\)维向量都表示一个高分辨率块（patch）用于后续的重建。<br>当然，也可以添加更多的卷积层（1*1的）来添加非线性特征，但会增加模型的复杂度，也需要更多的训练数据和时间，在本文中，我们采用单一的卷积层，因为它已经能取得较好的效果。</p>
<h4 id="Reconstruction"><a href="#Reconstruction" class="headerlink" title="Reconstruction"></a>Reconstruction</h4><p>这个操作汇聚所有的高分辨率patch构成最够的高分辨率图像，我们期望这个图像能与X相似。<br>在传统的方法中，预测的重叠高分辨率块经常取平均后得到最后的图像，这个平均化可以看作是预先定义好的用于一系列特征映射的滤波器（每个位置都是高分辨率块的“扁平”矢量形式），因此，我们定义一个卷积层产生最后的超分辨率图像：<br>\[F(Y) = W_3 \times F_2(Y) + B_3\]<br>W3的大小为\(n_2 \times f_3 \times f_3 \times c\)，\(B_3\)是一个c维向量。<br>如果这个高分辨率块都在图像域，我们把这个滤波器当成均值滤波器；如果这些高分辨率块在其他域，则\(W_3\)首先将系数投影到图像域然后再做均值，无论哪种情况，\(W_3\)都是一个线性滤波器。 </p>
<h4 id="Relationship-to-Sparse-Coding-Based-Methods"><a href="#Relationship-to-Sparse-Coding-Based-Methods" class="headerlink" title="Relationship to Sparse-Coding-Based Methods"></a>Relationship to Sparse-Coding-Based Methods</h4><p>基于稀疏编码的图像超分辨率方法也可以看作是一个卷积神经网络，如图：</p>
<blockquote>
<p><img src="http://sotavision.cn/showdoc/server/../Public/Uploads/2020-01-22/5e2802fc8daf9.png" alt="" loading="lazy"></p>
</blockquote>
<p>上图展示了基于稀疏编码的SR方法可以看成是一种卷积神经网络（非线性映射不同），但在稀疏编码中，被不是所有的操作都有优化，而卷积神经网络中，低分辨率字典、高分辨率字典、非线性映射，以及减去均值和求平均值等经过滤波器进行优化，所以我们的方法是一种端对端的映射。</p>
<h4 id="loss-function"><a href="#loss-function" class="headerlink" title="loss function"></a>loss function</h4><p>学习端对端的映射函数F需要评估以下参数：\(\theta =\{W_1,W_2,W_3,B_1,B_2,B_3\}\)。最小化重建函数\(F(Y;\theta)\) 与对于的高分辨率图像X之间的损失，给出一组高分辨率图像 \(\{X_i\}\) 和对应得低分辨率图像 \(\{Y_i\}\)，使用 均方误差（Mean Squared Error，MSE)作为损失函数：</p>
<blockquote>
<p><img src="http://sotavision.cn/showdoc/server/../Public/Uploads/2020-01-22/5e28044c70151.png" alt="" loading="lazy"></p>
</blockquote>
<hr>
<h3 id="Experiment"><a href="#Experiment" class="headerlink" title="Experiment"></a>Experiment</h3><h4 id="Experiments-Details"><a href="#Experiments-Details" class="headerlink" title="Experiments Details"></a>Experiments Details</h4><p>论文官方代码：<a href="http://mmlab.ie.cuhk.edu.hk/projects/SRCNN.html" target="_blank" rel="noopener">http://mmlab.ie.cuhk.edu.hk/projects/SRCNN.html</a></p>
<h4 id="Network-analysis"><a href="#Network-analysis" class="headerlink" title="Network analysis"></a>Network analysis</h4><h5 id="Filter-number"><a href="#Filter-number" class="headerlink" title="Filter number"></a>Filter number</h5><blockquote>
<p><img src="http://sotavision.cn/showdoc/server/../Public/Uploads/2020-01-22/5e2805178eb8a.png" alt="" loading="lazy"></p>
</blockquote>
<h5 id="Filter-size"><a href="#Filter-size" class="headerlink" title="Filter size"></a>Filter size</h5><blockquote>
<p><img src="http://sotavision.cn/showdoc/server/../Public/Uploads/2020-01-22/5e2805412afe6.png" alt="" loading="lazy"></p>
</blockquote>
<h5 id="Number-of-layers"><a href="#Number-of-layers" class="headerlink" title="Number of layers"></a>Number of layers</h5><blockquote>
<p><img src="http://sotavision.cn/showdoc/server/../Public/Uploads/2020-01-22/5e280574a57f2.png" alt="" loading="lazy"><br><img src="http://sotavision.cn/showdoc/server/../Public/Uploads/2020-01-22/5e28058918dc4.png" alt="" loading="lazy"></p>
</blockquote>
<h4 id="Comparisons-with-state-of-the-arts"><a href="#Comparisons-with-state-of-the-arts" class="headerlink" title="Comparisons with state-of-the-arts"></a>Comparisons with state-of-the-arts</h4><blockquote>
<p><img src="http://sotavision.cn/showdoc/server/../Public/Uploads/2020-01-22/5e2805cb6b141.png" alt="" loading="lazy"></p>
</blockquote>
<hr>
<h3 id="Conclusions"><a href="#Conclusions" class="headerlink" title="Conclusions"></a>Conclusions</h3><ul>
<li>提出了一种新的深度学习方法用于单幅图像的超分辨率重建，传统的基于稀疏编码的方法可以看作一个深的卷积神经网络。</li>
<li>提出的SRCNN方法是一种在LR和HR图像之间的端对端映射，在优化时几乎不需要额外的预处理和后处理，结构也比较简单，比以往的方法都要好。</li>
<li>推测通过采取更多的滤波器和不同的训练策略可以获得另外的表现效果。</li>
</ul>
</div><ul class="post-copyright"><li class="post-copyright-author"><strong>本文作者：</strong>秩同道合</li><li class="post-copyright-link"><strong>本文链接：</strong><a href="http://alexzou14.github.io/2020/04/30/SRCNN/" title="Image Super-Resolution Using Deep Convolutional Networks 论文阅读笔记">http://alexzou14.github.io/2020/04/30/SRCNN/</a></li><li class="post-copyright-license"><strong>版权声明：</strong>本博客所有文章除特别声明外，均默认采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh" target="_blank" rel="noopener" title="CC BY-NC-SA 4.0 "><svg class="icon"><use xlink:href="#icon-creative-commons-line"></use></svg><svg class="icon"><use xlink:href="#icon-creative-commons-by-line"></use></svg><svg class="icon"><use xlink:href="#icon-creative-commons-nc-line"></use></svg><svg class="icon"><use xlink:href="#icon-creative-commons-sa-line"></use></svg></a> 许可协议。</li></ul></section></article><div class="post-nav"><div class="post-nav-item"><a class="post-nav-prev" href="/2020/04/30/SRDenseNet/" rel="prev" title="Image Super-Resolution Using Dense Skip Connections 论文阅读笔记"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-arrow-left-s-line"></use></svg><span class="post-nav-text">Image Super-Resolution Using Dense Skip Connections 论文阅读笔记</span></a></div><div class="post-nav-item"><a class="post-nav-next" href="/2020/04/30/PFNL/" rel="next" title="Progressive Fusion Video Super-Resolution Network via Exploiting Non-Local Spatio-Temporal Correlations论文阅读笔记"><span class="post-nav-text">Progressive Fusion Video Super-Resolution Network via Exploiting Non-Local Spatio-Temporal Correlations论文阅读笔记</span><svg class="icon" aria-hidden="true"><use xlink:href="#icon-arrow-right-s-line"></use></svg></a></div></div><div id="comment"><div class="comment-tooltip text-center"><span>若您无 GitHub 账号，可直接在下方匿名评论。</span><br><span>若您想及时得到回复提醒，建议跳转 GitHub Issues 评论。</span><br><span>若没有本文 Issue，您可以使用 Comment 模版新建。</span><br><a class="hty-button hty-button--raised" id="github-issues" href="https://github.com/AlexZou14/alexzou14.github.io/issues?q=is:issue+Image Super-Resolution Using Deep Convolutional Networks 论文阅读笔记" target="_blank" rel="noopener">GitHub Issues</a></div><div class="comment-container" id="valine-container"></div></div><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.css"><script defer src="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.js"></script><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex@latest/dist/contrib/copy-tex.min.css"><script defer src="https://cdn.jsdelivr.net/npm/katex@latest/dist/contrib/copy-tex.min.js"></script><script defer src="https://cdn.jsdelivr.net/npm/katex@latest/dist/contrib/auto-render.min.js" onload="renderMathInElement(document.body);"></script></div></main><footer class="sidebar-translate" id="footer"><div class="copyright"><span>&copy; 2019 – 2020 </span><span class="with-love" id="animate"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-cloud-line"></use></svg></span><span class="author"> 秩同道合</span></div><div class="powered"><span>由 <a href="https://hexo.io" target="_blank" rel="noopener">Hexo</a> 驱动 v4.2.0</span><span class="footer-separator">|</span><span>主题 - <a rel="noopener" href="https://github.com/YunYouJun/hexo-theme-yun" target="_blank"><span>Yun</span></a> v0.3.1</span></div><script defer src="https://cdn.jsdelivr.net/npm/valine@latest/dist/Valine.min.js"></script><script>function initValine() {
  new Valine(CONFIG.valine);
}
document.addEventListener("DOMContentLoaded", function() {
  initValine();
});</script></footer><a class="hty-icon-button" id="goUp" aria-label="back-to-top" href="#"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-arrow-up-s-line"></use></svg></a></div><script defer src="/js/utils.js"></script><script defer src="/js/hexo-theme-yun.js"></script><script src="https://cdn.jsdelivr.net/npm/live2d-widget@^3.1.3/lib/L2Dwidget.min.js"></script><script>L2Dwidget.init({"pluginRootPath":"live2dw/","pluginJsPath":"lib/","pluginModelPath":"assets/","tagMode":false,"debug":false,"model":{"jsonPath":"https://cdn.jsdelivr.net/npm/live2d-widget-model-hijiki@1.0.5/assets/hijiki.model.json"},"display":{"position":"right","width":200,"height":400},"mobile":{"show":true},"react":{"opacity":0.8},"dialog":{"enable":true,"hitokoto":true},"log":false});</script></body></html>