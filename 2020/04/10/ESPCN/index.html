<!DOCTYPE html><html lang="zh-CN"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0"><meta name="description" content="Real-Time Single Image and Video Super-Resolution Using an EfficientSub-Pixel Convolutional Neural Network论文阅读笔记"><meta name="keywords" content="深度学习,笔记,超分辨率"><meta name="author" content="秩同道合"><meta name="copyright" content="秩同道合"><meta name="theme-color" content="#0078E7"><title>Real-Time Single Image and Video Super-Resolution Using an EfficientSub-Pixel Convolutional Neural Network论文阅读笔记 | 秩同道合的小站</title><link rel="shortcut icon" href="https://cdn.jsdelivr.net/gh/AlexZou14/CDN/img/favicon.ico"><link rel="preload" href="/css/hexo-theme-yun.css" as="style"><link rel="preload" href="/js/utils.js" as="script"><link rel="preload" href="/js/hexo-theme-yun.js" as="script"><link rel="prefetch" href="/js/sidebar.js" as="script"><link rel="preconnect" href="https://cdn.jsdelivr.net" crossorigin><link rel="stylesheet" href="/css/hexo-theme-yun.css"><link rel="alternate" href="/atom.xml" title="秩同道合的小站"><script id="yun-config">
    let Yun = window.Yun || {};
    let CONFIG = {"root":"/","title":"秩同道合的小站","version":"0.3.1","anonymous_image":"https://cdn.jsdelivr.net/gh/YunYouJun/cdn/img/avatar/none.jpg","say":{"api":"https://v1.hitokoto.cn","hitokoto":true},"valine":{"el":"#valine-container","verify":false,"notify":false,"appId":"eCgP91hRSX8OtvCIR4MgLfcl-gzGzoHsz","appKey":"N5gVT8kUx5O0wMvc47SU040Y","serverURLs":null,"placeholder":"大佬求指教&nbsp_(:з」∠)_ （填写邮箱可以收到回复通知～）","avatar":null,"meta":["nick","mail","link"],"pageSize":10,"lang":"zh-cn","visitor":true},"fireworks":{"colors":["102, 167, 221","62, 131, 225","33, 78, 194"]}};
  </script><script src="//at.alicdn.com/t/font_1140697_pa6cswvjpq.js" async></script><meta name="generator" content="Hexo 4.2.0"><link rel="alternate" href="/atom.xml" title="秩同道合的小站" type="application/atom+xml">
</head><body><script defer src="https://cdn.jsdelivr.net/npm/animejs@latest/anime.min.js"></script><script defer src="/js/ui/fireworks.js"></script><canvas class="fireworks"></canvas><div class="container"><a class="sidebar-toggle sidebar-toggle-fixed hty-icon-button"><div class="hamburger hamburger--spin" type="button"><span class="hamburger-box"><span class="hamburger-inner"></span></span></div></a><aside class="sidebar"><ul class="sidebar-nav"><li class="sidebar-nav-item sidebar-nav-toc sidebar-nav-active hty-icon-button" data-target="post-toc-wrap" title="文章目录"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-list-ordered"></use></svg></li><li class="sidebar-nav-item sidebar-nav-overview hty-icon-button" data-target="site-overview-wrap" title="站点概览"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-passport-line"></use></svg></li></ul><div class="sidebar-panel" id="site-overview-wrap"><div class="site-info fix-top"><a class="site-author-avatar" href="/about" title="秩同道合"><img loading="lazy" src="https://cdn.jsdelivr.net/gh/AlexZou14/CDN/img/touxiang.jpg" alt="秩同道合"></a><div class="site-author-name"><a href="/about">秩同道合</a></div><a class="site-name" href="/about/site.html">秩同道合的小站</a><sub class="site-subtitle">寻找志趣相投的伙伴！</sub><div class="site-desciption">我和你，以及我们的秩相同所以我们才等价！</div></div><nav class="site-state"><a class="site-state-item hty-icon-button icon-home" href="/" title="首页"><span class="site-state-item-icon"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-home-4-line"></use></svg></span></a><div class="site-state-item site-state-posts"><a href="/archives" title="归档"><span class="site-state-item-icon"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-archive-line"></use></svg></span><span class="site-state-item-count">23</span></a></div><div class="site-state-item site-state-categories"><a href="/categories" title="分类"><span class="site-state-item-icon"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-folder-2-line"></use></svg></span><span class="site-state-item-count">1</span></a></div><div class="site-state-item site-state-tags"><a href="/tags" title="标签"><span class="site-state-item-icon"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-price-tag-3-line"></use></svg></span><span class="site-state-item-count">3</span></a></div><a class="site-state-item hty-icon-button" href="https://github.com/AlexZou14" target="_blank" rel="noopener" title="reward.comment"><span class="site-state-item-icon"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-hand-coin-line"></use></svg></span></a></nav><hr style="margin-bottom:0.5rem"><div class="links-of-author"><a class="links-of-author-item hty-icon-button" rel="noopener" href="/atom.xml" title="RSS" target="_blank" style="color:orange"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-rss-line"></use></svg></a><a class="links-of-author-item hty-icon-button" rel="noopener" href="https://github.com/AlexZou14" title="GitHub" target="_blank" style="color:#181717"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-github-line"></use></svg></a><a class="links-of-author-item hty-icon-button" rel="noopener" href="mailto:1120375574@qq.com" title="E-Mail" target="_blank" style="color:#8E71C1"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-mail-line"></use></svg></a><a class="links-of-author-item hty-icon-button" rel="noopener" href="https://space.bilibili.com/19164044" title="哔哩哔哩" target="_blank" style="color:#FF8EB3"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-bilibili-line"></use></svg></a></div><hr style="margin:0.5rem 1rem"><div class="links"><a class="links-item hty-icon-button" href="http://sotavision.cn" target="_blank" rel="noopener" title="工作组" style="color:#000000"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-send-plane-2-line"></use></svg></a></div></div><script defer src="/js/sidebar.js"></script><div class="sidebar-panel sidebar-panel-active" id="post-toc-wrap"><div class="post-toc"><div class="post-toc-progress"><div class="progress-bar"></div><div class="progress-info"><span class="progress-notice">您已经读了</span><span class="progress-num">0</span><span class="progress-percentage">%</span></div></div><div class="post-toc-content"><ol class="toc"><li class="toc-item toc-level-3"><a class="toc-link" href="#Abstract"><span class="toc-number">1.</span> <span class="toc-text">Abstract</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Introduction"><span class="toc-number">2.</span> <span class="toc-text">Introduction</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Related-work"><span class="toc-number">3.</span> <span class="toc-text">Related work</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Proposed-Method"><span class="toc-number">4.</span> <span class="toc-text">Proposed Method</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Network-structure"><span class="toc-number">4.1.</span> <span class="toc-text">Network structure</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Efficient-sub-pixel-convolution-layer"><span class="toc-number">4.2.</span> <span class="toc-text">Efficient sub-pixel convolution layer</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Loss-function"><span class="toc-number">4.3.</span> <span class="toc-text">Loss function</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Experiment"><span class="toc-number">5.</span> <span class="toc-text">Experiment</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Comparison-to-the-state-of-the-art"><span class="toc-number">5.1.</span> <span class="toc-text">Comparison to the state-of-the-art</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Run-time-evaluations"><span class="toc-number">5.2.</span> <span class="toc-text">Run time evaluations</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Conclusions"><span class="toc-number">6.</span> <span class="toc-text">Conclusions</span></a></li></ol></div></div></div></aside><main class="sidebar-translate" id="content"><div id="post"><article class="post-block" itemscope itemtype="http://schema.org/Article"><link itemprop="mainEntityOfPage" href="http://alexzou14.github.io/2020/04/10/ESPCN/"><span hidden itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="name" content="秩同道合"><meta itemprop="description" content="Real-Time Single Image and Video Super-Resolution Using an EfficientSub-Pixel Convolutional Neural Network论文阅读笔记"></span><span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="秩同道合的小站"></span><header class="post-header"><h1 class="post-title" itemprop="name headline">Real-Time Single Image and Video Super-Resolution Using an EfficientSub-Pixel Convolutional Neural Network论文阅读笔记</h1><div class="post-meta"><span class="post-time"><span class="post-meta-item-icon"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-calendar-line"></use></svg></span> <time title="创建时间：2020-04-10 20:55:35" itemprop="dateCreated datePublished" datetime="2020-04-10T20:55:35+08:00">2020-04-10</time></span><div class="post-count"><span class="post-symbolcount"><span class="post-meta-item-icon" title="本文字数"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-file-word-line"></use></svg></span> <span title="本文字数">5k</span><span class="post-meta-divider">-</span><span class="post-meta-item-icon" title="阅读时长"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-timer-line"></use></svg></span> <span title="阅读时长">10 分钟</span></span></div><span class="leancloud_visitors" id="/2020/04/10/ESPCN/" data-flag-title="Real-Time Single Image and Video Super-Resolution Using an EfficientSub-Pixel Convolutional Neural Network论文阅读笔记"><span class="post-meta-item-icon" title="阅读次数"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-eye-line"></use></svg> <span class="leancloud-visitors-count"></span></span></span><div class="post-classify"><span class="post-category"><span class="post-meta-item-icon"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-folder-line"></use></svg></span> <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a class="category" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/" itemprop="url" rel="index"><span itemprop="text">论文笔记</span></a></span></span><span class="post-tag"><span class="post-meta-divider">-</span><a class="tag" href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"><span class="post-meta-item-icon"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-price-tag-3-line"></use></svg></span><span class="tag-name">深度学习</span></a><a class="tag" href="/tags/%E7%AC%94%E8%AE%B0/"><span class="post-meta-item-icon"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-price-tag-3-line"></use></svg></span><span class="tag-name">笔记</span></a><a class="tag" href="/tags/%E8%B6%85%E5%88%86%E8%BE%A8%E7%8E%87/"><span class="post-meta-item-icon"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-price-tag-3-line"></use></svg></span><span class="tag-name">超分辨率</span></a></span></div></div></header><section class="post-body" itemprop="articleBody"><div class="post-content post-markdown"><h3 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h3><p>近来，基于深层神经网络的几种模型在单像超分辨率的重构精度和计算性能方面取得了巨大的成功。在这些方法中，低分辨率（LR）输入图像在重建之前使用单个滤波器（通常是双三次插值）被放大到高分辨率（HR）空间再输入到网络中重建，这样会增加了计算复杂度。在本文中，作者提出了第一个能够实现1080p视频实时SR的卷积神经网络。为了实现这一点，我们提出了一种新颖的CNN架构，其中在LR空间中提取特征图。另外，我们引入了一个有效的亚像素卷积层，该层学习了一个升序滤波器阵列，将最终的LR特征图升级到HR输出。通过这样做，我们有效地更换SR管道中的手工双三次插值滤波器，并为每个特征图进行了专门训练的更复杂的升频滤波器，同时还降低了整个SR操作的计算复杂度。</p>
<hr>
<h3 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h3><p>全局SR问题假设LR数据是HR数据的低通滤波（模糊），下采样和噪声版本。 由于在不可逆低通滤波和子采样期间发生的高频信息的丢失，这是一个高度ill-posed的问题。 此外，SR操作实际上是从LR到HR空间的一对多映射，其可以具有多个解决方案，其中确定正确的解决方案是不容易的。 基于许多SR技术的一个关键假设是大部分高频数据是冗余的，因此可以从低频分量精确地重构。因此，SR是一个推理问题，因此依赖于图像的统计信息。</p>
<hr>
<h3 id="Related-work"><a href="#Related-work" class="headerlink" title="Related work"></a>Related work</h3><p>许多方法假定多个图像可看作具有不同视角的相同场景的LR实例，即具有独特的先前仿射变换。这些可以被分类为多图像SR方法，并且通过基于附加信息来限制ill-posed问题并尝试反转下采样过程的方式来利用显式冗余信息。然而，这些方法通常需要计算复杂的图像配准和融合阶段，其准确性直接影响结果的质量。另一个方法是单图像超分辨率（SISR）技术。这些技术寻求学习自然数据中存在的隐性冗余，以从单个LR实例中恢复丢失的HR信息。这通常以图像的局部空间相关性和视频中的附加时间相关性的形式出现。在这种情况下，需要以重建约束的形式的先验信息来限制重构的解空间。<br>本文主要贡献为：<br>1.upscaling由network最后一层处理，也就表示每各LR图像可以从network中之间得到反馈并在LR空间里中进行特种提取。（减少计算和存储器的复杂度）</p>
<p>2.对于具有L层的网络，我们学习nL-1特征映射的nL-1升级滤波器，而不是输入图像的一个升频滤波器。此外，不使用显式插值滤波器意味着网络隐式地学习SR所需的处理。因此，与在第一层的单个固定滤波器放大相比，网络能够学习更好和更复杂的LR到HR映射。这导致模型的重建精度的额外增益</p>
<hr>
<h3 id="Proposed-Method"><a href="#Proposed-Method" class="headerlink" title="Proposed Method"></a>Proposed Method</h3><h4 id="Network-structure"><a href="#Network-structure" class="headerlink" title="Network structure"></a>Network structure</h4><blockquote>
<p><img src="http://sotavision.cn/showdoc/server/../Public/Uploads/2020-04-10/5e90616e8620f.png" alt="" loading="lazy"></p>
</blockquote>
<p>ESPCN的核心概念是亚像素卷积层(sub-pixel convolutional layer)。如上图所示，网络的输入是原始低分辨率图像，通过两个卷积层以后，得到的特征图像大小与输入图像一样，但是特征通道为\(r^2\)（r是图像的目标放大倍数)。将每个像素的个通道\(r^2\)重新排列成一个r x r的区域，对应于高分辨率图像中的一个r x r大小的子块，从而大小为\( r^2 \times H \times W \) 的特征图像被重新排列成\( 1 \times rH \times rW \)大小的高分辨率图像。这个变换虽然被称作sub-pixel convolution, 但实际上并没有卷积操作。通过使用sub-pixel convolution, 图像从低分辨率到高分辨率放大的过程，插值函数被隐含地包含在前面的卷积层中，可以自动学习到。只在最后一层对图像大小做变换，前面的卷积运算由于在低分辨率图像上进行，因此效率会较高。</p>
<p>该网络可以用数学公式表达为：<br>\[ f^1(I^{LR};W_1,b_1)=\phi(W_1 \ast I^{LR}+b_1) \]<br>\[ f^l(I^{LR};W_{1:l},b_{1:l})=\phi(W_l \ast f^{l-1}(I^{LR})+b_l) \]</p>
<h4 id="Efficient-sub-pixel-convolution-layer"><a href="#Efficient-sub-pixel-convolution-layer" class="headerlink" title="Efficient sub-pixel convolution layer"></a>Efficient sub-pixel convolution layer</h4><p>deconvolution, FSRCNN在最后的上采样层，通过学习最后的deconvolution layer。但deconvolution本质上是可以看做一种特殊的卷积，理论上后面要通过stack filters才能使得性能有更大的提升。本文提到的亚像素卷积sub-pixel Layer，其实跟常规的卷积层没有什么不同，不同的是其输出的特征通道数为r^2，其中r为缩放倍数。表达式为：<br>\[ I^{SR}=f^L(I^{LR})=\mathcal{PS}(W_L \ast f^{L-1}(I^{LR})+b_L) \]</p>
<p>可从上面的公式看到，所得的高分辨率图像是通过PS操作，将r^2维度的低分辨率特征重新排列成高分辨率图像。其中PS操作称之为： periodic shuffling<br>本质上就是将低分辨率特征，按照特定位置，周期性的插入到高分辨率图像中，可以通过颜色观测到上图的插入方式。可以表达为：</p>
<div>
    \[ \mathcal{PS}(T)_{x,y,c}=T_{\left \lfloor x/r \right\rfloor,\left \lfloor y/r \right\rfloor, C \cdot r \cdot mod(y,r)+C \cdot mod(x,r)+c} \]
</div>

<h4 id="Loss-function"><a href="#Loss-function" class="headerlink" title="Loss function"></a>Loss function</h4><p>文章用了MSE loss优化网络：<br>\[ l(W_{1:l},b_{1:L})=\dfrac{1}{r^2HW}\displaystyle\sum_{x=1}^{rH}\sum_{x=1}^{rW}(I_{x,y}^{HR}-f_{x,y}^L(I^{LR}))^2 \]</p>
<hr>
<h3 id="Experiment"><a href="#Experiment" class="headerlink" title="Experiment"></a>Experiment</h3><h4 id="Comparison-to-the-state-of-the-art"><a href="#Comparison-to-the-state-of-the-art" class="headerlink" title="Comparison to the state-of-the-art"></a>Comparison to the state-of-the-art</h4><p>图像质量：</p>
<blockquote>
<p><img src="http://sotavision.cn/showdoc/server/../Public/Uploads/2020-04-10/5e906a516211f.png" alt="" loading="lazy"></p>
</blockquote>
<p>重建质量也是能够达到当时最优的。</p>
<h4 id="Run-time-evaluations"><a href="#Run-time-evaluations" class="headerlink" title="Run time evaluations"></a>Run time evaluations</h4><p>运行速度:</p>
<blockquote>
<p><img src="http://sotavision.cn/showdoc/server/../Public/Uploads/2020-04-10/5e906ad840ea9.png" alt="" loading="lazy"></p>
</blockquote>
<p>速度上是相当快的，基本上能在视频上进行实时处理</p>
<hr>
<h3 id="Conclusions"><a href="#Conclusions" class="headerlink" title="Conclusions"></a>Conclusions</h3><ol>
<li>作者提出了ESPCN模型，并达到了当时最先进的性能。</li>
<li>证明了第一层的固定滤波器升级不能为SISR提供任何额外信息，但需要更多的计算复杂性。</li>
<li>提出了一种新颖的亚像素卷积层，与去卷积层相比，它能够以非常小的额外计算成本将LR数据超解析为HR空间。 在升级因子为4的扩展基准标记数据集上进行的评估表明，与之前的CNN方法相比，我们具有显着的速度（&gt; 10倍）和性能（图像上的+ 0.15dB和视频上的+ 0.39dB）增强。 这使我们的模型成为第一个能够在单个GPU上实时生成SR高清视频的CNN模型。</li>
</ol>
</div><ul class="post-copyright"><li class="post-copyright-author"><strong>本文作者：</strong>秩同道合</li><li class="post-copyright-link"><strong>本文链接：</strong><a href="http://alexzou14.github.io/2020/04/10/ESPCN/" title="Real-Time Single Image and Video Super-Resolution Using an EfficientSub-Pixel Convolutional Neural Network论文阅读笔记">http://alexzou14.github.io/2020/04/10/ESPCN/</a></li><li class="post-copyright-license"><strong>版权声明：</strong>本博客所有文章除特别声明外，均默认采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh" target="_blank" rel="noopener" title="CC BY-NC-SA 4.0 "><svg class="icon"><use xlink:href="#icon-creative-commons-line"></use></svg><svg class="icon"><use xlink:href="#icon-creative-commons-by-line"></use></svg><svg class="icon"><use xlink:href="#icon-creative-commons-nc-line"></use></svg><svg class="icon"><use xlink:href="#icon-creative-commons-sa-line"></use></svg></a> 许可协议。</li></ul></section></article><div class="post-nav"><div class="post-nav-item"><a class="post-nav-prev" href="/2020/04/10/SRNTT/" rel="prev" title="Image Super-Resolution by Neural Texture Transfer论文阅读笔记"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-arrow-left-s-line"></use></svg><span class="post-nav-text">Image Super-Resolution by Neural Texture Transfer论文阅读笔记</span></a></div><div class="post-nav-item"><a class="post-nav-next" href="/2020/04/08/SRFBN/" rel="next" title="Feedback Network for Image Super-Resolution论文阅读笔记"><span class="post-nav-text">Feedback Network for Image Super-Resolution论文阅读笔记</span><svg class="icon" aria-hidden="true"><use xlink:href="#icon-arrow-right-s-line"></use></svg></a></div></div><div id="comment"><div class="comment-tooltip text-center"><span>若您无 GitHub 账号，可直接在下方匿名评论。</span><br><span>若您想及时得到回复提醒，建议跳转 GitHub Issues 评论。</span><br><span>若没有本文 Issue，您可以使用 Comment 模版新建。</span><br><a class="hty-button hty-button--raised" id="github-issues" href="https://github.com/AlexZou14/alexzou14.github.io/issues?q=is:issue+Real-Time Single Image and Video Super-Resolution Using an EfficientSub-Pixel Convolutional Neural Network论文阅读笔记" target="_blank" rel="noopener">GitHub Issues</a></div><div class="comment-container" id="valine-container"></div></div><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.css"><script defer src="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.js"></script><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex@latest/dist/contrib/copy-tex.min.css"><script defer src="https://cdn.jsdelivr.net/npm/katex@latest/dist/contrib/copy-tex.min.js"></script><script defer src="https://cdn.jsdelivr.net/npm/katex@latest/dist/contrib/auto-render.min.js" onload="renderMathInElement(document.body);"></script></div></main><footer class="sidebar-translate" id="footer"><div class="copyright"><span>&copy; 2019 – 2020 </span><span class="with-love" id="animate"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-cloud-line"></use></svg></span><span class="author"> 秩同道合</span></div><div class="powered"><span>由 <a href="https://hexo.io" target="_blank" rel="noopener">Hexo</a> 驱动 v4.2.0</span><span class="footer-separator">|</span><span>主题 - <a rel="noopener" href="https://github.com/YunYouJun/hexo-theme-yun" target="_blank"><span>Yun</span></a> v0.3.1</span></div><script defer src="https://cdn.jsdelivr.net/npm/valine@latest/dist/Valine.min.js"></script><script>function initValine() {
  new Valine(CONFIG.valine);
}
document.addEventListener("DOMContentLoaded", function() {
  initValine();
});</script></footer><a class="hty-icon-button" id="goUp" aria-label="back-to-top" href="#"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-arrow-up-s-line"></use></svg></a></div><script defer src="/js/utils.js"></script><script defer src="/js/hexo-theme-yun.js"></script><script src="https://cdn.jsdelivr.net/npm/live2d-widget@^3.1.3/lib/L2Dwidget.min.js"></script><script>L2Dwidget.init({"pluginRootPath":"live2dw/","pluginJsPath":"lib/","pluginModelPath":"assets/","tagMode":false,"debug":false,"model":{"jsonPath":"https://cdn.jsdelivr.net/npm/live2d-widget-model-hijiki@1.0.5/assets/hijiki.model.json"},"display":{"position":"right","width":200,"height":400},"mobile":{"show":true},"react":{"opacity":0.8},"dialog":{"enable":true,"hitokoto":true},"log":false});</script></body></html>