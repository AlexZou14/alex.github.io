<!DOCTYPE html><html lang="zh-CN"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0"><meta name="description" content="Image Super-Resolution by Neural Texture Transfer论文阅读笔记"><meta name="keywords" content="深度学习,笔记,超分辨率"><meta name="author" content="秩同道合"><meta name="copyright" content="秩同道合"><meta name="theme-color" content="#0078E7"><title>Image Super-Resolution by Neural Texture Transfer论文阅读笔记 | 秩同道合的小站</title><link rel="shortcut icon" href="https://cdn.jsdelivr.net/gh/AlexZou14/CDN/img/favicon.ico"><link rel="preload" href="/css/hexo-theme-yun.css" as="style"><link rel="preload" href="/js/utils.js" as="script"><link rel="preload" href="/js/hexo-theme-yun.js" as="script"><link rel="prefetch" href="/js/sidebar.js" as="script"><link rel="preconnect" href="https://cdn.jsdelivr.net" crossorigin><link rel="stylesheet" href="/css/hexo-theme-yun.css"><link rel="alternate" href="/atom.xml" title="秩同道合的小站"><script id="yun-config">
    let Yun = window.Yun || {};
    let CONFIG = {"root":"/","title":"秩同道合的小站","version":"0.3.1","anonymous_image":"https://cdn.jsdelivr.net/gh/YunYouJun/cdn/img/avatar/none.jpg","say":{"api":"https://v1.hitokoto.cn","hitokoto":true},"valine":{"el":"#valine-container","verify":false,"notify":false,"appId":"eCgP91hRSX8OtvCIR4MgLfcl-gzGzoHsz","appKey":"N5gVT8kUx5O0wMvc47SU040Y","serverURLs":null,"placeholder":"大佬求指教&nbsp_(:з」∠)_ （填写邮箱可以收到回复通知～）","avatar":null,"meta":["nick","mail","link"],"pageSize":10,"lang":"zh-cn","visitor":true},"fireworks":{"colors":["102, 167, 221","62, 131, 225","33, 78, 194"]}};
  </script><script src="//at.alicdn.com/t/font_1140697_pa6cswvjpq.js" async></script><meta name="generator" content="Hexo 4.2.0"><link rel="alternate" href="/atom.xml" title="秩同道合的小站" type="application/atom+xml">
</head><body><script defer src="https://cdn.jsdelivr.net/npm/animejs@latest/anime.min.js"></script><script defer src="/js/ui/fireworks.js"></script><canvas class="fireworks"></canvas><div class="container"><a class="sidebar-toggle sidebar-toggle-fixed hty-icon-button"><div class="hamburger hamburger--spin" type="button"><span class="hamburger-box"><span class="hamburger-inner"></span></span></div></a><aside class="sidebar"><ul class="sidebar-nav"><li class="sidebar-nav-item sidebar-nav-toc sidebar-nav-active hty-icon-button" data-target="post-toc-wrap" title="文章目录"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-list-ordered"></use></svg></li><li class="sidebar-nav-item sidebar-nav-overview hty-icon-button" data-target="site-overview-wrap" title="站点概览"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-passport-line"></use></svg></li></ul><div class="sidebar-panel" id="site-overview-wrap"><div class="site-info fix-top"><a class="site-author-avatar" href="/about" title="秩同道合"><img loading="lazy" src="https://cdn.jsdelivr.net/gh/AlexZou14/CDN/img/touxiang.jpg" alt="秩同道合"></a><div class="site-author-name"><a href="/about">秩同道合</a></div><a class="site-name" href="/about/site.html">秩同道合的小站</a><sub class="site-subtitle">寻找志趣相投的伙伴！</sub><div class="site-desciption">我和你，以及我们的秩相同所以我们才等价！</div></div><nav class="site-state"><a class="site-state-item hty-icon-button icon-home" href="/" title="首页"><span class="site-state-item-icon"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-home-4-line"></use></svg></span></a><div class="site-state-item site-state-posts"><a href="/archives" title="归档"><span class="site-state-item-icon"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-archive-line"></use></svg></span><span class="site-state-item-count">24</span></a></div><div class="site-state-item site-state-categories"><a href="/categories" title="分类"><span class="site-state-item-icon"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-folder-2-line"></use></svg></span><span class="site-state-item-count">1</span></a></div><div class="site-state-item site-state-tags"><a href="/tags" title="标签"><span class="site-state-item-icon"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-price-tag-3-line"></use></svg></span><span class="site-state-item-count">3</span></a></div><a class="site-state-item hty-icon-button" href="https://github.com/AlexZou14" target="_blank" rel="noopener" title="reward.comment"><span class="site-state-item-icon"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-hand-coin-line"></use></svg></span></a></nav><hr style="margin-bottom:0.5rem"><div class="links-of-author"><a class="links-of-author-item hty-icon-button" rel="noopener" href="/atom.xml" title="RSS" target="_blank" style="color:orange"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-rss-line"></use></svg></a><a class="links-of-author-item hty-icon-button" rel="noopener" href="https://github.com/AlexZou14" title="GitHub" target="_blank" style="color:#181717"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-github-line"></use></svg></a><a class="links-of-author-item hty-icon-button" rel="noopener" href="mailto:1120375574@qq.com" title="E-Mail" target="_blank" style="color:#8E71C1"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-mail-line"></use></svg></a><a class="links-of-author-item hty-icon-button" rel="noopener" href="https://space.bilibili.com/19164044" title="哔哩哔哩" target="_blank" style="color:#FF8EB3"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-bilibili-line"></use></svg></a></div><hr style="margin:0.5rem 1rem"><div class="links"><a class="links-item hty-icon-button" href="http://sotavision.cn" target="_blank" rel="noopener" title="工作组" style="color:#000000"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-send-plane-2-line"></use></svg></a></div></div><script defer src="/js/sidebar.js"></script><div class="sidebar-panel sidebar-panel-active" id="post-toc-wrap"><div class="post-toc"><div class="post-toc-progress"><div class="progress-bar"></div><div class="progress-info"><span class="progress-notice">您已经读了</span><span class="progress-num">0</span><span class="progress-percentage">%</span></div></div><div class="post-toc-content"><ol class="toc"><li class="toc-item toc-level-3"><a class="toc-link" href="#Abstract"><span class="toc-number">1.</span> <span class="toc-text">Abstract</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Introduction"><span class="toc-number">2.</span> <span class="toc-text">Introduction</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Related-work"><span class="toc-number">3.</span> <span class="toc-text">Related work</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Deep-Learning-based-SISR"><span class="toc-number">3.1.</span> <span class="toc-text">Deep Learning based SISR</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Reference-based-Super-Resolution"><span class="toc-number">3.2.</span> <span class="toc-text">Reference-based Super-Resolution</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Proposed-Method"><span class="toc-number">4.</span> <span class="toc-text">Proposed Method</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Feature-Swapping"><span class="toc-number">4.1.</span> <span class="toc-text">Feature Swapping</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Neural-Texture-Transfer"><span class="toc-number">4.2.</span> <span class="toc-text">Neural Texture Transfer</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Loss-function"><span class="toc-number">4.3.</span> <span class="toc-text">Loss function</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Experiment"><span class="toc-number">5.</span> <span class="toc-text">Experiment</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Experiments-Details"><span class="toc-number">5.1.</span> <span class="toc-text">Experiments Details</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#DataSet"><span class="toc-number">5.2.</span> <span class="toc-text">DataSet</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Experiment-Result"><span class="toc-number">5.3.</span> <span class="toc-text">Experiment Result</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Conclusions"><span class="toc-number">6.</span> <span class="toc-text">Conclusions</span></a></li></ol></div></div></div></aside><main class="sidebar-translate" id="content"><div id="post"><article class="post-block" itemscope itemtype="http://schema.org/Article"><link itemprop="mainEntityOfPage" href="http://alexzou14.github.io/2020/04/10/SRNTT/"><span hidden itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="name" content="秩同道合"><meta itemprop="description" content="Image Super-Resolution by Neural Texture Transfer论文阅读笔记"></span><span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="秩同道合的小站"></span><header class="post-header"><h1 class="post-title" itemprop="name headline">Image Super-Resolution by Neural Texture Transfer论文阅读笔记</h1><div class="post-meta"><span class="post-time"><span class="post-meta-item-icon"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-calendar-line"></use></svg></span> <time title="创建时间：2020-04-10 21:08:38" itemprop="dateCreated datePublished" datetime="2020-04-10T21:08:38+08:00">2020-04-10</time></span><div class="post-count"><span class="post-symbolcount"><span class="post-meta-item-icon" title="本文字数"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-file-word-line"></use></svg></span> <span title="本文字数">7k</span><span class="post-meta-divider">-</span><span class="post-meta-item-icon" title="阅读时长"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-timer-line"></use></svg></span> <span title="阅读时长">14 分钟</span></span></div><span class="leancloud_visitors" id="/2020/04/10/SRNTT/" data-flag-title="Image Super-Resolution by Neural Texture Transfer论文阅读笔记"><span class="post-meta-item-icon" title="阅读次数"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-eye-line"></use></svg> <span class="leancloud-visitors-count"></span></span></span><div class="post-classify"><span class="post-category"><span class="post-meta-item-icon"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-folder-line"></use></svg></span> <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a class="category" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/" itemprop="url" rel="index"><span itemprop="text">论文笔记</span></a></span></span><span class="post-tag"><span class="post-meta-divider">-</span><a class="tag" href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"><span class="post-meta-item-icon"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-price-tag-3-line"></use></svg></span><span class="tag-name">深度学习</span></a><a class="tag" href="/tags/%E7%AC%94%E8%AE%B0/"><span class="post-meta-item-icon"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-price-tag-3-line"></use></svg></span><span class="tag-name">笔记</span></a><a class="tag" href="/tags/%E8%B6%85%E5%88%86%E8%BE%A8%E7%8E%87/"><span class="post-meta-item-icon"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-price-tag-3-line"></use></svg></span><span class="tag-name">超分辨率</span></a></span></div></div></header><section class="post-body" itemprop="articleBody"><div class="post-content post-markdown"><h3 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h3><p>当参考（Ref）图像的内容与给定出输入LR相似时，基于参考的超分辨率方法（RefSR）在恢复高分辨率（HR）细节方面已被证明是有希望的。由于RefSR方法不稳定，生成的图像质量不高，所以本文的目的是通过参考图像利用更多的纹理细节，甚至更强的鲁棒性，释放参考图像的潜力。作者受到风格迁移的思想激发，可以将RefSR方法可以应用神经风格迁移方法。<br>本文提出了一个端到端的深层模型，它使HR细节通过自适应转移相似的语义信息，本文方法主要包括两步：</p>
<ol>
<li>特征空间的纹理匹配，</li>
<li>移匹配的纹理。</li>
</ol>
<p>另外，本文提出一个CUFFED5数据集，这个数据集包含不同相似级别的参考图像。</p>
<hr>
<h3 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h3><p>在神经网络的引入下，解决SISR卓越de的方法得到了很大的提升。由于SISR问题的不定性，这导致现有的大部分方法还是在大放大倍数下会产生模糊的结果。在感知损失和对抗损失的引入下提升了图像的感知细节，但是同时又会在一定程度上产生虚假纹理和伪影。<br>基于参考（reference-based）的方法, 即RefSR，通过利用高分辨率（HR）参考图像的丰富纹理来补偿低分辨率（LR）图像丢失的细节。但是之前的方法需要参考图像与LR包含相似的内容，并且需要图像对齐，否则的话，这些方法效果会很差。RefSR理想上应该可以有效利用参考图像，如果参考图像与LR不同，也不应该损害LR的复原效果，即RefSR应不差于SISR。<br>现有的RefSP模型对参考图像有很高的要求，要求参考图像与模糊图像的内容相仿且具有良好的对齐，这是比较难做到的，后来有人提出使用optical flow(一种图像对齐算法)先对参考图像和模糊图像进行对齐，然后送入RefSR模型。但是optical flow在两张图象的错位极其严重时表现欠佳，因此Adobe团队提出了基于纹理迁移的图像超分辨率模型(Image Super-Resolution by Neural Texture Transfer)，简称为SRNTT。<br>SRNTT主要有以下几个贡献：</p>
<ul>
<li>解决了现有SISR方法会出现虚假纹理的问题</li>
<li>放松了现有的RefSR方法的约束问题，不要求参考图像与模糊图像严格对齐</li>
<li>提高了现有RefSR方法的鲁棒性，即使使用相似性不是很高的参考图像也可以得到较好的结果构建了一个基准数据集CUFED5</li>
</ul>
<hr>
<h3 id="Related-work"><a href="#Related-work" class="headerlink" title="Related work"></a>Related work</h3><h4 id="Deep-Learning-based-SISR"><a href="#Deep-Learning-based-SISR" class="headerlink" title="Deep Learning based SISR"></a>Deep Learning based SISR</h4><p>单图超分辨率模型(SISR)的输入只有一张图象，模型会从这一张图像提取一些高频信息并使用特殊的方法合成到原图上去，以完成超分辨率的过程。这种方法有一个缺点，模糊图像毕竟不含有我们想要的高频信息，所以即便我们使用特殊的方法去提取，最后得到的结果也不可能与实际情况完全相同，也就是说，最后模型得到的图像存在一些虚假的纹理，虽然在视觉效果上图像是清晰的，但是图像的细节信息却是假的。</p>
<h4 id="Reference-based-Super-Resolution"><a href="#Reference-based-Super-Resolution" class="headerlink" title="Reference-based Super-Resolution"></a>Reference-based Super-Resolution</h4><p>基于参考图像的超分辨率(RefSR)。这种模型的输入图像有两个，一个是模糊图像，一个是清晰图像。模型会从清晰图像中提取真实的高频信息，然后将其合成到模糊图像中去。也许你会有一个疑问，既然已经有了清晰的图像，为什么我们去做超分辨率？这是因为清晰图像的角度、拍摄内容、光线等不一定乐意是我们满意，但它的高频信息却是我们需要的。</p>
<hr>
<h3 id="Proposed-Method"><a href="#Proposed-Method" class="headerlink" title="Proposed Method"></a>Proposed Method</h3><p>网络结构如下图。</p>
<blockquote>
<p><img src="http://sotavision.cn/showdoc/server/../Public/Uploads/2020-03-04/5e5f701e31110.png" alt="" loading="lazy"></p>
</blockquote>
<p>SRNTT主要由两个部分组成，一是上图中蓝色方框之外的部分，称之为特征交换；另一部分为蓝色方框内部的纹理迁移部分。</p>
<h4 id="Feature-Swapping"><a href="#Feature-Swapping" class="headerlink" title="Feature Swapping"></a>Feature Swapping</h4><p>特征交换这一部的主要工作就是在Ref中找到与LR最接近的块，然后替换掉LR中的块。这就涉及两个步骤: 如何计算相似度、如何交换对应块。</p>
<ul>
<li>如何计算相似度</li>
</ul>
<p>论文提出的方法不是计算整个图的相似度，而是分块计算。这里因为LR和Ref的大小不同，先对LR用bicubic进行上采样使LR和Ref具有相同的大小，而同时考虑到二者的分辨率不同，对Ref用bicubic进行下采样再上采样，使得\(I^{Ref\downarrow\uparrow}\)的模糊程度跟\(I^{LR\uparrow}\)接近。<br>考虑到\(I^{LR\uparrow}\)和\(I^{Ref\downarrow\uparrow}\)的块在颜色和光照上面可能有些不同，于是论文不直接对Patch的像素进行计算相似度，而在高层次的特征图上计算，即\(\phi(I)\)，来突出结构和纹理信息。<br>论文采用内积方法来计算相似度：<br>\[s_{i,j}=\langle{P_i(\phi(I^{LR\uparrow})),\dfrac{P_i(\phi(I^{LR\uparrow}))}{\left |P_i(\phi(I^{LR\uparrow})\right |}}\rangle\]<br>上式，计算了LR的 i-th patch和模糊化参考图像 的 j-th patch的相似性。注意，对参考patch进行了归一化。可以通过卷积或者互相关来加速以上计算过程：<br>\[S_j=\phi(I^{LR\uparrow}*{\dfrac{P_j(\phi(I^{LR\uparrow}))}{\left |P_j(\phi(I^{LR\uparrow})\right |}})\]<br>\(S_j\)即表示 s-th patch 相对 LR的相似性。相当于一个卷积核对LR进行卷积，计算结果即为Similarity map。</p>
<ul>
<li>交换操作</li>
</ul>
<p>因为在LR和Ref的特征空间中密集采样，所以每个LR位置都对应多个不同的卷积核的卷积结果，对应多个不同相似性的纹理特征。基于Similarity map，选择LR每个位置的相似性最高的Ref patch，构成交换特征图M (swapped feature map)：<br>\[P_{\omega(x,y)}(M)=P_{j^*}(\phi(I^{Ref})),j^*=\arg \max_jS_j(x,y)\]<br>即M在（x,y）位置对应的Ref patch是similairty score最大的Ref patch。由于每一个位置对应一个Ref patch，所以这些patch是重叠的，在重叠位置，取平均。另外，另外，注意计算相似性是使用\(I^{Ref\uparrow\downarrow}\)，纹理迁移则是利用 \(I^{Ref}\)。<br>在实现上，利用VGG-19提取特征，relu1_1, relu2_1,relu3_1用于多个尺度上纹理编码，但是为了加快匹配，只在relu3_1层进行匹配，将匹配结果对应到relu1_1和relu2_1。</p>
<h4 id="Neural-Texture-Transfer"><a href="#Neural-Texture-Transfer" class="headerlink" title="Neural Texture Transfer"></a>Neural Texture Transfer</h4><p>有了多尺度的swapped feature map，如何进行纹理迁移呢？采用residual blocks和跳跃连接方式，融合LR的特征和swap特征，并通过sub-pixel conv上采样。如下图：</p>
<blockquote>
<p><img src="http://sotavision.cn/showdoc/server/../Public/Uploads/2020-03-04/5e5f7c3c83fb7.png" alt="" loading="lazy"></p>
</blockquote>
<p>论文采用残差块和跳跃连接来建立基本的生成网络。记\(\psi_l\)是第l层的输出，可以定义为：<br>\[\psi_l=[\text{Res}(\psi_{l-1}||M_{l-1})+\psi_{l-1}]\uparrow_{2\times}\]<br>假设有L层，那最终SR:<br>\[I^{SR}=\text{Res}(\psi_{L-1}||M_{L-1})+\psi_{L-1}\]</p>
<h4 id="Loss-function"><a href="#Loss-function" class="headerlink" title="Loss function"></a>Loss function</h4><p>目标：1）保留LR的空间结构，2）提升SR的视觉质量，3）充分利用Ref的丰富纹理。提出四个loss function：</p>
<ul>
<li>Reconstruction loss: SR与HR的L1距离</li>
</ul>
<p>\[L_{rec}=\left |I^{HR}-I^{SR} \right |_1\]</p>
<ul>
<li>Perceptual loss： 采用VGG-19的relu5_1层</li>
</ul>
<p>\[{L_{per}=\dfrac{1}{V}\sum_{i=1}^C\left |{\phi_i(I^{HR})-\phi_i(I^{SR})}\right |_F}\]</p>
<ul>
<li>Adversarial loss：利用 WGAN-GP<div>
  \[L_{adv}=-\text{E}_{\tilde{x}\sim{P_g}}[D(\tilde{x})],\min_g \max_{D\in \text{D}}\text{E}_{x \sim{P_r}}[D(x)]-\text{E}_{\tilde{x}\sim{P_g}}[D(\tilde{x})]\]
</div></li>
<li>Texture loss：纹理损失是为了让SR的纹理与Ref纹理更接近，另外，通过使用similarity map作为权重，抑制不相似纹理的惩罚，放大相似纹理的惩罚，这样可以自适应地进行纹理迁移<div>
  \[L_{tex}=\sum_l{\lambda_l \left \|Gr(\phi_l(I^{SR})\cdot S_l^*)-Gr(M_l\cdot S_l^*) \right \|}_F\]
</div>

</li>
</ul>
<p>其中，\(G_r\)计算 Gram matrix。</p>
<hr>
<h3 id="Experiment"><a href="#Experiment" class="headerlink" title="Experiment"></a>Experiment</h3><h4 id="Experiments-Details"><a href="#Experiments-Details" class="headerlink" title="Experiments Details"></a>Experiments Details</h4><p>具体实验细节可以查看官方代码：<a href="https://github.com/ZZUTK/SRNTT" target="_blank" rel="noopener">https://github.com/ZZUTK/SRNTT</a></p>
<h4 id="DataSet"><a href="#DataSet" class="headerlink" title="DataSet"></a>DataSet</h4><p>本文基于CUFED构建了一个数据集，数据集包含不同相似度的LR-HR-Ref 图像对。共有4个相似度级别，不同的相似度是基于SIFT特征匹配计算的。测试数据集包含Sun80和Urban100。</p>
<h4 id="Experiment-Result"><a href="#Experiment-Result" class="headerlink" title="Experiment Result"></a>Experiment Result</h4><p>论文对比了不同的模型在不同数据集上的表现，最后结果是，定量观测PSNR值来看，在单图超分辨率领域，SRNTT取得第二名；在基于参考的超分辨领域，SRNTT优于现有的所有模型，位列第一。</p>
<blockquote>
<p><img src="http://sotavision.cn/showdoc/server/../Public/Uploads/2020-03-04/5e5f9e4edb182.png" alt="" loading="lazy"></p>
</blockquote>
<hr>
<h3 id="Conclusions"><a href="#Conclusions" class="headerlink" title="Conclusions"></a>Conclusions</h3><ul>
<li>论文学会用RefSR的思想，打破了SISR的束缚。（其实就是不再单纯地学习HR和LR的映射，而是引入RefSR）</li>
<li>论文提出了SRNTT，可以得到更好的超分辨率效果。</li>
<li>论文建立了一个新的数据集，CUFED5，对LR有不同相似程度的RefSR，用来进行进一步探索。</li>
</ul>
</div><ul class="post-copyright"><li class="post-copyright-author"><strong>本文作者：</strong>秩同道合</li><li class="post-copyright-link"><strong>本文链接：</strong><a href="http://alexzou14.github.io/2020/04/10/SRNTT/" title="Image Super-Resolution by Neural Texture Transfer论文阅读笔记">http://alexzou14.github.io/2020/04/10/SRNTT/</a></li><li class="post-copyright-license"><strong>版权声明：</strong>本博客所有文章除特别声明外，均默认采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh" target="_blank" rel="noopener" title="CC BY-NC-SA 4.0 "><svg class="icon"><use xlink:href="#icon-creative-commons-line"></use></svg><svg class="icon"><use xlink:href="#icon-creative-commons-by-line"></use></svg><svg class="icon"><use xlink:href="#icon-creative-commons-nc-line"></use></svg><svg class="icon"><use xlink:href="#icon-creative-commons-sa-line"></use></svg></a> 许可协议。</li></ul></section></article><div class="post-nav"><div class="post-nav-item"><a class="post-nav-prev" href="/2020/04/10/EBRN/" rel="prev" title="Embedded Block Residual Network:A Recursive Restoration Model for Single-Image Super-Resolution 论文阅读笔记"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-arrow-left-s-line"></use></svg><span class="post-nav-text">Embedded Block Residual Network:A Recursive Restoration Model for Single-Image Super-Resolution 论文阅读笔记</span></a></div><div class="post-nav-item"><a class="post-nav-next" href="/2020/04/10/ESPCN/" rel="next" title="Real-Time Single Image and Video Super-Resolution Using an EfficientSub-Pixel Convolutional Neural Network论文阅读笔记"><span class="post-nav-text">Real-Time Single Image and Video Super-Resolution Using an EfficientSub-Pixel Convolutional Neural Network论文阅读笔记</span><svg class="icon" aria-hidden="true"><use xlink:href="#icon-arrow-right-s-line"></use></svg></a></div></div><div id="comment"><div class="comment-tooltip text-center"><span>若您无 GitHub 账号，可直接在下方匿名评论。</span><br><span>若您想及时得到回复提醒，建议跳转 GitHub Issues 评论。</span><br><span>若没有本文 Issue，您可以使用 Comment 模版新建。</span><br><a class="hty-button hty-button--raised" id="github-issues" href="https://github.com/AlexZou14/alexzou14.github.io/issues?q=is:issue+Image Super-Resolution by Neural Texture Transfer论文阅读笔记" target="_blank" rel="noopener">GitHub Issues</a></div><div class="comment-container" id="valine-container"></div></div><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.css"><script defer src="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.js"></script><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex@latest/dist/contrib/copy-tex.min.css"><script defer src="https://cdn.jsdelivr.net/npm/katex@latest/dist/contrib/copy-tex.min.js"></script><script defer src="https://cdn.jsdelivr.net/npm/katex@latest/dist/contrib/auto-render.min.js" onload="renderMathInElement(document.body);"></script></div></main><footer class="sidebar-translate" id="footer"><div class="copyright"><span>&copy; 2019 – 2020 </span><span class="with-love" id="animate"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-cloud-line"></use></svg></span><span class="author"> 秩同道合</span></div><div class="powered"><span>由 <a href="https://hexo.io" target="_blank" rel="noopener">Hexo</a> 驱动 v4.2.0</span><span class="footer-separator">|</span><span>主题 - <a rel="noopener" href="https://github.com/YunYouJun/hexo-theme-yun" target="_blank"><span>Yun</span></a> v0.3.1</span></div><script defer src="https://cdn.jsdelivr.net/npm/valine@latest/dist/Valine.min.js"></script><script>function initValine() {
  new Valine(CONFIG.valine);
}
document.addEventListener("DOMContentLoaded", function() {
  initValine();
});</script></footer><a class="hty-icon-button" id="goUp" aria-label="back-to-top" href="#"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-arrow-up-s-line"></use></svg></a></div><script defer src="/js/utils.js"></script><script defer src="/js/hexo-theme-yun.js"></script><script src="https://cdn.jsdelivr.net/npm/live2d-widget@^3.1.3/lib/L2Dwidget.min.js"></script><script>L2Dwidget.init({"pluginRootPath":"live2dw/","pluginJsPath":"lib/","pluginModelPath":"assets/","tagMode":false,"debug":false,"model":{"jsonPath":"https://cdn.jsdelivr.net/npm/live2d-widget-model-hijiki@1.0.5/assets/hijiki.model.json"},"display":{"position":"right","width":200,"height":400},"mobile":{"show":true},"react":{"opacity":0.8},"dialog":{"enable":true,"hitokoto":true},"log":false});</script></body></html>