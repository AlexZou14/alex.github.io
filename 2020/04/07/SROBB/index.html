<!DOCTYPE html><html lang="zh-CN"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0"><meta name="description" content="SROBB:Targeted Perceptual Loss for Single Image Super-Resolution论文阅读笔记"><meta name="keywords" content="深度学习,笔记,超分辨率"><meta name="author" content="秩同道合"><meta name="copyright" content="秩同道合"><meta name="theme-color" content="#0078E7"><title>SROBB:Targeted Perceptual Loss for Single Image Super-Resolution论文阅读笔记 | 秩同道合的小站</title><link rel="shortcut icon" href="https://cdn.jsdelivr.net/gh/AlexZou14/CDN/img/favicon.ico"><link rel="preload" href="/css/hexo-theme-yun.css" as="style"><link rel="preload" href="/js/utils.js" as="script"><link rel="preload" href="/js/hexo-theme-yun.js" as="script"><link rel="prefetch" href="/js/sidebar.js" as="script"><link rel="preconnect" href="https://cdn.jsdelivr.net" crossorigin><link rel="stylesheet" href="/css/hexo-theme-yun.css"><link rel="alternate" href="/atom.xml" title="秩同道合的小站"><script id="yun-config">
    let Yun = window.Yun || {};
    let CONFIG = {"root":"/","title":"秩同道合的小站","version":"0.3.1","anonymous_image":"https://cdn.jsdelivr.net/gh/YunYouJun/cdn/img/avatar/none.jpg","say":{"api":"https://v1.hitokoto.cn","hitokoto":true},"valine":{"el":"#valine-container","verify":false,"notify":false,"appId":"eCgP91hRSX8OtvCIR4MgLfcl-gzGzoHsz","appKey":"N5gVT8kUx5O0wMvc47SU040Y","serverURLs":null,"placeholder":"大佬求指教&nbsp_(:з」∠)_ （填写邮箱可以收到回复通知～）","avatar":null,"meta":["nick","mail","link"],"pageSize":10,"lang":"zh-cn","visitor":true},"fireworks":{"colors":["102, 167, 221","62, 131, 225","33, 78, 194"]}};
  </script><script src="//at.alicdn.com/t/font_1140697_pa6cswvjpq.js" async></script><meta name="generator" content="Hexo 4.2.0"><link rel="alternate" href="/atom.xml" title="秩同道合的小站" type="application/atom+xml">
</head><body><script defer src="https://cdn.jsdelivr.net/npm/animejs@latest/anime.min.js"></script><script defer src="/js/ui/fireworks.js"></script><canvas class="fireworks"></canvas><div class="container"><a class="sidebar-toggle sidebar-toggle-fixed hty-icon-button"><div class="hamburger hamburger--spin" type="button"><span class="hamburger-box"><span class="hamburger-inner"></span></span></div></a><aside class="sidebar"><ul class="sidebar-nav"><li class="sidebar-nav-item sidebar-nav-toc sidebar-nav-active hty-icon-button" data-target="post-toc-wrap" title="文章目录"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-list-ordered"></use></svg></li><li class="sidebar-nav-item sidebar-nav-overview hty-icon-button" data-target="site-overview-wrap" title="站点概览"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-passport-line"></use></svg></li></ul><div class="sidebar-panel" id="site-overview-wrap"><div class="site-info fix-top"><a class="site-author-avatar" href="/about" title="秩同道合"><img loading="lazy" src="https://cdn.jsdelivr.net/gh/AlexZou14/CDN/img/touxiang.jpg" alt="秩同道合"></a><div class="site-author-name"><a href="/about">秩同道合</a></div><a class="site-name" href="/about/site.html">秩同道合的小站</a><sub class="site-subtitle">寻找志趣相投的伙伴！</sub><div class="site-desciption">我和你，以及我们的秩相同所以我们才等价！</div></div><nav class="site-state"><a class="site-state-item hty-icon-button icon-home" href="/" title="首页"><span class="site-state-item-icon"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-home-4-line"></use></svg></span></a><div class="site-state-item site-state-posts"><a href="/archives" title="归档"><span class="site-state-item-icon"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-archive-line"></use></svg></span><span class="site-state-item-count">17</span></a></div><div class="site-state-item site-state-categories"><a href="/categories" title="分类"><span class="site-state-item-icon"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-folder-2-line"></use></svg></span><span class="site-state-item-count">1</span></a></div><div class="site-state-item site-state-tags"><a href="/tags" title="标签"><span class="site-state-item-icon"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-price-tag-3-line"></use></svg></span><span class="site-state-item-count">3</span></a></div><a class="site-state-item hty-icon-button" href="https://github.com/AlexZou14" target="_blank" rel="noopener" title="reward.comment"><span class="site-state-item-icon"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-hand-coin-line"></use></svg></span></a></nav><hr style="margin-bottom:0.5rem"><div class="links-of-author"><a class="links-of-author-item hty-icon-button" rel="noopener" href="/atom.xml" title="RSS" target="_blank" style="color:orange"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-rss-line"></use></svg></a><a class="links-of-author-item hty-icon-button" rel="noopener" href="https://github.com/AlexZou14" title="GitHub" target="_blank" style="color:#181717"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-github-line"></use></svg></a><a class="links-of-author-item hty-icon-button" rel="noopener" href="mailto:1120375574@qq.com" title="E-Mail" target="_blank" style="color:#8E71C1"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-mail-line"></use></svg></a><a class="links-of-author-item hty-icon-button" rel="noopener" href="https://space.bilibili.com/19164044" title="哔哩哔哩" target="_blank" style="color:#FF8EB3"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-bilibili-line"></use></svg></a></div><hr style="margin:0.5rem 1rem"><div class="links"><a class="links-item hty-icon-button" href="http://sotavision.cn" target="_blank" rel="noopener" title="工作组" style="color:#000000"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-send-plane-2-line"></use></svg></a></div></div><script defer src="/js/sidebar.js"></script><div class="sidebar-panel sidebar-panel-active" id="post-toc-wrap"><div class="post-toc"><div class="post-toc-progress"><div class="progress-bar"></div><div class="progress-info"><span class="progress-notice">您已经读了</span><span class="progress-num">0</span><span class="progress-percentage">%</span></div></div><div class="post-toc-content"><ol class="toc"><li class="toc-item toc-level-3"><a class="toc-link" href="#Abstract"><span class="toc-number">1.</span> <span class="toc-text">Abstract</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Introduction"><span class="toc-number">2.</span> <span class="toc-text">Introduction</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Related-work"><span class="toc-number">3.</span> <span class="toc-text">Related work</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Proposed-Method"><span class="toc-number">4.</span> <span class="toc-text">Proposed Method</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Targeted-perceptual-loss"><span class="toc-number">4.1.</span> <span class="toc-text">Targeted perceptual loss</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#OBB-Object-background-and-boundary-label"><span class="toc-number">4.2.</span> <span class="toc-text">OBB: Object, background and boundary label</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Architecture"><span class="toc-number">4.3.</span> <span class="toc-text">Architecture</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Experiment"><span class="toc-number">5.</span> <span class="toc-text">Experiment</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Experiments-Details"><span class="toc-number">5.1.</span> <span class="toc-text">Experiments Details</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Qualitative-Results"><span class="toc-number">5.2.</span> <span class="toc-text">Qualitative Results</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Quantitative-Results"><span class="toc-number">5.3.</span> <span class="toc-text">Quantitative Results</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Conclusions"><span class="toc-number">6.</span> <span class="toc-text">Conclusions</span></a></li></ol></div></div></div></aside><main class="sidebar-translate" id="content"><div id="post"><article class="post-block" itemscope itemtype="http://schema.org/Article"><link itemprop="mainEntityOfPage" href="http://alexzou14.github.io/2020/04/07/SROBB/"><span hidden itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="name" content="秩同道合"><meta itemprop="description" content="SROBB:Targeted Perceptual Loss for Single Image Super-Resolution论文阅读笔记"></span><span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="秩同道合的小站"></span><header class="post-header"><h1 class="post-title" itemprop="name headline">SROBB:Targeted Perceptual Loss for Single Image Super-Resolution论文阅读笔记</h1><div class="post-meta"><span class="post-time"><span class="post-meta-item-icon"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-calendar-line"></use></svg></span> <time title="创建时间：2020-04-07 19:34:04" itemprop="dateCreated datePublished" datetime="2020-04-07T19:34:04+08:00">2020-04-07</time></span><div class="post-count"><span class="post-symbolcount"><span class="post-meta-item-icon" title="本文字数"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-file-word-line"></use></svg></span> <span title="本文字数">7.4k</span><span class="post-meta-divider">-</span><span class="post-meta-item-icon" title="阅读时长"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-timer-line"></use></svg></span> <span title="阅读时长">15 分钟</span></span></div><span class="leancloud_visitors" id="/2020/04/07/SROBB/" data-flag-title="SROBB:Targeted Perceptual Loss for Single Image Super-Resolution论文阅读笔记"><span class="post-meta-item-icon" title="阅读次数"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-eye-line"></use></svg> <span class="leancloud-visitors-count"></span></span></span><div class="post-classify"><span class="post-category"><span class="post-meta-item-icon"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-folder-line"></use></svg></span> <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a class="category" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/" itemprop="url" rel="index"><span itemprop="text">论文笔记</span></a></span></span><span class="post-tag"><span class="post-meta-divider">-</span><a class="tag" href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"><span class="post-meta-item-icon"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-price-tag-3-line"></use></svg></span><span class="tag-name">深度学习</span></a><a class="tag" href="/tags/%E7%AC%94%E8%AE%B0/"><span class="post-meta-item-icon"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-price-tag-3-line"></use></svg></span><span class="tag-name">笔记</span></a><a class="tag" href="/tags/%E8%B6%85%E5%88%86%E8%BE%A8%E7%8E%87/"><span class="post-meta-item-icon"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-price-tag-3-line"></use></svg></span><span class="tag-name">超分辨率</span></a></span></div></div></header><section class="post-body" itemprop="articleBody"><div class="post-content post-markdown"><h3 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h3><p>由于传统学习方法能力有限，没有考虑任何的语义信息产生了较大的误差，所以本文提出了一种更客观的地从知觉损失中获益的新方法。优化了一个基于深度网络的解码器，该解码器具有目标函数，可以使用相应的方式在不同的语义级别下处理图像。该方法利用我们提出的OBB(对象、背景和边界)标签，由分割标签生成，在考虑背景纹理相似性的同时，估计一个合适的边界感知损失。我们展示了我们提出的方法可以得到更真实的纹理和更清晰的边缘，并且在标准基准测试的定性结果和广泛的用户研究结果方面都优于其他最先进的算法。</p>
<hr>
<h3 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h3><p>最近，SISR的另一个突破是将感知损失函数用于训练前馈网络，而不是使用每像素损失函数，如均方误差(MSE)。它解决了MSE优化导致的纹理模糊问题，同时也带来了对抗性的损失，在感知图像质量方面实现了近真实感重建。虽然感知损失在SISR中取得了很大的成功，但将其应用于整体图像，不考虑语义信息，限制了其网络能力。在感知功能方面，最先进的方法使用不同层次的特征来恢复原始图像;这个选择决定了他们是关注局部信息(如边缘)、中层特性(如纹理)还是与语义信息对应的高层特性。在这些方法中，以同样的方法计算了整个图像的感知损失，这意味着在边缘、前景或图像背景上使用了相同级别的特征。</p>
<blockquote>
<p><img src="http://sotavision.cn/showdoc/server/../Public/Uploads/2020-03-24/5e796c1108431.png" alt="" loading="lazy"><br>图1所示。针对高分辨率图像的特点，提出了一种在训练过程中利用分割标签对不同语义层次的图像进行分割的方法;我们优化了我们的SISR模型，通过最小化感知误差，分别对应的边缘只在物体边界和纹理的背景区域。结果从左至右:原始图像，超分辨率图像仅使用像素丢失函数，像素丢失+感知丢失函数和像素丢失+目标感知丢失函数(our)。</p>
</blockquote>
<p>为了解决上述问题，我们提出了一种新的方法，以更客观的方式从知觉损失中获益。图1显示了我们所提议的方法的概述。特别是，我们使用像素级的分割注释来构建我们所提议的OBB标签，从而能够找到目标感知特性，这些特性可以用来最小化不同图像区域的适当损失:例如，边缘的损失和训练过程中图像纹理的损失。我们展示了我们的方法使用目标知觉损失优于其他最先进的算法在定性结果和用户研究实验，并导致更现实的纹理和更锋利的边缘。</p>
<hr>
<h3 id="Related-work"><a href="#Related-work" class="headerlink" title="Related work"></a>Related work</h3><p>这节作者主要回顾了近期SR的各种方法如SRCN，DRCN等。尽管针对SISR任务提出了不同的体系结构，但是基于优化的方法的行为主要是由目标函数的选择驱动的。这些方法所使用的目标函数大多包含一个损失项，即超分辨率HR图像与真实HR图像之间的像素距离。然而，由于所有可能的解决方案的像素平均，仅使用这个函数就会导致图像模糊和超平滑。<br>感知驱动的方法在视觉质量方面显著提高了图像的超分辨率。基于感知相似度的思想，提出了一种利用预先训练的特征提取器的特定层(如VGG)来最小化特征空间中的感知损失。在相似的工作中，提出的内容损失来生成具有自然图像统计的图像，它关注的是特征分布而不是仅仅比较外观。SRGAN提出在感知损失的基础上，利用对抗性损失来支持自然图像流形上的输出。虽然这些方法产生了接近于光真实感的结果，但它们以同样的方式估计了整个图像的重建误差，没有利用任何可以提高视觉质量的语义信息。<br>在这项工作中，我们研究了一种利用图像内部语义信息的新方法，产生具有精细结构的逼真的超分辨率图像。</p>
<hr>
<h3 id="Proposed-Method"><a href="#Proposed-Method" class="headerlink" title="Proposed Method"></a>Proposed Method</h3><p>作者引入了一个损失函数，它包含三项:1-像素级损失(MSE)、2-对抗性损失和3-新目标感知损失函数。<br>MSE和对抗性损失术语的定义如下:</p>
<ul>
<li>像素级别损失是目前为止SR中最常用的损失函数，它在图像域中计算原始图像与超分辨率图像之间的像素方向均方误差(MSE)。使用它作为一个独立的目标函数的主要缺点是解决了一个覆盖的重建。接受MSE损失训练的网络试图找到合理解决方案的像素平均，这导致感知质量较差，边缘和纹理中缺乏高频细节。</li>
<li>在SRGAN的启发下，我们将SR模型建立在一个对抗性的环境中，给出了一个可行的解决方案。特别地，我们使用一个额外的网络(鉴别器)，它可以与我们的SR解码器竞争。生成器(SR解码器)试图生成伪图像来欺骗鉴别器，而鉴别器的目标是将生成的结果与真实的HR图像区分开来。这种设置的结果在感知上优于通过最小化像素方面的MSE和经典感知损失得到的解决方案。</li>
</ul>
<h4 id="Targeted-perceptual-loss"><a href="#Targeted-perceptual-loss" class="headerlink" title="Targeted perceptual loss"></a>Targeted perceptual loss</h4><p>作者提出了一种新的方法，以有针对性的方式利用感知相似性，重建更有吸引力的边缘和纹理。</p>
<blockquote>
<p><img src="http://sotavision.cn/showdoc/server/../Public/Uploads/2020-03-24/5e7979791a063.png" alt="" loading="lazy"><br>图2. 选择不同CNN层来估计图像不同区域感知损失的效果，如边缘和纹理:(a)使用更深的卷积层(中层特征)，VGG-16[29]的ReLU 4-1， (b)使用早期卷积层(低层特征)，VGG-16网络的ReLU 1-2。</p>
</blockquote>
<p>目标丢失函数尝试在区域周围选择更真实的纹理，其中纹理的类型似乎很重要，例如，树，同时尝试解决边界区域周围更锋利的边缘。为此，作者首先在图像中定义三种类型的区域:1-背景、2-边界和3-对象，然后使用不同的函数计算每个区域的目标知觉损失。<br><strong>背景</strong>( \(\mathcal G_b\) ):我们认为四类是背景：“天空”、“植物”、“地面”和“水”。 我们选择这些类别是因为它们的具体外观；这些标签区域的整体纹理更多。我们计算中层CNN特征来估计SR和HR图像之间的感知相似性。 在这里，我们使用VGG-16的ReLU4-3层来实现这一目的。<br><strong>边界</strong>( \(\mathcal G_e\) ):所有分隔对象和背景的边缘都被认为是边界。通过一些预处理，我们将这些边缘扩展为一条贯穿所有边界的条带。我们估计了一个早期CNN层在SR和HR图像之间的特征距离，这个特征距离更侧重于低层空间信息，主要是边缘和斑点。特别地，我们最小化了VGG-16的ReLU 2-2层的感知损失。<br><strong>目标</strong>( \(\mathcal G_o\) ):由于现实世界中物体的形状和纹理种类繁多，因此判断早期的特征对于感知缺失功能来说是更合适还是更适合使用深层的特征对于感知缺失功能来说是一个挑战;例如，在斑马的图像中，更清晰的边缘比整体纹理更重要。尽管如此，强迫网络估计树的精确边缘可能会误导优化过程。因此，我们不考虑任何类型的知觉损失对定义为对象的领域加权为零，并依赖于MSE和对抗性损失。</p>
<p>为了计算特定图像区域的感知损失，我们对语义类进行二值分割掩码(对感兴趣的类像素值为1，其他地方像素值为0)。每个掩码都分类地表示图像的不同区域，并分别按元素乘上HR图像和估计的超分辨率图像SR。我们可以得出，掩码HR与超分辨率图像在特征空间中的所有非零距离都对应于该图像可见区域的内容:边界( \(M_{OBB}^{boundaries}\) )用掩码对应边缘，背景( \(M_{OBB}^{background}\) )用掩码对应纹理。<br>整体目标感知损失函数为:<br>\(L_{perc.} = \alpha\cdot \mathcal G_{e}(I^{SR}\circ M_{OBB}^{boundary}, I^{HR}\circ M_{OBB}^{boundary})\)<br>\(+\beta\cdot \mathcal G_{b}(I^{SR}\circ M_{OBB}^{boundary}, I^{HR}\circ M_{OBB}^{boundary})+\gamma\cdot \mathcal G_{o}\)</p>
<h4 id="OBB-Object-background-and-boundary-label"><a href="#OBB-Object-background-and-boundary-label" class="headerlink" title="OBB: Object, background and boundary label"></a>OBB: Object, background and boundary label</h4><p>为了充分利用基于感知丢失的图像超分辨率，作者通过提出的目标丢失函数来增强语义细节(对象、背景和边界出现在图像上的地方)。此外，现有的分割任务注释，如coco-stuff只提供了关于对象和背景的空间信息，没有使用表示边缘区域的类，即本文中的边界。因此，作者提出了我们的标注方法(图3)，为图像的语义信息提供更好的空间控制。</p>
<blockquote>
<p><img src="http://sotavision.cn/showdoc/server/../Public/Uploads/2020-03-24/5e797f7c995a5.png" alt="" loading="lazy"><br>图3. 构造一个OBB标签。我们根据“对象”、“背景”或“边界”类的初始像素级标签为每个区域分配一个类。</p>
</blockquote>
<p>为了得到一个较厚的条带，将不同的类分开，作者通过计算了一个d1大小的圆盘的膨胀。将结果区域标记为“boundary”类。将分割标签中的“sky”、“plant”、“ground”和“water”类作为“Background”。所有剩余的对象类都被认为是“对象”类。</p>
<h4 id="Architecture"><a href="#Architecture" class="headerlink" title="Architecture"></a>Architecture</h4><blockquote>
<p><img src="http://sotavision.cn/showdoc/server/../Public/Uploads/2020-03-24/5e79803d2eab1.png" alt="" loading="lazy"><br>图4. SR解码器原理图。我们在训练SR解码器的同时，使用了目标感知损失以及MSE和对抗性损失。在该模式中，k、n和s分别对应内核大小、特征映射数和步长大小。</p>
</blockquote>
<p>为了与SRGAN方法进行公平的比较，并对所提出的目标感知丢失进行消融研究，我们使用与SRGAN相同的SR解码器。</p>
<hr>
<h3 id="Experiment"><a href="#Experiment" class="headerlink" title="Experiment"></a>Experiment</h3><h4 id="Experiments-Details"><a href="#Experiments-Details" class="headerlink" title="Experiments Details"></a>Experiments Details</h4><p>为了创建OBB标签，作者使用COCO-Stuff数据集中的一组随机的50K图像，其中包含用于分割任务的91个类的语义标签。<br>训练过程分两步进行;首先，对SR解码器进行了25个周期的预训练，仅以像素方向的均方误差为损失函数。在此基础上，增加了目标知觉损失函数和对抗性损失函数，训练时间延长了55个迭代。每一项的权重在新的有针对性的知觉丧失,α和β,设置为2×10−6和1.5×10−6,分别。与SRGAN相同，将对抗性损失函数和MSE损失函数的权重分别设置为1.0和1×10−3。我们将用于生成OBB标签的磁盘直径d1设置为2.0。在这两个步骤中都使用了Adam优化器。将学习率设置为1×10−3，然后每20个迭代衰减10倍。我们还交替优化了与SRGAN提出的参数相似的鉴别器。</p>
<h4 id="Qualitative-Results"><a href="#Qualitative-Results" class="headerlink" title="Qualitative Results"></a>Qualitative Results</h4><p>我们的方法主要是利用分割标签对具有边界和背景的感知损失项的解码器进行优化。虽然我们没有将知觉损失专门应用于物体区域，但是我们的实验表明，训练后的模型与其他方法相比，在某种程度上泛化了，它重建了更真实的物体。</p>
<blockquote>
<p><img src="http://sotavision.cn/showdoc/server/../Public/Uploads/2020-03-24/5e79828535900.png" alt="" loading="lazy"><br><img src="http://sotavision.cn/showdoc/server/../Public/Uploads/2020-03-24/5e798298319e8.png" alt="" loading="lazy"></p>
</blockquote>
<h4 id="Quantitative-Results"><a href="#Quantitative-Results" class="headerlink" title="Quantitative Results"></a>Quantitative Results</h4><blockquote>
<p><img src="http://sotavision.cn/showdoc/server/../Public/Uploads/2020-03-24/5e7982c291776.png" alt="" loading="lazy"><br>表1 Set5和Set14测试集的“婴儿”和“狒狒”图像的双三次插值、LapSRN、SRGAN和SROBB(我们的)的比较。最佳度量(SSIM, PSNR，lpip)用粗体突出显示。可视化比较如图5所示。</p>
</blockquote>
<p>学习感知图像Patch相似度(LPIPS)度量是最近引入的一种基于参考的图像质量评估度量，它的目的是估计两幅图像之间的感知相似度。</p>
<hr>
<h3 id="Conclusions"><a href="#Conclusions" class="headerlink" title="Conclusions"></a>Conclusions</h3><ol>
<li>针对基于CNN的单图像超分辨率，提出了一种新的目标感知丢失函数。</li>
<li>提出的目标函数用相关的损失项对图像的不同区域进行惩罚，即在训练过程中对边缘和纹理使用边缘损失和纹理损失。</li>
<li>引入了我们的OBB标签，从像素分割标签创建，以提供更好的空间控制的语义信息的图像。</li>
<li>提出的定向知觉损失训练在感知效果上更令人满意，并且优于目前最先进的SR方法。</li>
</ol>
</div><ul class="post-copyright"><li class="post-copyright-author"><strong>本文作者：</strong>秩同道合</li><li class="post-copyright-link"><strong>本文链接：</strong><a href="http://alexzou14.github.io/2020/04/07/SROBB/" title="SROBB:Targeted Perceptual Loss for Single Image Super-Resolution论文阅读笔记">http://alexzou14.github.io/2020/04/07/SROBB/</a></li><li class="post-copyright-license"><strong>版权声明：</strong>本博客所有文章除特别声明外，均默认采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh" target="_blank" rel="noopener" title="CC BY-NC-SA 4.0 "><svg class="icon"><use xlink:href="#icon-creative-commons-line"></use></svg><svg class="icon"><use xlink:href="#icon-creative-commons-by-line"></use></svg><svg class="icon"><use xlink:href="#icon-creative-commons-nc-line"></use></svg><svg class="icon"><use xlink:href="#icon-creative-commons-sa-line"></use></svg></a> 许可协议。</li></ul></section></article><div class="post-nav"><div class="post-nav-item"><a class="post-nav-prev" href="/2020/04/08/LapSRN/" rel="prev" title="Deep Laplacian Pyramid Networks for Fast and Accurate Super-Resolution论文阅读笔记"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-arrow-left-s-line"></use></svg><span class="post-nav-text">Deep Laplacian Pyramid Networks for Fast and Accurate Super-Resolution论文阅读笔记</span></a></div><div class="post-nav-item"><a class="post-nav-next" href="/2020/04/07/NatSR/" rel="next" title="Natural and Realistic Single Image Super-Resolution with Explicit Natural Manifold Discrimination论文阅读笔记"><span class="post-nav-text">Natural and Realistic Single Image Super-Resolution with Explicit Natural Manifold Discrimination论文阅读笔记</span><svg class="icon" aria-hidden="true"><use xlink:href="#icon-arrow-right-s-line"></use></svg></a></div></div><div id="comment"><div class="comment-tooltip text-center"><span>若您无 GitHub 账号，可直接在下方匿名评论。</span><br><span>若您想及时得到回复提醒，建议跳转 GitHub Issues 评论。</span><br><span>若没有本文 Issue，您可以使用 Comment 模版新建。</span><br><a class="hty-button hty-button--raised" id="github-issues" href="https://github.com/AlexZou14/alexzou14.github.io/issues?q=is:issue+SROBB:Targeted Perceptual Loss for Single Image Super-Resolution论文阅读笔记" target="_blank" rel="noopener">GitHub Issues</a></div><div class="comment-container" id="valine-container"></div></div><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.css"><script defer src="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.js"></script><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex@latest/dist/contrib/copy-tex.min.css"><script defer src="https://cdn.jsdelivr.net/npm/katex@latest/dist/contrib/copy-tex.min.js"></script><script defer src="https://cdn.jsdelivr.net/npm/katex@latest/dist/contrib/auto-render.min.js" onload="renderMathInElement(document.body);"></script></div></main><footer class="sidebar-translate" id="footer"><div class="copyright"><span>&copy; 2019 – 2020 </span><span class="with-love" id="animate"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-cloud-line"></use></svg></span><span class="author"> 秩同道合</span></div><div class="powered"><span>由 <a href="https://hexo.io" target="_blank" rel="noopener">Hexo</a> 驱动 v4.2.0</span><span class="footer-separator">|</span><span>主题 - <a rel="noopener" href="https://github.com/YunYouJun/hexo-theme-yun" target="_blank"><span>Yun</span></a> v0.3.1</span></div><script defer src="https://cdn.jsdelivr.net/npm/valine@latest/dist/Valine.min.js"></script><script>function initValine() {
  new Valine(CONFIG.valine);
}
document.addEventListener("DOMContentLoaded", function() {
  initValine();
});</script></footer><a class="hty-icon-button" id="goUp" aria-label="back-to-top" href="#"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-arrow-up-s-line"></use></svg></a></div><script defer src="/js/utils.js"></script><script defer src="/js/hexo-theme-yun.js"></script><script src="https://cdn.jsdelivr.net/npm/live2d-widget@^3.1.3/lib/L2Dwidget.min.js"></script><script>L2Dwidget.init({"pluginRootPath":"live2dw/","pluginJsPath":"lib/","pluginModelPath":"assets/","tagMode":false,"debug":false,"model":{"jsonPath":"https://cdn.jsdelivr.net/npm/live2d-widget-model-hijiki@1.0.5/assets/hijiki.model.json"},"display":{"position":"right","width":200,"height":400},"mobile":{"show":true},"react":{"opacity":0.8},"dialog":{"enable":true,"hitokoto":true},"log":false});</script></body></html>