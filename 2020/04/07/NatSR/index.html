<!DOCTYPE html><html lang="zh-CN"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0"><meta name="description" content="Natural and Realistic Single Image Super-Resolution with Explicit Natural Manifold Discrimination论文阅读笔记"><meta name="keywords" content="深度学习,笔记,超分辨率"><meta name="author" content="秩同道合"><meta name="copyright" content="秩同道合"><meta name="theme-color" content="#0078E7"><title>Natural and Realistic Single Image Super-Resolution with Explicit Natural Manifold Discrimination论文阅读笔记 | 秩同道合的小站</title><link rel="shortcut icon" href="https://cdn.jsdelivr.net/gh/AlexZou14/CDN/img/favicon.ico"><link rel="preload" href="/css/hexo-theme-yun.css" as="style"><link rel="preload" href="/js/utils.js" as="script"><link rel="preload" href="/js/hexo-theme-yun.js" as="script"><link rel="prefetch" href="/js/sidebar.js" as="script"><link rel="preconnect" href="https://cdn.jsdelivr.net" crossorigin><link rel="stylesheet" href="/css/hexo-theme-yun.css"><link rel="alternate" href="/atom.xml" title="秩同道合的小站"><script id="yun-config">
    let Yun = window.Yun || {};
    let CONFIG = {"root":"/","title":"秩同道合的小站","version":"0.3.1","anonymous_image":"https://cdn.jsdelivr.net/gh/YunYouJun/cdn/img/avatar/none.jpg","say":{"api":"https://v1.hitokoto.cn","hitokoto":true},"valine":{"el":"#valine-container","verify":false,"notify":false,"appId":"eCgP91hRSX8OtvCIR4MgLfcl-gzGzoHsz","appKey":"N5gVT8kUx5O0wMvc47SU040Y","serverURLs":null,"placeholder":"大佬求指教&nbsp_(:з」∠)_ （填写邮箱可以收到回复通知～）","avatar":null,"meta":["nick","mail","link"],"pageSize":10,"lang":"zh-cn","visitor":true},"fireworks":{"colors":["102, 167, 221","62, 131, 225","33, 78, 194"]}};
  </script><script src="//at.alicdn.com/t/font_1140697_pa6cswvjpq.js" async></script><meta name="generator" content="Hexo 4.2.0"><link rel="alternate" href="/atom.xml" title="秩同道合的小站" type="application/atom+xml">
</head><body><script defer src="https://cdn.jsdelivr.net/npm/animejs@latest/anime.min.js"></script><script defer src="/js/ui/fireworks.js"></script><canvas class="fireworks"></canvas><div class="container"><a class="sidebar-toggle sidebar-toggle-fixed hty-icon-button"><div class="hamburger hamburger--spin" type="button"><span class="hamburger-box"><span class="hamburger-inner"></span></span></div></a><aside class="sidebar"><ul class="sidebar-nav"><li class="sidebar-nav-item sidebar-nav-toc sidebar-nav-active hty-icon-button" data-target="post-toc-wrap" title="文章目录"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-list-ordered"></use></svg></li><li class="sidebar-nav-item sidebar-nav-overview hty-icon-button" data-target="site-overview-wrap" title="站点概览"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-passport-line"></use></svg></li></ul><div class="sidebar-panel" id="site-overview-wrap"><div class="site-info fix-top"><a class="site-author-avatar" href="/about" title="秩同道合"><img loading="lazy" src="https://cdn.jsdelivr.net/gh/AlexZou14/CDN/img/touxiang.jpg" alt="秩同道合"></a><div class="site-author-name"><a href="/about">秩同道合</a></div><a class="site-name" href="/about/site.html">秩同道合的小站</a><sub class="site-subtitle">寻找志趣相投的伙伴！</sub><div class="site-desciption">我和你，以及我们的秩相同所以我们才等价！</div></div><nav class="site-state"><a class="site-state-item hty-icon-button icon-home" href="/" title="首页"><span class="site-state-item-icon"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-home-4-line"></use></svg></span></a><div class="site-state-item site-state-posts"><a href="/archives" title="归档"><span class="site-state-item-icon"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-archive-line"></use></svg></span><span class="site-state-item-count">20</span></a></div><div class="site-state-item site-state-categories"><a href="/categories" title="分类"><span class="site-state-item-icon"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-folder-2-line"></use></svg></span><span class="site-state-item-count">1</span></a></div><div class="site-state-item site-state-tags"><a href="/tags" title="标签"><span class="site-state-item-icon"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-price-tag-3-line"></use></svg></span><span class="site-state-item-count">3</span></a></div><a class="site-state-item hty-icon-button" href="https://github.com/AlexZou14" target="_blank" rel="noopener" title="reward.comment"><span class="site-state-item-icon"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-hand-coin-line"></use></svg></span></a></nav><hr style="margin-bottom:0.5rem"><div class="links-of-author"><a class="links-of-author-item hty-icon-button" rel="noopener" href="/atom.xml" title="RSS" target="_blank" style="color:orange"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-rss-line"></use></svg></a><a class="links-of-author-item hty-icon-button" rel="noopener" href="https://github.com/AlexZou14" title="GitHub" target="_blank" style="color:#181717"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-github-line"></use></svg></a><a class="links-of-author-item hty-icon-button" rel="noopener" href="mailto:1120375574@qq.com" title="E-Mail" target="_blank" style="color:#8E71C1"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-mail-line"></use></svg></a><a class="links-of-author-item hty-icon-button" rel="noopener" href="https://space.bilibili.com/19164044" title="哔哩哔哩" target="_blank" style="color:#FF8EB3"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-bilibili-line"></use></svg></a></div><hr style="margin:0.5rem 1rem"><div class="links"><a class="links-item hty-icon-button" href="http://sotavision.cn" target="_blank" rel="noopener" title="工作组" style="color:#000000"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-send-plane-2-line"></use></svg></a></div></div><script defer src="/js/sidebar.js"></script><div class="sidebar-panel sidebar-panel-active" id="post-toc-wrap"><div class="post-toc"><div class="post-toc-progress"><div class="progress-bar"></div><div class="progress-info"><span class="progress-notice">您已经读了</span><span class="progress-num">0</span><span class="progress-percentage">%</span></div></div><div class="post-toc-content"><ol class="toc"><li class="toc-item toc-level-3"><a class="toc-link" href="#Abstract"><span class="toc-number">1.</span> <span class="toc-text">Abstract</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Introduction"><span class="toc-number">2.</span> <span class="toc-text">Introduction</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Related-work"><span class="toc-number">3.</span> <span class="toc-text">Related work</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Single-Image-Super-Resolution"><span class="toc-number">3.1.</span> <span class="toc-text">Single Image Super-Resolution</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Perception-Oriented-Super-Resolution"><span class="toc-number">3.2.</span> <span class="toc-text">Perception Oriented Super-Resolution</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Modeling-the-SISR"><span class="toc-number">3.3.</span> <span class="toc-text">Modeling the SISR</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Proposed-Method"><span class="toc-number">4.</span> <span class="toc-text">Proposed Method</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Natural-Manifold-Discrimination"><span class="toc-number">4.1.</span> <span class="toc-text">Natural Manifold Discrimination</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#Designing-Natural-Manifold"><span class="toc-number">4.1.1.</span> <span class="toc-text">Designing Natural Manifold</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#Natural-Manifold-Discriminator"><span class="toc-number">4.1.2.</span> <span class="toc-text">Natural Manifold Discriminator</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Natural-and-Realistic-Super-Resolution"><span class="toc-number">4.2.</span> <span class="toc-text">Natural and Realistic Super-Resolution</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Training-Loss-Function"><span class="toc-number">4.3.</span> <span class="toc-text">Training Loss Function</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Experiment"><span class="toc-number">5.</span> <span class="toc-text">Experiment</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Experiments-Details"><span class="toc-number">5.1.</span> <span class="toc-text">Experiments Details</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#FR-IQA-Results"><span class="toc-number">5.2.</span> <span class="toc-text">FR-IQA Results</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#NR-IQA-Results"><span class="toc-number">5.3.</span> <span class="toc-text">NR-IQA Results</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Conclusions"><span class="toc-number">6.</span> <span class="toc-text">Conclusions</span></a></li></ol></div></div></div></aside><main class="sidebar-translate" id="content"><div id="post"><article class="post-block" itemscope itemtype="http://schema.org/Article"><link itemprop="mainEntityOfPage" href="http://alexzou14.github.io/2020/04/07/NatSR/"><span hidden itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="name" content="秩同道合"><meta itemprop="description" content="Natural and Realistic Single Image Super-Resolution with Explicit Natural Manifold Discrimination论文阅读笔记"></span><span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="秩同道合的小站"></span><header class="post-header"><h1 class="post-title" itemprop="name headline">Natural and Realistic Single Image Super-Resolution with Explicit Natural Manifold Discrimination论文阅读笔记</h1><div class="post-meta"><span class="post-time"><span class="post-meta-item-icon"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-calendar-line"></use></svg></span> <time title="创建时间：2020-04-07 18:52:26" itemprop="dateCreated datePublished" datetime="2020-04-07T18:52:26+08:00">2020-04-07</time></span><div class="post-count"><span class="post-symbolcount"><span class="post-meta-item-icon" title="本文字数"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-file-word-line"></use></svg></span> <span title="本文字数">7.7k</span><span class="post-meta-divider">-</span><span class="post-meta-item-icon" title="阅读时长"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-timer-line"></use></svg></span> <span title="阅读时长">15 分钟</span></span></div><span class="leancloud_visitors" id="/2020/04/07/NatSR/" data-flag-title="Natural and Realistic Single Image Super-Resolution with Explicit Natural Manifold Discrimination论文阅读笔记"><span class="post-meta-item-icon" title="阅读次数"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-eye-line"></use></svg> <span class="leancloud-visitors-count"></span></span></span><div class="post-classify"><span class="post-category"><span class="post-meta-item-icon"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-folder-line"></use></svg></span> <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a class="category" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/" itemprop="url" rel="index"><span itemprop="text">论文笔记</span></a></span></span><span class="post-tag"><span class="post-meta-divider">-</span><a class="tag" href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"><span class="post-meta-item-icon"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-price-tag-3-line"></use></svg></span><span class="tag-name">深度学习</span></a><a class="tag" href="/tags/%E7%AC%94%E8%AE%B0/"><span class="post-meta-item-icon"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-price-tag-3-line"></use></svg></span><span class="tag-name">笔记</span></a><a class="tag" href="/tags/%E8%B6%85%E5%88%86%E8%BE%A8%E7%8E%87/"><span class="post-meta-item-icon"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-price-tag-3-line"></use></svg></span><span class="tag-name">超分辨率</span></a></span></div></div></header><section class="post-body" itemprop="articleBody"><div class="post-content post-markdown"><h3 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h3><p>许多使用CNN网络的SISR模型使用失真导向（distortion-oriented）的损失函数，这类模型很难恢复真实图像纹理内容和细节，看起来较模糊，没有较好的视觉效果。恢复真实纹理和细节在图像超分辨领域仍然是一项挑战，目前有一些关于这方面的工作，如SRGAN，EnhancedNet，SFT-GAN，但是，这些方法在生成这些不真实（fake）细节时，通常会产生不需要的伪影，整幅图像看起来总有一些不自然。文中，提出了一种重建真实超分辨率图像的方法，重构的图像具有非常高的视觉效果并保持图像的真实性。作者在低层图像域定义了真实先验（naturalness prior），并约束重构图像在自然流形（natural manifold）。</p>
<hr>
<h3 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h3><p>单一图像超分率（SISR）是一个传统的图像恢复问题，主要目的是将低分辨率图像恢复成高分辨率图像。由于SISR是一个不定问题，不同的SR方法会产生不一样的图像。目前做SR的方法无论是使用MSE还是使用GAN的方法，都会产生不自然的图像效果。使用MSE损失函数会使得恢复图像过于平滑，使用现有感知损失如SRGAN，EhanceNet,会产生一些不需要的细节、伪影等。如下图：</p>
<blockquote>
<p><img src="http://sotavision.cn/showdoc/server/../Public/Uploads/2020-03-24/5e79cd373c660.png" alt="" loading="lazy"></p>
</blockquote>
<p>所以本文借鉴了SR的领域先验知识，提出一个方法来约束低级领域先验来代替高级语言。设计一个判别器判定图像的自然程度，以此为监督对图像做SR，这是这篇论文的基本思路。<br>本文主要贡献有：</p>
<ol>
<li>设计了基于CNN网络的自然流形鉴别器（natural manifold discriminator）。</li>
<li>基于不规则残差学习的CNN结构，distortion-oriented，即fractal residual super-resolution (FRSR)。</li>
<li>我们提出了一种面向感知的SISR方法，（NatSR)网络，可生成真实纹理和自然细节，获得高视觉质量。</li>
</ol>
<hr>
<h3 id="Related-work"><a href="#Related-work" class="headerlink" title="Related work"></a>Related work</h3><h4 id="Single-Image-Super-Resolution"><a href="#Single-Image-Super-Resolution" class="headerlink" title="Single Image Super-Resolution"></a>Single Image Super-Resolution</h4><p>这里主要讲了早期SR方法，不管是传统方法还是深度学习方法都是一鉴别失真为导向的，目的是为了达到更高的PSNR。这些方法往往会导致生成的图像过于平滑，感知细节丢失等问题。</p>
<h4 id="Perception-Oriented-Super-Resolution"><a href="#Perception-Oriented-Super-Resolution" class="headerlink" title="Perception Oriented Super-Resolution"></a>Perception Oriented Super-Resolution</h4><p>由于上面SR方法为了达到更高的PSNR从而使得生成图像更平滑，所以近期面向感知的方法受到广泛关注，并且Johson提出像素域感知不是感知质量的最优解，特征空间损失才更符合人类感知模型。所以近期的SFT-GAN通过调整目标像素语义类别来限制特征空间达到更好的效果。</p>
<h4 id="Modeling-the-SISR"><a href="#Modeling-the-SISR" class="headerlink" title="Modeling the SISR"></a>Modeling the SISR</h4><p>论文认为LR和HR在频率域上的关系如下所示，如果对LR做SR的结果可能会出现图下面的情形:</p>
<blockquote>
<p><img src="http://sotavision.cn/showdoc/server/../Public/Uploads/2020-03-24/5e79d02537df0.png" alt="" loading="lazy"></p>
</blockquote>
<p>所以LR-HR的对应关系可以描述成：\( I_{LR}=h(I_{HR})^{\downarrow} \)<br>由于SISR是为给定的LR找到HR，所以它通常被建模为找到条件似然\( p(I_{HR}|I_{LR}) \)。</p>
<hr>
<h3 id="Proposed-Method"><a href="#Proposed-Method" class="headerlink" title="Proposed Method"></a>Proposed Method</h3><h4 id="Natural-Manifold-Discrimination"><a href="#Natural-Manifold-Discrimination" class="headerlink" title="Natural Manifold Discrimination"></a>Natural Manifold Discrimination</h4><h5 id="Designing-Natural-Manifold"><a href="#Designing-Natural-Manifold" class="headerlink" title="Designing Natural Manifold"></a>Designing Natural Manifold</h5><p>根据上图这里将图像HR space分成如下图的三类，如下图所示：</p>
<blockquote>
<p><img src="http://sotavision.cn/showdoc/server/../Public/Uploads/2020-03-24/5e79d1009156b.png" alt="" loading="lazy"></p>
</blockquote>
<p>其中模糊集A可以定义成：\( A={I_A|I_A=(1-\alpha)h(I_{LR}^{\uparrow})+\alpha I_{HR}} \)<br>模糊集A中的图像降置为LR可以描述为：</p>
<p>\( h(I_A)^{\downarrow} \)<br>\( =h((1-\alpha)h(I_{LR}^{\uparrow})+\alpha I_{HR})^{\downarrow} \)<br>\( =h((1-\alpha)h(I_{LR}^{\uparrow}))^{\downarrow}+h(\alpha I_{HR})^{\downarrow} \)<br>\( =(1-\alpha)h(I_{LR}^{\uparrow})^{\downarrow}+\alpha h(I_{HR})^{\downarrow} \)<br>\( =(1-\alpha)h(I_{LR})+\alpha h(I_{LR}) \)<br>\( =I_{LR} \)</p>
<p>噪声集B可以描述为：\( B={I_B|I_B=I_{HR}+n} \)<br>模糊集B中的图像降置为LR可以描述为：</p>
<p>\( h(I_B)^{\downarrow} \)<br>\( =h(I_{HR}+n)^{\downarrow} \)<br>\( =h(I_{HR})^{\downarrow}+h(n)^{\downarrow} \)<br>\( =h(I_{HR})^{\downarrow} \)<br>\( =I_{LR} \)</p>
<p>论文论证了模糊和带噪的SR图片下采样后都会等价于LR图片。</p>
<h5 id="Natural-Manifold-Discriminator"><a href="#Natural-Manifold-Discriminator" class="headerlink" title="Natural Manifold Discriminator"></a>Natural Manifold Discriminator</h5><p>这里论文构建了两类这样的图片，然后设计了一个判别器进行分类学习，具体设置见论文，判别器如下所示：</p>
<blockquote>
<p><img src="http://sotavision.cn/showdoc/server/../Public/Uploads/2020-03-24/5e79d4d569e85.png" alt="" loading="lazy"></p>
</blockquote>
<p>其中这个判别器的交叉熵损失函数定义为：</p>
<div>
    \[ - \text{E}_{x\in{A\cup B}}[\log(1-D_{NM}(x))]-\text{E}_{x\in{N}}[\log(D_{NM}(x))] \]
</div>

<h4 id="Natural-and-Realistic-Super-Resolution"><a href="#Natural-and-Realistic-Super-Resolution" class="headerlink" title="Natural and Realistic Super-Resolution"></a>Natural and Realistic Super-Resolution</h4><p>论文的SR网络，使用了残差dense block，网络结构如下所示：</p>
<blockquote>
<p><img src="http://sotavision.cn/showdoc/server/../Public/Uploads/2020-03-24/5e79d72147ae5.png" alt="" loading="lazy"><br><img src="http://sotavision.cn/showdoc/server/../Public/Uploads/2020-03-24/5e79d698d801c.png" alt="" loading="lazy"><br><img src="http://sotavision.cn/showdoc/server/../Public/Uploads/2020-03-24/5e79d73b9ccbf.png" alt="" loading="lazy"></p>
</blockquote>
<h4 id="Training-Loss-Function"><a href="#Training-Loss-Function" class="headerlink" title="Training Loss Function"></a>Training Loss Function</h4><ul>
<li>Reconstruction Loss</li>
</ul>
<p>\[ L_{Recon}=\text{E}[\left|I_{HR}-I_{SR} \right|_1] \]</p>
<ul>
<li>Naturalness Loss</li>
</ul>
<p>\[ L_{Natural}=\text{E}[-\log(D_{MN}(I_{SR}))] \]</p>
<ul>
<li>Adversarial Loss<br>使用相对真实生成对抗网络RaGAN，与ESRGAN使用的GAN相同。</li>
</ul>
<div>
    \[ L_{G}=-\text{E}_{x_r\sim \text P_r}[\log(\tilde{D}(x_r))]-\text{E}_{x_f\sim \text P_g}[\log(1-\tilde{D}(x_f))] \]
    \[ L_{D}=-\text{E}_{x_f\sim \text P_g}[\log(\tilde{D}(x_f))]-\text{E}_{x_r\sim \text P_r}[\log(1-\tilde{D}(x_r))] \]
</div>

<p>其中：</p>
<div>
    \[ \tilde{D}(x_r)=\text{sigmoid}(C(x_r)-\text E_{x_f\sim \text P_g}[C(x_f)]) \]
    \[ \tilde{D}(x_f)=\text{sigmoid}(C(x_f)-\text E_{x_r\sim \text P_r}[C(x_r)]) \]
</div>
- Overall Loss

<p>\[ L=\lambda_1L_{Recon}+\lambda_2L_{Natural}+\lambda_3L_{G} \]<br>FRSR:\( \lambda_2=\lambda_3=0 \)<br>NatSR：\( \lambda_1=1,\lambda_2=10^{-3}\text{and}\lambda_3=10^{-3} \)</p>
<hr>
<h3 id="Experiment"><a href="#Experiment" class="headerlink" title="Experiment"></a>Experiment</h3><h4 id="Experiments-Details"><a href="#Experiments-Details" class="headerlink" title="Experiments Details"></a>Experiments Details</h4><p>实验细节可以参考官方代码：<a href="https://github.com/JWSoh/NatSR" target="_blank" rel="noopener">https://github.com/JWSoh/NatSR</a>.</p>
<h4 id="FR-IQA-Results"><a href="#FR-IQA-Results" class="headerlink" title="FR-IQA Results"></a>FR-IQA Results</h4><blockquote>
<p><img src="http://sotavision.cn/showdoc/server/../Public/Uploads/2020-03-24/5e79dc87c7f6d.png" alt="" loading="lazy"></p>
</blockquote>
<ul>
<li>考虑到参数的数量，我们的FRSR也是一种有效的方法</li>
<li>对于面向感知的方法，我们的方法比SRGAN和EnhanceNet在像素域上更接近原始图像</li>
</ul>
<h4 id="NR-IQA-Results"><a href="#NR-IQA-Results" class="headerlink" title="NR-IQA Results"></a>NR-IQA Results</h4><blockquote>
<p><img src="http://sotavision.cn/showdoc/server/../Public/Uploads/2020-03-24/5e79dced15055.png" alt="" loading="lazy"><br>从图像评价指标上看，NatSR算法并不是最好的，但是作者认为，在视觉效果上，NatSR效果更好，在图像纹理和细节方面表现更好。</p>
</blockquote>
<hr>
<h3 id="Conclusions"><a href="#Conclusions" class="headerlink" title="Conclusions"></a>Conclusions</h3><ol>
<li>提出了一种基于残差密集块和分形残差学习的网络结构</li>
<li>提出了基于CNN的自然流形判别器（NMD）</li>
<li>设计了一种感知损失函数</li>
<li>与具有相似参数的模型相比，我们的方法具有相当大的增益。</li>
</ol>
</div><ul class="post-copyright"><li class="post-copyright-author"><strong>本文作者：</strong>秩同道合</li><li class="post-copyright-link"><strong>本文链接：</strong><a href="http://alexzou14.github.io/2020/04/07/NatSR/" title="Natural and Realistic Single Image Super-Resolution with Explicit Natural Manifold Discrimination论文阅读笔记">http://alexzou14.github.io/2020/04/07/NatSR/</a></li><li class="post-copyright-license"><strong>版权声明：</strong>本博客所有文章除特别声明外，均默认采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh" target="_blank" rel="noopener" title="CC BY-NC-SA 4.0 "><svg class="icon"><use xlink:href="#icon-creative-commons-line"></use></svg><svg class="icon"><use xlink:href="#icon-creative-commons-by-line"></use></svg><svg class="icon"><use xlink:href="#icon-creative-commons-nc-line"></use></svg><svg class="icon"><use xlink:href="#icon-creative-commons-sa-line"></use></svg></a> 许可协议。</li></ul></section></article><div class="post-nav"><div class="post-nav-item"><a class="post-nav-prev" href="/2020/04/07/SROBB/" rel="prev" title="SROBB:Targeted Perceptual Loss for Single Image Super-Resolution论文阅读笔记"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-arrow-left-s-line"></use></svg><span class="post-nav-text">SROBB:Targeted Perceptual Loss for Single Image Super-Resolution论文阅读笔记</span></a></div><div class="post-nav-item"><a class="post-nav-next" href="/2020/04/07/DRRN/" rel="next" title="Image Super-Resolution via Deep Recursive Residual Network论文阅读笔记"><span class="post-nav-text">Image Super-Resolution via Deep Recursive Residual Network论文阅读笔记</span><svg class="icon" aria-hidden="true"><use xlink:href="#icon-arrow-right-s-line"></use></svg></a></div></div><div id="comment"><div class="comment-tooltip text-center"><span>若您无 GitHub 账号，可直接在下方匿名评论。</span><br><span>若您想及时得到回复提醒，建议跳转 GitHub Issues 评论。</span><br><span>若没有本文 Issue，您可以使用 Comment 模版新建。</span><br><a class="hty-button hty-button--raised" id="github-issues" href="https://github.com/AlexZou14/alexzou14.github.io/issues?q=is:issue+Natural and Realistic Single Image Super-Resolution with Explicit Natural Manifold Discrimination论文阅读笔记" target="_blank" rel="noopener">GitHub Issues</a></div><div class="comment-container" id="valine-container"></div></div><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.css"><script defer src="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.js"></script><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex@latest/dist/contrib/copy-tex.min.css"><script defer src="https://cdn.jsdelivr.net/npm/katex@latest/dist/contrib/copy-tex.min.js"></script><script defer src="https://cdn.jsdelivr.net/npm/katex@latest/dist/contrib/auto-render.min.js" onload="renderMathInElement(document.body);"></script></div></main><footer class="sidebar-translate" id="footer"><div class="copyright"><span>&copy; 2019 – 2020 </span><span class="with-love" id="animate"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-cloud-line"></use></svg></span><span class="author"> 秩同道合</span></div><div class="powered"><span>由 <a href="https://hexo.io" target="_blank" rel="noopener">Hexo</a> 驱动 v4.2.0</span><span class="footer-separator">|</span><span>主题 - <a rel="noopener" href="https://github.com/YunYouJun/hexo-theme-yun" target="_blank"><span>Yun</span></a> v0.3.1</span></div><script defer src="https://cdn.jsdelivr.net/npm/valine@latest/dist/Valine.min.js"></script><script>function initValine() {
  new Valine(CONFIG.valine);
}
document.addEventListener("DOMContentLoaded", function() {
  initValine();
});</script></footer><a class="hty-icon-button" id="goUp" aria-label="back-to-top" href="#"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-arrow-up-s-line"></use></svg></a></div><script defer src="/js/utils.js"></script><script defer src="/js/hexo-theme-yun.js"></script><script src="https://cdn.jsdelivr.net/npm/live2d-widget@^3.1.3/lib/L2Dwidget.min.js"></script><script>L2Dwidget.init({"pluginRootPath":"live2dw/","pluginJsPath":"lib/","pluginModelPath":"assets/","tagMode":false,"debug":false,"model":{"jsonPath":"https://cdn.jsdelivr.net/npm/live2d-widget-model-hijiki@1.0.5/assets/hijiki.model.json"},"display":{"position":"right","width":200,"height":400},"mobile":{"show":true},"react":{"opacity":0.8},"dialog":{"enable":true,"hitokoto":true},"log":false});</script></body></html>