<!DOCTYPE html><html lang="zh-CN"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0"><meta name="description" content="Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network论文阅读笔记"><meta name="keywords" content="深度学习,笔记,超分辨率"><meta name="author" content="秩同道合"><meta name="copyright" content="秩同道合"><meta name="theme-color" content="#0078E7"><title>Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network论文阅读笔记 | 秩同道合的小站</title><link rel="shortcut icon" href="https://cdn.jsdelivr.net/gh/AlexZou14/CDN/img/favicon.ico"><link rel="preload" href="/css/hexo-theme-yun.css" as="style"><link rel="preload" href="/js/utils.js" as="script"><link rel="preload" href="/js/hexo-theme-yun.js" as="script"><link rel="prefetch" href="/js/sidebar.js" as="script"><link rel="preconnect" href="https://cdn.jsdelivr.net" crossorigin><link rel="stylesheet" href="/css/hexo-theme-yun.css"><link rel="alternate" href="/atom.xml" title="秩同道合的小站"><script id="yun-config">
    let Yun = window.Yun || {};
    let CONFIG = {"root":"/","title":"秩同道合的小站","version":"0.3.1","anonymous_image":"https://cdn.jsdelivr.net/gh/YunYouJun/cdn/img/avatar/none.jpg","say":{"api":"https://v1.hitokoto.cn","hitokoto":true},"valine":{"el":"#valine-container","verify":false,"notify":false,"appId":"eCgP91hRSX8OtvCIR4MgLfcl-gzGzoHsz","appKey":"N5gVT8kUx5O0wMvc47SU040Y","serverURLs":null,"placeholder":"大佬求指教&nbsp_(:з」∠)_ （填写邮箱可以收到回复通知～）","avatar":null,"meta":["nick","mail","link"],"pageSize":10,"lang":"zh-cn","visitor":true},"fireworks":{"colors":["102, 167, 221","62, 131, 225","33, 78, 194"]}};
  </script><script src="//at.alicdn.com/t/font_1140697_pa6cswvjpq.js" async></script><meta name="generator" content="Hexo 4.2.0"><link rel="alternate" href="/atom.xml" title="秩同道合的小站" type="application/atom+xml">
</head><body><script defer src="https://cdn.jsdelivr.net/npm/animejs@latest/anime.min.js"></script><script defer src="/js/ui/fireworks.js"></script><canvas class="fireworks"></canvas><div class="container"><a class="sidebar-toggle sidebar-toggle-fixed hty-icon-button"><div class="hamburger hamburger--spin" type="button"><span class="hamburger-box"><span class="hamburger-inner"></span></span></div></a><aside class="sidebar"><ul class="sidebar-nav"><li class="sidebar-nav-item sidebar-nav-toc sidebar-nav-active hty-icon-button" data-target="post-toc-wrap" title="文章目录"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-list-ordered"></use></svg></li><li class="sidebar-nav-item sidebar-nav-overview hty-icon-button" data-target="site-overview-wrap" title="站点概览"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-passport-line"></use></svg></li></ul><div class="sidebar-panel" id="site-overview-wrap"><div class="site-info fix-top"><a class="site-author-avatar" href="/about" title="秩同道合"><img loading="lazy" src="https://cdn.jsdelivr.net/gh/AlexZou14/CDN/img/touxiang.jpg" alt="秩同道合"></a><div class="site-author-name"><a href="/about">秩同道合</a></div><a class="site-name" href="/about/site.html">秩同道合的小站</a><sub class="site-subtitle">寻找志趣相投的伙伴！</sub><div class="site-desciption">我和你，以及我们的秩相同所以我们才等价！</div></div><nav class="site-state"><a class="site-state-item hty-icon-button icon-home" href="/" title="首页"><span class="site-state-item-icon"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-home-4-line"></use></svg></span></a><div class="site-state-item site-state-posts"><a href="/archives" title="归档"><span class="site-state-item-icon"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-archive-line"></use></svg></span><span class="site-state-item-count">17</span></a></div><div class="site-state-item site-state-categories"><a href="/categories" title="分类"><span class="site-state-item-icon"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-folder-2-line"></use></svg></span><span class="site-state-item-count">1</span></a></div><div class="site-state-item site-state-tags"><a href="/tags" title="标签"><span class="site-state-item-icon"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-price-tag-3-line"></use></svg></span><span class="site-state-item-count">3</span></a></div><a class="site-state-item hty-icon-button" href="https://github.com/AlexZou14" target="_blank" rel="noopener" title="reward.comment"><span class="site-state-item-icon"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-hand-coin-line"></use></svg></span></a></nav><hr style="margin-bottom:0.5rem"><div class="links-of-author"><a class="links-of-author-item hty-icon-button" rel="noopener" href="/atom.xml" title="RSS" target="_blank" style="color:orange"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-rss-line"></use></svg></a><a class="links-of-author-item hty-icon-button" rel="noopener" href="https://github.com/AlexZou14" title="GitHub" target="_blank" style="color:#181717"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-github-line"></use></svg></a><a class="links-of-author-item hty-icon-button" rel="noopener" href="mailto:1120375574@qq.com" title="E-Mail" target="_blank" style="color:#8E71C1"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-mail-line"></use></svg></a><a class="links-of-author-item hty-icon-button" rel="noopener" href="https://space.bilibili.com/19164044" title="哔哩哔哩" target="_blank" style="color:#FF8EB3"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-bilibili-line"></use></svg></a></div><hr style="margin:0.5rem 1rem"><div class="links"><a class="links-item hty-icon-button" href="http://sotavision.cn" target="_blank" rel="noopener" title="工作组" style="color:#000000"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-send-plane-2-line"></use></svg></a></div></div><script defer src="/js/sidebar.js"></script><div class="sidebar-panel sidebar-panel-active" id="post-toc-wrap"><div class="post-toc"><div class="post-toc-progress"><div class="progress-bar"></div><div class="progress-info"><span class="progress-notice">您已经读了</span><span class="progress-num">0</span><span class="progress-percentage">%</span></div></div><div class="post-toc-content"><ol class="toc"><li class="toc-item toc-level-3"><a class="toc-link" href="#Abstract"><span class="toc-number">1.</span> <span class="toc-text">Abstract</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Introduction"><span class="toc-number">2.</span> <span class="toc-text">Introduction</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Related-work"><span class="toc-number">3.</span> <span class="toc-text">Related work</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Image-super-resolution"><span class="toc-number">3.1.</span> <span class="toc-text">Image super-resolution</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Design-of-convolutional-neural-networks"><span class="toc-number">3.2.</span> <span class="toc-text">Design of convolutional neural networks</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Contribution"><span class="toc-number">3.3.</span> <span class="toc-text">Contribution</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Proposed-Method"><span class="toc-number">4.</span> <span class="toc-text">Proposed Method</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Network-structure"><span class="toc-number">4.1.</span> <span class="toc-text">Network structure</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Loss-function"><span class="toc-number">4.2.</span> <span class="toc-text">Loss function</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Experiment"><span class="toc-number">5.</span> <span class="toc-text">Experiment</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Network-analysis"><span class="toc-number">5.1.</span> <span class="toc-text">Network analysis</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#Investigation-of-content-loss"><span class="toc-number">5.1.1.</span> <span class="toc-text">Investigation of content loss</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Performance-PSNR-time-vs-network-depth"><span class="toc-number">5.2.</span> <span class="toc-text">Performance (PSNR&#x2F;time) vs. network depth</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Mean-opinion-score-MOS-testing"><span class="toc-number">5.3.</span> <span class="toc-text">Mean opinion score (MOS) testing</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Comparisons-with-state-of-the-arts"><span class="toc-number">5.4.</span> <span class="toc-text">Comparisons with state-of-the-arts</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Conclusions"><span class="toc-number">6.</span> <span class="toc-text">Conclusions</span></a></li></ol></div></div></div></aside><main class="sidebar-translate" id="content"><div id="post"><article class="post-block" itemscope itemtype="http://schema.org/Article"><link itemprop="mainEntityOfPage" href="http://alexzou14.github.io/2020/04/11/SRGAN/"><span hidden itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="name" content="秩同道合"><meta itemprop="description" content="Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network论文阅读笔记"></span><span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="秩同道合的小站"></span><header class="post-header"><h1 class="post-title" itemprop="name headline">Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network论文阅读笔记</h1><div class="post-meta"><span class="post-time"><span class="post-meta-item-icon"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-calendar-line"></use></svg></span> <time title="创建时间：2020-04-11 18:04:44" itemprop="dateCreated datePublished" datetime="2020-04-11T18:04:44+08:00">2020-04-11</time></span><div class="post-count"><span class="post-symbolcount"><span class="post-meta-item-icon" title="本文字数"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-file-word-line"></use></svg></span> <span title="本文字数">7.2k</span><span class="post-meta-divider">-</span><span class="post-meta-item-icon" title="阅读时长"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-timer-line"></use></svg></span> <span title="阅读时长">14 分钟</span></span></div><span class="leancloud_visitors" id="/2020/04/11/SRGAN/" data-flag-title="Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network论文阅读笔记"><span class="post-meta-item-icon" title="阅读次数"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-eye-line"></use></svg> <span class="leancloud-visitors-count"></span></span></span><div class="post-classify"><span class="post-category"><span class="post-meta-item-icon"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-folder-line"></use></svg></span> <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a class="category" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/" itemprop="url" rel="index"><span itemprop="text">论文笔记</span></a></span></span><span class="post-tag"><span class="post-meta-divider">-</span><a class="tag" href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"><span class="post-meta-item-icon"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-price-tag-3-line"></use></svg></span><span class="tag-name">深度学习</span></a><a class="tag" href="/tags/%E7%AC%94%E8%AE%B0/"><span class="post-meta-item-icon"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-price-tag-3-line"></use></svg></span><span class="tag-name">笔记</span></a><a class="tag" href="/tags/%E8%B6%85%E5%88%86%E8%BE%A8%E7%8E%87/"><span class="post-meta-item-icon"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-price-tag-3-line"></use></svg></span><span class="tag-name">超分辨率</span></a></span></div></div></header><section class="post-body" itemprop="articleBody"><div class="post-content post-markdown"><h3 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h3><p>  在超分辨率领域飞速发展的时候，仍然有一个核心问题没有解决：当我们在用大的上采样因子实现图像恢复时，我们如何恢复更精细的纹理细节？基于优化的超分辨率方法的行为主要是由目标函数的选择驱动的。最近的工作主要集中在最小化方均根误差（MSE）上。由此产生的估计具有高峰值信噪比（PSNR），但它们通常缺乏高频细节，并且在感觉上不能令人满意。<br>这篇文章提出了以下的方法来处理这一问题：</p>
<ul>
<li>更改损失函数，将传统的MSE换成：对抗损失（perceptual loss）和内容损失（content loss）</li>
<li>引入对抗生成网络，将传统的像素空间的Content Loss转换为对抗性质的相似性。</li>
<li>引入深度残差网络，来提取图片中的细节纹理。</li>
</ul>
<hr>
<h3 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h3><p>SR问题有一个现状：就是尤其在大的采样因子的条件下，重构的图像通常都缺少细节纹理。因为他们是给予像素上的差异来优化。<br>SRGAN实现思路：</p>
<ul>
<li>采用具有skip connection的深度残差网络（ResNet），MSE作为content loss</li>
<li>采用VGG网络提取特征，并与Discriminator结合，来定义新的perceptual loss</li>
</ul>
<hr>
<h3 id="Related-work"><a href="#Related-work" class="headerlink" title="Related work"></a>Related work</h3><h4 id="Image-super-resolution"><a href="#Image-super-resolution" class="headerlink" title="Image super-resolution"></a>Image super-resolution</h4><p>基于预测的恢复方法：（插值）比如说线性差值，双三次差值，Lanczos滤波（注：其实就是基于插值的上采样方法），速度非常快，但是会产生关于平滑的图像，导致纹理边缘信息丢失。更加强大的方法是想要建立低分辨率图像到高分辨率图像的复杂映射，并且依赖于训练数据：</p>
<ul>
<li>利用图像中不同尺度的补丁来驱动SR。</li>
<li>自我字典通过延伸来进行小变换和形状变化。</li>
<li>卷积稀疏编码方法，通过处理整个图像而不是重叠补丁来提高一致性。</li>
</ul>
<p>为了重建真实的纹理细节，同时避免过度伪影，Tai等人将基于先验的梯度曲线的边缘定向SR算法与基于学习的细节合成的益处相结合。提出了一种多尺度字典来捕获不同尺度的相似图像块的冗余。<br>邻域嵌入方法通过在低维流形中找到类似的LR训练补片并将它们相应的HR补片组合用于重建来上采样LRimage补片。</p>
<h4 id="Design-of-convolutional-neural-networks"><a href="#Design-of-convolutional-neural-networks" class="headerlink" title="Design of convolutional neural networks"></a>Design of convolutional neural networks</h4><p>卷积神经网络的引入给SR问题带来了新的方法：</p>
<ul>
<li>SRCNN将稀疏表示编码到神经网络架构中，使用双三次插值来放大输入图像并对端到端的三层深度卷积网络进行训练，以实现最先进的SR性能。</li>
<li>随后，表明使网络能够直接学习上采样滤波器可以进一步提高准确度和速度的性能。凭借其深度递归卷积网络（DRCN），Kim等人提出了一种高性能的架构，允许长距离像素依赖，同时保持模型参数的数量很少。</li>
<li>Johnson等人的着作，和布鲁纳等人。 依靠更接近感知相似性的损失函数来恢复视觉上更有说服力的HR图像。</li>
</ul>
<h4 id="Contribution"><a href="#Contribution" class="headerlink" title="Contribution"></a>Contribution</h4><ul>
<li>使用ResNet优化了过去的MSE损失。</li>
<li>提出SRGAN，这是一个基于GAN的网络，为新的感知损失进行了优化。在这里，我们用基于VGG网络的特征映射的损失计算替代基于MSE的内容丢失[49]，这对于像素空间的变化更加不变。</li>
<li>通过对来自三个公共基准数据集的图像进行广泛的平均意见得分（MOS）测试来证实，SRGAN是一种新的技术水平，通过大幅度的边缘，用于估计具有高放大因子的照片般逼真的重构图像。</li>
</ul>
<hr>
<h3 id="Proposed-Method"><a href="#Proposed-Method" class="headerlink" title="Proposed Method"></a>Proposed Method</h3><h4 id="Network-structure"><a href="#Network-structure" class="headerlink" title="Network structure"></a>Network structure</h4><p>网络结构如下图所示：</p>
<blockquote>
<p><img src="http://sotavision.cn/showdoc/server/../Public/Uploads/2020-02-24/5e5377a6492d0.png" alt="" loading="lazy"></p>
</blockquote>
<p>这里生成网络运用了SRResNet，对抗网络用了VGG网络。<br>生成网络优化的参数表达式为：</p>
<div>
    \[ \hat{\theta}_G=\displaystyle\argmin_{\theta_G} \frac{1}{N} \sum_{n=1}^{N} l^{SR}(G_{\theta_G}(I_n^{LR}),I_n^{HR}) \]
</div>

<p>其中\(\hat{\theta}_G\)是指生成网络的参数，\({I^{LR}}\)是指\({I^{HR}}\)通过高斯滤波器降置得到的。<br>我们进一步定义了一个鉴别器网络\(D_{\theta_D}\)，我们与\(G_{\theta_G}\)交替优化，以解决以下问题：</p>
<div>
    \[ \min_{G_{\theta_G}}\max_{D_{\theta_D}}{E_{I^{HR}\sim {p_{train}(I^{HR})}}}[\log{D_{\theta_D}}(I^{HR})]+{E_{I^{LR}\sim {p_{G}(I^{LR})}}}[\log(1-{D_{\theta_D}}(G_{\theta_G}(I^{LR})))] \]
</div>

<p>这里就相当于让LR图像经过生成网络得到一张SR图像，用SR图像进过鉴别器来判别这个图像是否和HR图像接近，如果否则仅需通过生成器生成图像，如果是则输出SR图像。所以这里最小化生成器的损失函数，最大化判别器的损失函数。</p>
<h4 id="Loss-function"><a href="#Loss-function" class="headerlink" title="Loss function"></a>Loss function</h4><ul>
<li>Perceptual loss function</li>
</ul>
<blockquote>
<p>\[ l^{SR}=l_X^{SR}+10^{-3}l_{Gen}^{SR} \]<br>其中\(l_X^{SR}\)是内容损失函数，\(l_{Gen}^{SR}\)是对抗损失函数。这里是用上面的损失函数来使得生成器的生成的图片接近原图</p>
</blockquote>
<ul>
<li>Content loss</li>
</ul>
<blockquote>
<p>这里可以使用两个内容损失函数：<br>MSE loss: </p>
</blockquote>
<div>
    \[l_{MSE}^{SR}=\dfrac{1}{r^2WH}\displaystyle\sum_{x=1}^{rW}\sum_{y=1}^{rH}(I_{x,y}^{HR}-G_{\theta_G}(I^{LR})_{x,y})^2\]
</div>

<p>VGG loss:</p>
<div>
    \[l_{VGG/i,j}^{SR}=\dfrac{1}{W_{i,j}H_{i,j}}\displaystyle\sum_{x=1}^{W_{i,j}}\sum_{y=1}^{H_{i,j}}(\phi_{i,j}(I^{HR})_{x,y}-\phi_{i,j}(G_{\theta_G}(I^{LR}))_{x,y})^2\]
</div>

<ul>
<li>Adversarial loss</li>
</ul>
<blockquote>
<p>\[l_{Gen}^{SR}=\displaystyle\sum_{n=1}^{N}-\log{D_{\theta_D}(G_{\theta_G}(I^{LR}))} \]<br>这里用优化上面的函数来替代之前提出的对抗损失函数来优化对抗网络，使鉴别器的鉴别的效果最大化</p>
</blockquote>
<hr>
<h3 id="Experiment"><a href="#Experiment" class="headerlink" title="Experiment"></a>Experiment</h3><h4 id="Network-analysis"><a href="#Network-analysis" class="headerlink" title="Network analysis"></a>Network analysis</h4><h5 id="Investigation-of-content-loss"><a href="#Investigation-of-content-loss" class="headerlink" title="Investigation of content loss"></a>Investigation of content loss</h5><blockquote>
<p><img src="http://sotavision.cn/showdoc/server/../Public/Uploads/2020-02-24/5e539ef6a1f4f.png" alt="" loading="lazy"></p>
</blockquote>
<p>SRGAN网络可以极大的提升生成图像的感知质量，都可以取得较高的MOS分数。</p>
<h4 id="Performance-PSNR-time-vs-network-depth"><a href="#Performance-PSNR-time-vs-network-depth" class="headerlink" title="Performance (PSNR/time) vs. network depth"></a>Performance (PSNR/time) vs. network depth</h4><p>文中研究了网络深度和PSNR结果值及网络预测时间的关系,同时还实验了skip-connection的影响,其中网络的深度是通过调整残差快的数量来实现的,注意网络最终选取的数量为16.结果图如下:</p>
<blockquote>
<p><img src="http://sotavision.cn/showdoc/server/../Public/Uploads/2020-02-24/5e53a181eea4f.png" alt="" loading="lazy"></p>
</blockquote>
<ul>
<li>注意实验中以PSNR值来衡量生成图像质量,从图中可以看出随着深度增加PSNR值逐渐增大,但是增加速度逐渐变慢,这也证实了增加网络深度可以提升生成图像质量.</li>
<li>左图中的蓝线与红线的差别就是有无skip-connection,这也证实了其必要性.</li>
<li>预测时间随深度增加线性增长,skip-connection并不影响预测时间.</li>
</ul>
<h4 id="Mean-opinion-score-MOS-testing"><a href="#Mean-opinion-score-MOS-testing" class="headerlink" title="Mean opinion score (MOS) testing"></a>Mean opinion score (MOS) testing</h4><p>文中在Set5,Set14和BSD100三种数据集上对各种方法找人类受试者进行了打分,其中分值越大表示效果越好.结果如下:</p>
<ul>
<li>MOS:平均主观得分,此标准更符合人类的感知</li>
<li>Set5数据集分辨率较低,在做放大之后效果区分并不明显,不过从分辨率最高的BSD100数据集上的结果可以明显看出SRGAN的效果最好.<blockquote>
<p><img src="http://sotavision.cn/showdoc/server/../Public/Uploads/2020-02-24/5e53a27883a8e.png" alt="" loading="lazy"></p>
</blockquote>
</li>
</ul>
<h4 id="Comparisons-with-state-of-the-arts"><a href="#Comparisons-with-state-of-the-arts" class="headerlink" title="Comparisons with state-of-the-arts"></a>Comparisons with state-of-the-arts</h4><blockquote>
<p><img src="http://sotavision.cn/showdoc/server/../Public/Uploads/2020-02-24/5e53a2b980f5c.png" alt="" loading="lazy"></p>
</blockquote>
<p>由上图可以得知，PSNR和SSIM数值虽然不是最高，但是效果也不差，平均意见分数在所有方法中获得的最高。</p>
<hr>
<h3 id="Conclusions"><a href="#Conclusions" class="headerlink" title="Conclusions"></a>Conclusions</h3><ul>
<li>使用ResNet优化了过去的MSE损失。</li>
<li>提出SRGAN，这是一个基于GAN的网络，为新的感知损失进行了优化。在这里，我们用基于VGG网络的特征映射的损失计算替代基于MSE的内容丢失[49]，这对于像素空间的变化更加不变。</li>
<li>通过对来自三个公共基准数据集的图像进行广泛的平均意见得分（MOS）测试来证实，SRGAN是一种新的技术水平，通过大幅度的边缘，用于估计具有高放大因子的照片般逼真的重构图像。</li>
</ul>
</div><ul class="post-copyright"><li class="post-copyright-author"><strong>本文作者：</strong>秩同道合</li><li class="post-copyright-link"><strong>本文链接：</strong><a href="http://alexzou14.github.io/2020/04/11/SRGAN/" title="Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network论文阅读笔记">http://alexzou14.github.io/2020/04/11/SRGAN/</a></li><li class="post-copyright-license"><strong>版权声明：</strong>本博客所有文章除特别声明外，均默认采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh" target="_blank" rel="noopener" title="CC BY-NC-SA 4.0 "><svg class="icon"><use xlink:href="#icon-creative-commons-line"></use></svg><svg class="icon"><use xlink:href="#icon-creative-commons-by-line"></use></svg><svg class="icon"><use xlink:href="#icon-creative-commons-nc-line"></use></svg><svg class="icon"><use xlink:href="#icon-creative-commons-sa-line"></use></svg></a> 许可协议。</li></ul></section></article><div class="post-nav"><div class="post-nav-item"><a class="post-nav-prev" href="/2020/04/11/RealSR/" rel="prev" title="Toward Real-World Single Image Super-Resolution:A New Benchmark and A New Model论文阅读笔记"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-arrow-left-s-line"></use></svg><span class="post-nav-text">Toward Real-World Single Image Super-Resolution:A New Benchmark and A New Model论文阅读笔记</span></a></div><div class="post-nav-item"><a class="post-nav-next" href="/2020/04/11/VDSR/" rel="next" title="Accurate Image Super-Resolution Using Very Deep Convolutional Networks论文阅读笔记"><span class="post-nav-text">Accurate Image Super-Resolution Using Very Deep Convolutional Networks论文阅读笔记</span><svg class="icon" aria-hidden="true"><use xlink:href="#icon-arrow-right-s-line"></use></svg></a></div></div><div id="comment"><div class="comment-tooltip text-center"><span>若您无 GitHub 账号，可直接在下方匿名评论。</span><br><span>若您想及时得到回复提醒，建议跳转 GitHub Issues 评论。</span><br><span>若没有本文 Issue，您可以使用 Comment 模版新建。</span><br><a class="hty-button hty-button--raised" id="github-issues" href="https://github.com/AlexZou14/alexzou14.github.io/issues?q=is:issue+Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network论文阅读笔记" target="_blank" rel="noopener">GitHub Issues</a></div><div class="comment-container" id="valine-container"></div></div><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.css"><script defer src="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.js"></script><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex@latest/dist/contrib/copy-tex.min.css"><script defer src="https://cdn.jsdelivr.net/npm/katex@latest/dist/contrib/copy-tex.min.js"></script><script defer src="https://cdn.jsdelivr.net/npm/katex@latest/dist/contrib/auto-render.min.js" onload="renderMathInElement(document.body);"></script></div></main><footer class="sidebar-translate" id="footer"><div class="copyright"><span>&copy; 2019 – 2020 </span><span class="with-love" id="animate"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-cloud-line"></use></svg></span><span class="author"> 秩同道合</span></div><div class="powered"><span>由 <a href="https://hexo.io" target="_blank" rel="noopener">Hexo</a> 驱动 v4.2.0</span><span class="footer-separator">|</span><span>主题 - <a rel="noopener" href="https://github.com/YunYouJun/hexo-theme-yun" target="_blank"><span>Yun</span></a> v0.3.1</span></div><script defer src="https://cdn.jsdelivr.net/npm/valine@latest/dist/Valine.min.js"></script><script>function initValine() {
  new Valine(CONFIG.valine);
}
document.addEventListener("DOMContentLoaded", function() {
  initValine();
});</script></footer><a class="hty-icon-button" id="goUp" aria-label="back-to-top" href="#"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-arrow-up-s-line"></use></svg></a></div><script defer src="/js/utils.js"></script><script defer src="/js/hexo-theme-yun.js"></script><script src="https://cdn.jsdelivr.net/npm/live2d-widget@^3.1.3/lib/L2Dwidget.min.js"></script><script>L2Dwidget.init({"pluginRootPath":"live2dw/","pluginJsPath":"lib/","pluginModelPath":"assets/","tagMode":false,"debug":false,"model":{"jsonPath":"https://cdn.jsdelivr.net/npm/live2d-widget-model-hijiki@1.0.5/assets/hijiki.model.json"},"display":{"position":"right","width":200,"height":400},"mobile":{"show":true},"react":{"opacity":0.8},"dialog":{"enable":true,"hitokoto":true},"log":false});</script></body></html>