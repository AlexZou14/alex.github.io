<!DOCTYPE html><html lang="zh-CN"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0"><meta name="description" content="ICCV2021 盲图像超分MANet:空间可变模糊核估计的互仿射网络"><meta name="keywords" content="深度学习,笔记,超分辨率"><meta name="author" content="秩同道合"><meta name="copyright" content="秩同道合"><meta name="theme-color" content="#0078E7"><title>ICCV2021 盲图像超分MANet:空间可变模糊核估计的互仿射网络 | 秩同道合的小站</title><link rel="shortcut icon" href="https://cdn.jsdelivr.net/gh/AlexZou14/CDN/img/favicon.ico"><link rel="preload" href="/css/hexo-theme-yun.css" as="style"><link rel="preload" href="/js/utils.js" as="script"><link rel="preload" href="/js/hexo-theme-yun.js" as="script"><link rel="prefetch" href="/js/sidebar.js" as="script"><link rel="preconnect" href="https://cdn.jsdelivr.net" crossorigin><link rel="stylesheet" href="/css/hexo-theme-yun.css"><link rel="alternate" href="/atom.xml" title="秩同道合的小站"><script id="yun-config">
    let Yun = window.Yun || {};
    let CONFIG = {"root":"/","title":"秩同道合的小站","version":"0.3.1","anonymous_image":"https://cdn.jsdelivr.net/gh/YunYouJun/cdn/img/avatar/none.jpg","say":{"api":"https://v1.hitokoto.cn","hitokoto":true},"valine":{"el":"#valine-container","verify":false,"notify":false,"appId":"eCgP91hRSX8OtvCIR4MgLfcl-gzGzoHsz","appKey":"N5gVT8kUx5O0wMvc47SU040Y","serverURLs":null,"placeholder":"大佬求指教&nbsp_(:з」∠)_ （填写邮箱可以收到回复通知～）","avatar":null,"meta":["nick","mail","link"],"pageSize":10,"lang":"zh-cn","visitor":true},"fireworks":{"colors":["102, 167, 221","62, 131, 225","33, 78, 194"]}};
  </script><script src="//at.alicdn.com/t/font_1140697_pa6cswvjpq.js" async></script><meta name="generator" content="Hexo 4.2.0"><link rel="alternate" href="/atom.xml" title="秩同道合的小站" type="application/atom+xml">
</head><body><script defer src="https://cdn.jsdelivr.net/npm/animejs@latest/anime.min.js"></script><script defer src="/js/ui/fireworks.js"></script><canvas class="fireworks"></canvas><div class="container"><a class="sidebar-toggle sidebar-toggle-fixed hty-icon-button"><div class="hamburger hamburger--spin" type="button"><span class="hamburger-box"><span class="hamburger-inner"></span></span></div></a><aside class="sidebar"><ul class="sidebar-nav"><li class="sidebar-nav-item sidebar-nav-toc sidebar-nav-active hty-icon-button" data-target="post-toc-wrap" title="文章目录"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-list-ordered"></use></svg></li><li class="sidebar-nav-item sidebar-nav-overview hty-icon-button" data-target="site-overview-wrap" title="站点概览"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-passport-line"></use></svg></li></ul><div class="sidebar-panel" id="site-overview-wrap"><div class="site-info fix-top"><a class="site-author-avatar" href="/about" title="秩同道合"><img loading="lazy" src="https://cdn.jsdelivr.net/gh/AlexZou14/CDN/img/touxiang.jpg" alt="秩同道合"></a><div class="site-author-name"><a href="/about">秩同道合</a></div><a class="site-name" href="/about/site.html">秩同道合的小站</a><sub class="site-subtitle">寻找志趣相投的伙伴！</sub><div class="site-desciption">我和你，以及我们的秩相同所以我们才等价！</div></div><nav class="site-state"><a class="site-state-item hty-icon-button icon-home" href="/" title="首页"><span class="site-state-item-icon"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-home-4-line"></use></svg></span></a><div class="site-state-item site-state-posts"><a href="/archives" title="归档"><span class="site-state-item-icon"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-archive-line"></use></svg></span><span class="site-state-item-count">28</span></a></div><div class="site-state-item site-state-categories"><a href="/categories" title="分类"><span class="site-state-item-icon"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-folder-2-line"></use></svg></span><span class="site-state-item-count">2</span></a></div><div class="site-state-item site-state-tags"><a href="/tags" title="标签"><span class="site-state-item-icon"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-price-tag-3-line"></use></svg></span><span class="site-state-item-count">5</span></a></div><a class="site-state-item hty-icon-button" href="https://github.com/AlexZou14" target="_blank" rel="noopener" title="reward.comment"><span class="site-state-item-icon"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-hand-coin-line"></use></svg></span></a></nav><hr style="margin-bottom:0.5rem"><div class="links-of-author"><a class="links-of-author-item hty-icon-button" rel="noopener" href="/atom.xml" title="RSS" target="_blank" style="color:orange"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-rss-line"></use></svg></a><a class="links-of-author-item hty-icon-button" rel="noopener" href="https://github.com/AlexZou14" title="GitHub" target="_blank" style="color:#181717"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-github-line"></use></svg></a><a class="links-of-author-item hty-icon-button" rel="noopener" href="mailto:1120375574@qq.com" title="E-Mail" target="_blank" style="color:#8E71C1"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-mail-line"></use></svg></a><a class="links-of-author-item hty-icon-button" rel="noopener" href="https://space.bilibili.com/19164044" title="哔哩哔哩" target="_blank" style="color:#FF8EB3"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-bilibili-line"></use></svg></a></div><hr style="margin:0.5rem 1rem"><div class="links"><a class="links-item hty-icon-button" href="http://sotavision.cn" target="_blank" rel="noopener" title="工作组" style="color:#000000"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-send-plane-2-line"></use></svg></a></div></div><script defer src="/js/sidebar.js"></script><div class="sidebar-panel sidebar-panel-active" id="post-toc-wrap"><div class="post-toc"><div class="post-toc-progress"><div class="progress-bar"></div><div class="progress-info"><span class="progress-notice">您已经读了</span><span class="progress-num">0</span><span class="progress-percentage">%</span></div></div><div class="post-toc-content"><ol class="toc"><li class="toc-item toc-level-3"><a class="toc-link" href="#Abstract"><span class="toc-number">1.</span> <span class="toc-text">Abstract</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Motivation"><span class="toc-number">2.</span> <span class="toc-text">Motivation</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Contributions"><span class="toc-number">3.</span> <span class="toc-text">Contributions</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Method"><span class="toc-number">4.</span> <span class="toc-text">Method</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Problem-Formulation"><span class="toc-number">4.1.</span> <span class="toc-text">Problem Formulation</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Proposed-Method"><span class="toc-number">4.2.</span> <span class="toc-text">Proposed Method</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#MANet"><span class="toc-number">4.2.1.</span> <span class="toc-text">MANet</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#Mutual-affine-convolution"><span class="toc-number">4.2.2.</span> <span class="toc-text">Mutual affine convolution</span></a></li></ol></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Experiments"><span class="toc-number">5.</span> <span class="toc-text">Experiments</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Conclusion"><span class="toc-number">6.</span> <span class="toc-text">Conclusion</span></a></li></ol></div></div></div></aside><main class="sidebar-translate" id="content"><div id="post"><article class="post-block" itemscope itemtype="http://schema.org/Article"><link itemprop="mainEntityOfPage" href="http://alexzou14.github.io/2021/09/28/MANet/"><span hidden itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="name" content="秩同道合"><meta itemprop="description" content="ICCV2021 盲图像超分MANet:空间可变模糊核估计的互仿射网络"></span><span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="秩同道合的小站"></span><header class="post-header"><h1 class="post-title" itemprop="name headline">ICCV2021 盲图像超分MANet:空间可变模糊核估计的互仿射网络</h1><div class="post-meta"><span class="post-time"><span class="post-meta-item-icon"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-calendar-line"></use></svg></span> <time title="创建时间：2021-09-28 15:29:33" itemprop="dateCreated datePublished" datetime="2021-09-28T15:29:33+08:00">2021-09-28</time></span><div class="post-count"><span class="post-symbolcount"><span class="post-meta-item-icon" title="本文字数"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-file-word-line"></use></svg></span> <span title="本文字数">6.4k</span><span class="post-meta-divider">-</span><span class="post-meta-item-icon" title="阅读时长"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-timer-line"></use></svg></span> <span title="阅读时长">13 分钟</span></span></div><span class="leancloud_visitors" id="/2021/09/28/MANet/" data-flag-title="ICCV2021 盲图像超分MANet:空间可变模糊核估计的互仿射网络"><span class="post-meta-item-icon" title="阅读次数"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-eye-line"></use></svg> <span class="leancloud-visitors-count"></span></span></span><div class="post-classify"><span class="post-category"><span class="post-meta-item-icon"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-folder-line"></use></svg></span> <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a class="category" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/" itemprop="url" rel="index"><span itemprop="text">论文笔记</span></a></span></span><span class="post-tag"><span class="post-meta-divider">-</span><a class="tag" href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"><span class="post-meta-item-icon"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-price-tag-3-line"></use></svg></span><span class="tag-name">深度学习</span></a><a class="tag" href="/tags/%E7%AC%94%E8%AE%B0/"><span class="post-meta-item-icon"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-price-tag-3-line"></use></svg></span><span class="tag-name">笔记</span></a><a class="tag" href="/tags/%E8%B6%85%E5%88%86%E8%BE%A8%E7%8E%87/"><span class="post-meta-item-icon"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-price-tag-3-line"></use></svg></span><span class="tag-name">超分辨率</span></a></span></div></div></header><section class="post-body" itemprop="articleBody"><div class="post-content post-markdown"><h3 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h3><p>目前大多数盲图像超分方法都假设模糊核在整个图像中都是一样的，然而真实场景图像中的模糊核是复杂多变的，这导致其方法很少适用于真实图像，从而导致这些方法在实际应用过程中性能下降。为了解决这些问题，作者提出了一个MANet(Mutual Affine Network)用于估计空间可变的模糊核。具体来说，MANet有两个显著的特点：</p>
<ul>
<li>它具有一个合适的感受野来保证退化的局部性。</li>
<li>它包含一个新的MAConv (Mutual Affine Conv)层，该层在不增加感受野、模型大小和计算负担的情况下增强了特征表达能力。</li>
</ul>
<p>在合成图像和真实图像上的大量实验表明，所提出的MANet不仅具有良好的空间变异和不变核估计性能，而且在与非盲SR方法相结合时还具有最先进的盲SR性能。</p>
<h3 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h3><img src="https://cdn.nlark.com/yuque/0/2021/png/22543740/1632795728697-1bf31255-c9e5-4c5a-8f86-9429124121ec.png" referrerpolicy="no-referrer" loading="lazy">

<p>目前盲图像超分方法假设模糊核在空间上是不变的，并且只估计整个图像的单个核，这导致了以下两个问题：</p>
<ul>
<li>现实世界中的模糊核通常在空间上是不同的，因此图像不同位置的模糊核应该是不同的。</li>
<li>即使在空间不变的假设下，估计整个图像的单个核也容易受到图像平坦区域的不利影响。因为在平坦区域估计出来的模糊核和边角区域的模糊核往往不尽相同，如图所示，因此估计单一模糊核是比较片面的。</li>
</ul>
<h3 id="Contributions"><a href="#Contributions" class="headerlink" title="Contributions"></a>Contributions</h3><p>本文主要贡献如下：</p>
<ul>
<li>作者提出了一个MANet的空间变换模糊核的估计框架。通过适当的感受野，他可以从较小的图块上估计模糊核。</li>
<li>作者提出了MAConv层，在不增加网络感受野的情况下，利用信道相互依赖性来增强特征表达能力，使其适合于模糊核的特征提取。与普通卷积层相比，模型参数和计算量降低了约30%。</li>
<li>与现有方法相比，MANet在空间变异和不变核估计方面都表现出良好的性能，与非盲SR模型相结合时可以获得最先进的盲SR性能。</li>
</ul>
<h3 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h3><h4 id="Problem-Formulation"><a href="#Problem-Formulation" class="headerlink" title="Problem Formulation"></a>Problem Formulation</h4><p>LR图像\( I^{LR} \)可以通过HR图像\( I^{HR} \)根据以下的图像降质量模型得到：</p>
<div>
    \[ I^{LR} = (k \otimes I^{HR})\downarrow_s + n \]
</div>

<p>由于模糊核在空间上是变化的，在这种情况下，图像和模糊核可以以向量形式写入。所以降质过程建模为：</p>
<div>
    \[ I^{LR} = (K \otimes I^{HR})\downarrow_s + n \]
</div>

<p>其中K表示与卷积矩阵类似的模糊矩阵。</p>
<h4 id="Proposed-Method"><a href="#Proposed-Method" class="headerlink" title="Proposed Method"></a>Proposed Method</h4><p>由不同核模糊的图像块具有不同的块分布。KernelGAN利用内部GAN的这一特性，并使用鉴别器将图像块区分为真假。然而，它只适用于空间不变的核估计，不能估计较小图像块的核。为了更进一步，作者提出了直接从图像块中估计核。</p>
<h5 id="MANet"><a href="#MANet" class="headerlink" title="MANet"></a>MANet</h5><p>目前神经网络通常堆叠多层以建立具有大感受野的深层模型。然而，对于空间可变核估计任务，我们需要保持退化的局部性。因此，我们提出了一种具有合适感受野的互仿射网络（MANet）。如下图所示：</p>
<img src="https://cdn.nlark.com/yuque/0/2021/png/22543740/1632797273533-4662f65f-9373-45e6-97e3-7ef820e79a81.png" referrerpolicy="no-referrer" loading="lazy">

<p>上图给出了MANet架构示意图，它包含特征提取与核重建两个模块。特征提取模块是一种类似UNet架构，由卷积、残差模块以及上/下采样构成；核重建模块由卷积、Softmax以及最近邻插值构成。预测得到的模糊核表示为\( K \in \mathcal{R}^{hw\times H \times W} \)。基于上述架构设计，MANet的感受野为\( 22\times 22 \)。</p>
<h5 id="Mutual-affine-convolution"><a href="#Mutual-affine-convolution" class="headerlink" title="Mutual affine convolution"></a>Mutual affine convolution</h5><p>一般来讲，小感受野意味着小网络、弱表达能力。一种可能的方案是提升通道数量，但这会带来指数级的参数量与计算量提升。为解决该问题，我们提出了MAConv，见下图。<br><img src="https://cdn.nlark.com/yuque/0/2021/png/22543740/1632799670040-dc1a7106-b0fb-4d63-9d44-f37214e741cf.png" referrerpolicy="no-referrer" loading="lazy"></p>
<p>作者首先将输入的\( x \in \mathcal{R}^{C_{in}\times H_f \times W_f} \)通道分离成S份，如下表达式所示：</p>
<div>
    \[ x_1, x_2, \cdots,x_S = \text{split}(x) \]
</div>

<p>对于\( x_i \in \mathcal{R}^{\frac{C_{in}}{S}\times H_f \times W_f} \), 我们采用\( x \in \mathcal{R}^{\frac{C_{in}(S-1)}{S}\times H_f \times W_f} \)为其互补信息。我们将上述两者送入到MAConv层进行处理：</p>
<div>
    \[ \beta_i, \gamma_i = \text{split}(\mathcal{F}(\bar{x}_i)) \]
</div>
<div>
    \[ y_i=\beta_i \odot x_i + \gamma_i \]
</div>

<p>在完成上述变换后，作者采用\( 3\times 3 \)卷积生成特征\( z_i = conv_i(y_i) \)。最后将所得特征进行拼接生成MAConv的输出：</p>
<div>
    \[ z = concat(z_1, z_2, \cdots, z_S) \]
</div>
MAConv通过互仿射变换探索了不同通道之间的相互关系，这种设计可以有提升特征表达能力，同时极大降低模型大小与计算复杂度。

<h3 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h3><p>下表对比了卷积、组卷积以及MAConv在参数量、内存占用、FLOPs以及推理耗时方面的对比。注：由于仿射变换不会提升感受野，MAConv的感受野仍为\( 3\times 3 \)；而稠密与SE模块会导致感受野极大提升而不适合于核估计。</p>
<img src="https://cdn.nlark.com/yuque/0/2021/png/22543740/1632813293355-cc809ab5-da2c-45ac-b9b5-3eee1659f264.png" referrerpolicy="no-referrer" loading="lazy">

<p>此外，从上表还可以看到：</p>
<ul>
<li>MAConv在LR图像上取得了最佳PSNR/SSIM指标，这说明所生成的模糊核可以更好的保持数据一致性；</li>
<li>提升通道数，MAConv的性能可以进一步提升，但同时也带来了参数量与FLOPs提升；</li>
<li>提升MAConv的split数同样会带来性能提升，这说明更多的split可以更好的探索通道相关性、提升特征表达能力。为平衡精度与推理耗时，我们将通道数与split数分别设置为\( [128,256,128],2 \)。</li>
</ul>
<img src="https://cdn.nlark.com/yuque/0/2021/png/22543740/1632813441744-1424df8b-67b5-449b-8324-eccc46bff3e4.png" referrerpolicy="no-referrer" loading="lazy">

<p>在真实应用场景，图像还可能存在噪声与压缩伪影。为测试在更复杂场景下的核估计性能，我们在训练过程中添加高斯与JPEG压缩噪声并在不同噪声水平下进行测试，参见上表。从表中可以看到：相比无噪情况，尽管出现了性能下降，但LR图像的PSNR范围仍为40.59-45.45dB，这无疑说明了所提方案在重度噪声干扰下的核估计性能。<br><img src="https://cdn.nlark.com/yuque/0/2021/png/22543740/1632813530485-7919cf59-8d4d-455e-b420-10184606f662.png" referrerpolicy="no-referrer" loading="lazy"></p>
<p>上表对比了不同盲超分方案的性能，从中可以看到：</p>
<ul>
<li>对比不同类型的空间可变核，所提MANet均取得了最佳性能；</li>
<li>当模糊核出现差异后，极具代表的BicubicSR模型RCAN与HAN出现了严重性能下降；</li>
<li>类似地，DIP也难以生成令人满意的记过，因其模糊核是固定的；</li>
<li>通过逐块优化核，SRSVD可以处理空间可变SR问题，但无疑会极大提升运行耗时；</li>
<li>IKC能够生成比其他方案更好的结果，但它对每个图像仅估计一个核，这无疑限制了其性能；</li>
<li>MANet在每个位置预测一个核，因此它可以处理空间可变退化问题，取得了大幅优于IKC的性能</li>
</ul>
<img src="https://cdn.nlark.com/yuque/0/2021/png/22543740/1632813589380-128f875a-747e-4849-a6b5-d6cbd9a40f83.png" referrerpolicy="no-referrer" loading="lazy">

<p>上图给出了几种方案在空间可变核与真实场景数据上的视觉效果对比，从中可以看到：MANet可以生成具有最佳视觉效果的结果 ，而其他方案要么存在过度模糊，要么存在过度锐化问题。<br><img src="https://cdn.nlark.com/yuque/0/2021/png/22543740/1632813708731-47ee3781-0817-4e53-83eb-3eedcb99ac57.png" referrerpolicy="no-referrer" loading="lazy"></p>
<p>上表对比了空间不变超分方案的性能，从中可以看到：</p>
<ul>
<li>在不同数据集、不同超分倍率下，所提MANet均取得了最佳性能 。</li>
<li>尽管KernelGAN可以从LR图像估计模糊核，但其性能与HAN、DIP相近；</li>
<li>IKC具有比其他方案更优的性能，但仍弱于所提MANet。</li>
</ul>
<h3 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h3><ul>
<li>作者提出了一种互仿射网络（MANet）用于空间变异的盲SR核估计。</li>
<li>MANet由特征提取和核重构模块组成，具有适度的感受野，以保持退化的局部性。</li>
<li>提出的互仿射卷积（MAConv）层，通过学习不同信道分裂之间的仿射变换来利用信道的相互依赖性，这可以在不增加模型感受野、模型大小和计算负担的情况下增强模型的表达能力。</li>
</ul>
</div><ul class="post-copyright"><li class="post-copyright-author"><strong>本文作者：</strong>秩同道合</li><li class="post-copyright-link"><strong>本文链接：</strong><a href="http://alexzou14.github.io/2021/09/28/MANet/" title="ICCV2021 盲图像超分MANet:空间可变模糊核估计的互仿射网络">http://alexzou14.github.io/2021/09/28/MANet/</a></li><li class="post-copyright-license"><strong>版权声明：</strong>本博客所有文章除特别声明外，均默认采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh" target="_blank" rel="noopener" title="CC BY-NC-SA 4.0 "><svg class="icon"><use xlink:href="#icon-creative-commons-line"></use></svg><svg class="icon"><use xlink:href="#icon-creative-commons-by-line"></use></svg><svg class="icon"><use xlink:href="#icon-creative-commons-nc-line"></use></svg><svg class="icon"><use xlink:href="#icon-creative-commons-sa-line"></use></svg></a> 许可协议。</li></ul></section></article><div class="post-nav"><div class="post-nav-item"><a class="post-nav-prev" href="/2021/09/28/mmediting/" rel="prev" title="mmediting 使用手册"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-arrow-left-s-line"></use></svg><span class="post-nav-text">mmediting 使用手册</span></a></div><div class="post-nav-item"><a class="post-nav-next" href="/2021/09/25/AMPRN/" rel="next" title="TIP2021 AMPRN:图像轻量超分辨率对抗网络"><span class="post-nav-text">TIP2021 AMPRN:图像轻量超分辨率对抗网络</span><svg class="icon" aria-hidden="true"><use xlink:href="#icon-arrow-right-s-line"></use></svg></a></div></div><div id="comment"><div class="comment-tooltip text-center"><span>若您无 GitHub 账号，可直接在下方匿名评论。</span><br><span>若您想及时得到回复提醒，建议跳转 GitHub Issues 评论。</span><br><span>若没有本文 Issue，您可以使用 Comment 模版新建。</span><br><a class="hty-button hty-button--raised" id="github-issues" href="https://github.com/AlexZou14/alexzou14.github.io/issues?q=is:issue+ICCV2021 盲图像超分MANet:空间可变模糊核估计的互仿射网络" target="_blank" rel="noopener">GitHub Issues</a></div><div class="comment-container" id="valine-container"></div></div><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.css"><script defer src="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.js"></script><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex@latest/dist/contrib/copy-tex.min.css"><script defer src="https://cdn.jsdelivr.net/npm/katex@latest/dist/contrib/copy-tex.min.js"></script><script defer src="https://cdn.jsdelivr.net/npm/katex@latest/dist/contrib/auto-render.min.js" onload="renderMathInElement(document.body);"></script></div></main><footer class="sidebar-translate" id="footer"><div class="copyright"><span>&copy; 2019 – 2021 </span><span class="with-love" id="animate"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-cloud-line"></use></svg></span><span class="author"> 秩同道合</span></div><div class="powered"><span>由 <a href="https://hexo.io" target="_blank" rel="noopener">Hexo</a> 驱动 v4.2.0</span><span class="footer-separator">|</span><span>主题 - <a rel="noopener" href="https://github.com/YunYouJun/hexo-theme-yun" target="_blank"><span>Yun</span></a> v0.3.1</span></div><script defer src="https://cdn.jsdelivr.net/npm/valine@latest/dist/Valine.min.js"></script><script>function initValine() {
  new Valine(CONFIG.valine);
}
document.addEventListener("DOMContentLoaded", function() {
  initValine();
});</script></footer><a class="hty-icon-button" id="goUp" aria-label="back-to-top" href="#"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-arrow-up-s-line"></use></svg></a></div><script defer src="/js/utils.js"></script><script defer src="/js/hexo-theme-yun.js"></script><script src="https://cdn.jsdelivr.net/npm/live2d-widget@^3.1.3/lib/L2Dwidget.min.js"></script><script>L2Dwidget.init({"pluginRootPath":"live2dw/","pluginJsPath":"lib/","pluginModelPath":"assets/","tagMode":false,"debug":false,"model":{"jsonPath":"https://cdn.jsdelivr.net/npm/live2d-widget-model-hijiki@1.0.5/assets/hijiki.model.json"},"display":{"position":"right","width":200,"height":400},"mobile":{"show":true},"react":{"opacity":0.8},"dialog":{"enable":true,"hitokoto":true},"log":false});</script></body></html>