<!DOCTYPE html><html lang="zh-CN"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0"><meta name="description" content="ICCV2021 注意力卷积新思路CRAN:上下文推理注意力图像超分辨率网络"><meta name="keywords" content="深度学习,笔记,超分辨率"><meta name="author" content="秩同道合"><meta name="copyright" content="秩同道合"><meta name="theme-color" content="#0078E7"><title>ICCV2021 注意力卷积新思路CRAN:上下文推理注意力图像超分辨率网络 | 秩同道合的小站</title><link rel="shortcut icon" href="https://cdn.jsdelivr.net/gh/AlexZou14/CDN/img/favicon.ico"><link rel="preload" href="/css/hexo-theme-yun.css" as="style"><link rel="preload" href="/js/utils.js" as="script"><link rel="preload" href="/js/hexo-theme-yun.js" as="script"><link rel="prefetch" href="/js/sidebar.js" as="script"><link rel="preconnect" href="https://cdn.jsdelivr.net" crossorigin><link rel="stylesheet" href="/css/hexo-theme-yun.css"><link rel="alternate" href="/atom.xml" title="秩同道合的小站"><script id="yun-config">
    let Yun = window.Yun || {};
    let CONFIG = {"root":"/","title":"秩同道合的小站","version":"0.3.1","anonymous_image":"https://cdn.jsdelivr.net/gh/YunYouJun/cdn/img/avatar/none.jpg","say":{"api":"https://v1.hitokoto.cn","hitokoto":true},"valine":{"el":"#valine-container","verify":false,"notify":false,"appId":"eCgP91hRSX8OtvCIR4MgLfcl-gzGzoHsz","appKey":"N5gVT8kUx5O0wMvc47SU040Y","serverURLs":null,"placeholder":"大佬求指教&nbsp_(:з」∠)_ （填写邮箱可以收到回复通知～）","avatar":null,"meta":["nick","mail","link"],"pageSize":10,"lang":"zh-cn","visitor":true},"fireworks":{"colors":["102, 167, 221","62, 131, 225","33, 78, 194"]}};
  </script><script src="//at.alicdn.com/t/font_1140697_pa6cswvjpq.js" async></script><meta name="generator" content="Hexo 4.2.0"><link rel="alternate" href="/atom.xml" title="秩同道合的小站" type="application/atom+xml">
</head><body><script defer src="https://cdn.jsdelivr.net/npm/animejs@latest/anime.min.js"></script><script defer src="/js/ui/fireworks.js"></script><canvas class="fireworks"></canvas><div class="container"><a class="sidebar-toggle sidebar-toggle-fixed hty-icon-button"><div class="hamburger hamburger--spin" type="button"><span class="hamburger-box"><span class="hamburger-inner"></span></span></div></a><aside class="sidebar"><ul class="sidebar-nav"><li class="sidebar-nav-item sidebar-nav-toc sidebar-nav-active hty-icon-button" data-target="post-toc-wrap" title="文章目录"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-list-ordered"></use></svg></li><li class="sidebar-nav-item sidebar-nav-overview hty-icon-button" data-target="site-overview-wrap" title="站点概览"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-passport-line"></use></svg></li></ul><div class="sidebar-panel" id="site-overview-wrap"><div class="site-info fix-top"><a class="site-author-avatar" href="/about" title="秩同道合"><img loading="lazy" src="https://cdn.jsdelivr.net/gh/AlexZou14/CDN/img/touxiang.jpg" alt="秩同道合"></a><div class="site-author-name"><a href="/about">秩同道合</a></div><a class="site-name" href="/about/site.html">秩同道合的小站</a><sub class="site-subtitle">寻找志趣相投的伙伴！</sub><div class="site-desciption">我和你，以及我们的秩相同所以我们才等价！</div></div><nav class="site-state"><a class="site-state-item hty-icon-button icon-home" href="/" title="首页"><span class="site-state-item-icon"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-home-4-line"></use></svg></span></a><div class="site-state-item site-state-posts"><a href="/archives" title="归档"><span class="site-state-item-icon"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-archive-line"></use></svg></span><span class="site-state-item-count">30</span></a></div><div class="site-state-item site-state-categories"><a href="/categories" title="分类"><span class="site-state-item-icon"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-folder-2-line"></use></svg></span><span class="site-state-item-count">2</span></a></div><div class="site-state-item site-state-tags"><a href="/tags" title="标签"><span class="site-state-item-icon"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-price-tag-3-line"></use></svg></span><span class="site-state-item-count">5</span></a></div><a class="site-state-item hty-icon-button" href="https://github.com/AlexZou14" target="_blank" rel="noopener" title="reward.comment"><span class="site-state-item-icon"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-hand-coin-line"></use></svg></span></a></nav><hr style="margin-bottom:0.5rem"><div class="links-of-author"><a class="links-of-author-item hty-icon-button" rel="noopener" href="/atom.xml" title="RSS" target="_blank" style="color:orange"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-rss-line"></use></svg></a><a class="links-of-author-item hty-icon-button" rel="noopener" href="https://github.com/AlexZou14" title="GitHub" target="_blank" style="color:#181717"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-github-line"></use></svg></a><a class="links-of-author-item hty-icon-button" rel="noopener" href="mailto:1120375574@qq.com" title="E-Mail" target="_blank" style="color:#8E71C1"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-mail-line"></use></svg></a><a class="links-of-author-item hty-icon-button" rel="noopener" href="https://space.bilibili.com/19164044" title="哔哩哔哩" target="_blank" style="color:#FF8EB3"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-bilibili-line"></use></svg></a></div><hr style="margin:0.5rem 1rem"><div class="links"><a class="links-item hty-icon-button" href="http://sotavision.cn" target="_blank" rel="noopener" title="工作组" style="color:#000000"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-send-plane-2-line"></use></svg></a></div></div><script defer src="/js/sidebar.js"></script><div class="sidebar-panel sidebar-panel-active" id="post-toc-wrap"><div class="post-toc"><div class="post-toc-progress"><div class="progress-bar"></div><div class="progress-info"><span class="progress-notice">您已经读了</span><span class="progress-num">0</span><span class="progress-percentage">%</span></div></div><div class="post-toc-content"><ol class="toc"><li class="toc-item toc-level-3"><a class="toc-link" href="#Abstract"><span class="toc-number">1.</span> <span class="toc-text">Abstract</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Method"><span class="toc-number">2.</span> <span class="toc-text">Method</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#上下文推理注意力卷积"><span class="toc-number">2.1.</span> <span class="toc-text">上下文推理注意力卷积</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#上下文信息提取"><span class="toc-number">2.1.1.</span> <span class="toc-text">上下文信息提取</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#上下文关系推理描述符"><span class="toc-number">2.1.2.</span> <span class="toc-text">上下文关系推理描述符</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#上下文推理注意卷积"><span class="toc-number">2.1.3.</span> <span class="toc-text">上下文推理注意卷积</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#上下文推理注意力图像超分辨率网络"><span class="toc-number">2.1.4.</span> <span class="toc-text">上下文推理注意力图像超分辨率网络</span></a></li></ol></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Experiments"><span class="toc-number">3.</span> <span class="toc-text">Experiments</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#消融实验"><span class="toc-number">3.1.</span> <span class="toc-text">消融实验</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#主要结果"><span class="toc-number">3.1.1.</span> <span class="toc-text">主要结果</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#卷积核的多样性"><span class="toc-number">3.1.2.</span> <span class="toc-text">卷积核的多样性</span></a></li></ol></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Conclusion"><span class="toc-number">4.</span> <span class="toc-text">Conclusion</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Reference"><span class="toc-number">5.</span> <span class="toc-text">Reference</span></a></li></ol></div></div></div></aside><main class="sidebar-translate" id="content"><div id="post"><article class="post-block" itemscope itemtype="http://schema.org/Article"><link itemprop="mainEntityOfPage" href="http://alexzou14.github.io/2021/10/26/CRAN/"><span hidden itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="name" content="秩同道合"><meta itemprop="description" content="ICCV2021 注意力卷积新思路CRAN:上下文推理注意力图像超分辨率网络"></span><span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="秩同道合的小站"></span><header class="post-header"><h1 class="post-title" itemprop="name headline">ICCV2021 注意力卷积新思路CRAN:上下文推理注意力图像超分辨率网络</h1><div class="post-meta"><span class="post-time"><span class="post-meta-item-icon"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-calendar-line"></use></svg></span> <time title="创建时间：2021-10-26 16:25:03" itemprop="dateCreated datePublished" datetime="2021-10-26T16:25:03+08:00">2021-10-26</time></span><div class="post-count"><span class="post-symbolcount"><span class="post-meta-item-icon" title="本文字数"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-file-word-line"></use></svg></span> <span title="本文字数">7.8k</span><span class="post-meta-divider">-</span><span class="post-meta-item-icon" title="阅读时长"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-timer-line"></use></svg></span> <span title="阅读时长">16 分钟</span></span></div><span class="leancloud_visitors" id="/2021/10/26/CRAN/" data-flag-title="ICCV2021 注意力卷积新思路CRAN:上下文推理注意力图像超分辨率网络"><span class="post-meta-item-icon" title="阅读次数"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-eye-line"></use></svg> <span class="leancloud-visitors-count"></span></span></span><div class="post-classify"><span class="post-category"><span class="post-meta-item-icon"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-folder-line"></use></svg></span> <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a class="category" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/" itemprop="url" rel="index"><span itemprop="text">论文笔记</span></a></span></span><span class="post-tag"><span class="post-meta-divider">-</span><a class="tag" href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"><span class="post-meta-item-icon"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-price-tag-3-line"></use></svg></span><span class="tag-name">深度学习</span></a><a class="tag" href="/tags/%E7%AC%94%E8%AE%B0/"><span class="post-meta-item-icon"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-price-tag-3-line"></use></svg></span><span class="tag-name">笔记</span></a><a class="tag" href="/tags/%E8%B6%85%E5%88%86%E8%BE%A8%E7%8E%87/"><span class="post-meta-item-icon"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-price-tag-3-line"></use></svg></span><span class="tag-name">超分辨率</span></a></span></div></div></header><section class="post-body" itemprop="articleBody"><div class="post-content post-markdown"><p>论文链接：<a href="https://openaccess.thecvf.com/content/ICCV2021/papers/Zhang_Context_Reasoning_Attention_Network_for_Image_Super-Resolution_ICCV_2021_paper.pdf" target="_blank" rel="noopener">Context Reasoning Attention Network for Image Super-Resolution</a></p>
<h3 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h3><p>如图1中SAN[9]和RFANet[38]所示，由于CNN中的基本卷积层大多用于提取局部特征，从而缺乏对全局上下文建模的能力，因此导致恢复出来的纹理细节都不正确。然而，利用全局上下文信息的CSNLN[41]方法都是通过将全局上下文合并到局部特征表示中进行全局特征交互而忽略了挖掘上下文信息之间的关系。</p>
<img src="https://cdn.nlark.com/yuque/0/2021/png/22543740/1635168718286-e905d4f5-7ee2-4d0c-b9a5-7dc209462508.png" referrerpolicy="no-referrer" loading="lazy">

<p>有神经科学表明神经元是根据上下文动态调节的，这一理论被大多数基于CNN的SR方法所忽略。作者基于这些观察和分析，提出了上下文推理注意网络（CRAN）来根据全局上下文自适应调整卷积核。具体来说，作者是提取了全局上下文描述符，并通过语义推理进一步增强了这些描述符。然后引入通道和空间交互来生成上下文推理注意掩码，并应用上下文推理注意掩码自适应地修改卷积核。<br>在这项工作中，作者的主要贡献有：</p>
<ul>
<li>作者提出了一种用于精确图像SR的上下文推理注意网络。我们的CRAN可以根据语义推理增强的全局上下文自适应地调整卷积核。</li>
<li>作者提出将上下文信息提取到潜在表示中，从而生成包含全局上下文描述符。作者进一步通过使用描述符与语义推理的关系来增强了描述符。</li>
<li>作者引入通道和空间交互来生成用于修改卷积核的上下文推理注意掩码。最后，我们得到了上下文推理注意卷积，这进一步作为构建图像SR块和网络的基础。</li>
</ul>
<h3 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h3><h4 id="上下文推理注意力卷积"><a href="#上下文推理注意力卷积" class="headerlink" title="上下文推理注意力卷积"></a>上下文推理注意力卷积</h4><img src="https://cdn.nlark.com/yuque/0/2021/png/22543740/1635214648004-4fea05b8-fa32-4647-8d6a-2ca953cf75ed.png" referrerpolicy="no-referrer" loading="lazy">

<p>作者在设计自适应修改滤波器的卷积借鉴了Context Guided Conv[37]，在中间添加了对应上下文的注意力关系以及对应的通道交互和空间交互操作，具体如下图：</p>
<img src="https://cdn.nlark.com/yuque/0/2021/png/22543740/1635214988896-2c02abe3-17e0-4cc2-a299-9fa3919072d1.png" referrerpolicy="no-referrer" loading="lazy">

<h5 id="上下文信息提取"><a href="#上下文信息提取" class="headerlink" title="上下文信息提取"></a>上下文信息提取</h5><p>为了提取上下文信息，作者首先通过使用池层将输入特征\(F_{in}\)的空间\(c_{in}\times h \times w\)大小减小到\(c_{in}\times h’ \times w’\)，然后通过一个共享的线性层，其权重为\(W_E \in \mathbb{R}^{h’\times w’ \times e}\)将每个通道投影到大小为\(e \)的潜在向量。按照之前Context Guided Conv的设计，我们将向量大小\(e\)设为\(\frac{k_1\times k_2}{2}\)，从而获得具有上下文信息的新特征，表示为\(F_C\in \mathbb{R}^{c_{in}\times e}\)。然后作者又将全局上下文信息写成一组向量\(F_C=[ \textbf{f}_1,\cdots, \textbf{f}_e ] \in \mathbb{R}^{c_{in}\times e}\)。</p>
<h5 id="上下文关系推理描述符"><a href="#上下文关系推理描述符" class="headerlink" title="上下文关系推理描述符"></a>上下文关系推理描述符</h5><p>基于之前的卷积推理工作，作者构建了上下文描述符之间的关系推理模型。具体地说，通过权重参数\(W_{\varphi}, W_{\phi}\)将上下文描述符嵌入到两个嵌入空间中。然后，正对的关系函数可以表达为：</p>
<div>
    \[ R(\textbf{f}_i, \textbf{f}_j)=(W_{\varphi}\textbf{f}_i)^T(W_{\phi}\textbf{f}_j) \]
</div>

<p>它获取每两个学习的上下文描述符\(\textbf{f}_i\)和\(\textbf{f}_j\)之间的关系，从而生成一个图。然后通过一个残差学习将\(F_C\)和原始输入桥接得到最终的全局上下文关系：</p>
<div>
    \[ F_C^* = \sigma([(R{F_C}^TW_g)W_r]^T)\odot F_C + F_C \]
</div>

<h5 id="上下文推理注意卷积"><a href="#上下文推理注意卷积" class="headerlink" title="上下文推理注意卷积"></a>上下文推理注意卷积</h5><p>作者采用增强的全局上下文信息\(F_C^*\)来更新卷积核，从而得到最终的注意力遮罩\(F_A \in \mathbb{R}^{c_{out}\times c_{in} \times k_1 \times k_2}\)。为了尽可能减少空间复杂度，作者将这个卷积遮罩分解成\(F_{A1}\in \mathbb{R}^{c_{out}\times k_1 \times k_2}\)和\(F_{A2}\in \mathbb{R}^{c_{in}\times k_1 \times k_2}\)<br>。然后分别利用空间交互和通道交互来得到\(F_{A1}\)和\(F_{A2}\)。<br><strong>通道相互作用：</strong>其中<strong>通道相互作用</strong>采用了深度可分离卷积来减少计算量，通过一个权重为\(W_{ci}\in \mathbb{R}^{\frac{c_{in}}{g}\times \frac{c_{out}}{g}}\)分组线性层进行投影。最后得到通道交互特征\(F_{CI}\in \mathbb{R}^{c_{out}\times e}\)。<br><strong>空间相互作用：</strong>然后，我们分别对\(F_C^*\)和\(F_{CI}\)分别进行<strong>空间相互作用</strong>，得到相应的张量\(F_{A1}\)和\(F_{A2}\)。具体来说就是利用两个权重共享的线性层将这两个特征\(F_C^*\)和\(F_{CI}\)映射为\(F_{A1}\)和\(F_{A2}\)，记作\(F_{A1} = F_{CI}W_{A1}\)和\(F_{A2} = F^*_{C}W_{A2}\)。<br><strong>上下文推理注意力卷积：</strong>在进行通道和空间交互之后， 作者直接利用\(F_{A1}\)和\(F_{A2}\)通过扩张通道数为\(c_{out}\times c_{in} \times k_1 \times k_2\)，然后再进行逐元素相加得到\(F_A\)。</p>
<div>
    \[ F_A = F_{A1} \oplus F_{A2} \]
    \[ (F_A)_{h,i,j,k} = \sigma((F_{A1})_{h,j,k}+(F_{A2})_{i,j,k}) \]
​</div>
最后，我们可以应用注意掩码\\(F_A\\)来调制卷积核权重\\(W\\)，如下所示：
<div>
    \[ W^* = W \odot F_A \]
</div>

<h5 id="上下文推理注意力图像超分辨率网络"><a href="#上下文推理注意力图像超分辨率网络" class="headerlink" title="上下文推理注意力图像超分辨率网络"></a>上下文推理注意力图像超分辨率网络</h5><img src="https://cdn.nlark.com/yuque/0/2021/png/22543740/1635232305547-ae3e1128-ae40-4c63-bb98-7f9521390c5d.png?x-oss-process=image%2Fresize%2Cw_1210%2Climit_0" referrerpolicy="no-referrer" loading="lazy">
作者采用了RCAN的网络结构，将原有的RCAN中的RCAB模块替换成了CRAB模块，其中CRAB就是利用了作者提出的上下文推理注意力卷积来进行构建的。采用了和RCAN中的参数设置，并且进行了一系列的消融实验证明作者提出的模块的有效性。

<h3 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h3><h4 id="消融实验"><a href="#消融实验" class="headerlink" title="消融实验"></a>消融实验</h4><img src="https://cdn.nlark.com/yuque/0/2021/png/22543740/1635232678229-8f7e4cee-371e-4765-af9d-4e167a8534dd.png" referrerpolicy="no-referrer" loading="lazy">
可以从表1中可以看出，包含注意力的模块可以获得比普通残差快更高的性能。作者提出的CRAB可以有效的考虑全局上下文的关系，从而获得好的性能，然后作者的模块通过CDRR，实现了进一步的性能提升，这证明了CDRR的有效性。
<img src="https://cdn.nlark.com/yuque/0/2021/png/22543740/1635233160784-31904da5-6147-4de4-a119-918b5cbdc5cc.png?x-oss-process=image%2Fresize%2Cw_1213%2Climit_0" referrerpolicy="no-referrer" loading="lazy">
如表2所示，作者提供了空间交互和通道交互组件的几种组合，可以发现每个组件都有助于提高性能。这证明了空间交互和通道交互操作的有效性。

<h5 id="主要结果"><a href="#主要结果" class="headerlink" title="主要结果"></a>主要结果</h5><img src="https://cdn.nlark.com/yuque/0/2021/png/22543740/1635233236387-a776f2c8-9409-4b40-bd10-e36a7ee13554.png?x-oss-process=image%2Fresize%2Cw_1183%2Climit_0" referrerpolicy="no-referrer" loading="lazy">
<img src="https://cdn.nlark.com/yuque/0/2021/png/22543740/1635235071733-e80dd1b0-1377-419c-9199-09f4ede43478.png?x-oss-process=image%2Fresize%2Cw_648%2Climit_0" referrerpolicy="no-referrer" loading="lazy">
上表对比了不同注意力超分方案的性能，从中可以看到：

<ul>
<li>作者提出的方法在所有的数据集上可以获得最佳的PSNR和SSIM。、</li>
<li>与RCAN相比，作者的方法通过修改其中的注意力模块从而得到了卓越的性能，这进一步证明了作者提出的CRAN可以通过调整Conv层内核和全局上下文推理注意来进一步提高性能。</li>
<li>作者提出的方法不仅在BI的降质过程上取得好的效果，在BD的降质表现上也取得了优异的性能。</li>
</ul>
<img src="https://cdn.nlark.com/yuque/0/2021/png/22543740/1635235239780-c6be3dd7-10d5-4356-ba99-1dc2746d070b.png?x-oss-process=image%2Fresize%2Cw_1305%2Climit_0" referrerpolicy="no-referrer" loading="lazy">
上图对比了不同方法在纹理细节恢复上的效果对比，可以看到：通过作者提出的全局上下文推理注意力卷积可以有效的恢复出正确的纹理细节。

<h5 id="卷积核的多样性"><a href="#卷积核的多样性" class="headerlink" title="卷积核的多样性"></a>卷积核的多样性</h5><img src="https://cdn.nlark.com/yuque/0/2021/png/22543740/1635235388328-52862e55-1435-4d23-9087-44c967761ccf.png" referrerpolicy="no-referrer" loading="lazy">
作者为了调查卷积核的多样性，作者考虑计算\\(F_A\\)和全为1的矩阵\\(I
\\)的欧氏距离，作者将100张图像随机转发到网络中，并计算每个样本的距离。如上图所示，可以看出：作者提出的卷积是根据图像进行自适应调整的，因此整个图像是波动的。
​

<h3 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h3><ul>
<li>作者借鉴了Context Guided Conv方法提出了一种全局上下文推理注意力卷积CRAC。</li>
<li>作者借鉴了其他推理网络从而提出了上下文关系推理描述符（CDRR），从而进一步增强描述符的上下文关系。</li>
<li>将提出来的CRAC应用到RCAN中获得了卓越的超分辨率性能。</li>
</ul>
<h3 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h3><p>[9] Tao Dai, Jianrui Cai, Y ongbing Zhang, Shu-Tao Xia, and Lei Zhang. Second-order attention network for single image super-resolution. In CVPR, 2019.</p>
<p>[37] Xudong Lin, Lin Ma, Wei Liu, and Shih-Fu Chang. Context-gated convolution. In ECCV, 2020.</p>
<p>[38] Jie Liu, Wenjie Zhang, Y uting Tang, Jie Tang, and Gangshan Wu. Residual feature aggregation network for image super-resolution. In CVPR, 2020.</p>
<p>[41] Yiqun Mei, Y uchen Fan, Y uqian Zhou, Lichao Huang, Thomas S Huang, and Humphrey Shi.  Image super-resolution with cross-scale non-local attention and exhaustive self-exemplars mining. In CVPR, 2020.</p>
</div><ul class="post-copyright"><li class="post-copyright-author"><strong>本文作者：</strong>秩同道合</li><li class="post-copyright-link"><strong>本文链接：</strong><a href="http://alexzou14.github.io/2021/10/26/CRAN/" title="ICCV2021 注意力卷积新思路CRAN:上下文推理注意力图像超分辨率网络">http://alexzou14.github.io/2021/10/26/CRAN/</a></li><li class="post-copyright-license"><strong>版权声明：</strong>本博客所有文章除特别声明外，均默认采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh" target="_blank" rel="noopener" title="CC BY-NC-SA 4.0 "><svg class="icon"><use xlink:href="#icon-creative-commons-line"></use></svg><svg class="icon"><use xlink:href="#icon-creative-commons-by-line"></use></svg><svg class="icon"><use xlink:href="#icon-creative-commons-nc-line"></use></svg><svg class="icon"><use xlink:href="#icon-creative-commons-sa-line"></use></svg></a> 许可协议。</li></ul></section></article><div class="post-nav"><div class="post-nav-item"></div><div class="post-nav-item"><a class="post-nav-next" href="/2021/10/04/SwinIR/" rel="next" title="ICCVW2021 SwinIR:ETH结合Swin Transformer提出SOTA复原方案"><span class="post-nav-text">ICCVW2021 SwinIR:ETH结合Swin Transformer提出SOTA复原方案</span><svg class="icon" aria-hidden="true"><use xlink:href="#icon-arrow-right-s-line"></use></svg></a></div></div><div id="comment"><div class="comment-tooltip text-center"><span>若您无 GitHub 账号，可直接在下方匿名评论。</span><br><span>若您想及时得到回复提醒，建议跳转 GitHub Issues 评论。</span><br><span>若没有本文 Issue，您可以使用 Comment 模版新建。</span><br><a class="hty-button hty-button--raised" id="github-issues" href="https://github.com/AlexZou14/alexzou14.github.io/issues?q=is:issue+ICCV2021 注意力卷积新思路CRAN:上下文推理注意力图像超分辨率网络" target="_blank" rel="noopener">GitHub Issues</a></div><div class="comment-container" id="valine-container"></div></div><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.css"><script defer src="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.js"></script><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex@latest/dist/contrib/copy-tex.min.css"><script defer src="https://cdn.jsdelivr.net/npm/katex@latest/dist/contrib/copy-tex.min.js"></script><script defer src="https://cdn.jsdelivr.net/npm/katex@latest/dist/contrib/auto-render.min.js" onload="renderMathInElement(document.body);"></script></div></main><footer class="sidebar-translate" id="footer"><div class="copyright"><span>&copy; 2019 – 2021 </span><span class="with-love" id="animate"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-cloud-line"></use></svg></span><span class="author"> 秩同道合</span></div><div class="powered"><span>由 <a href="https://hexo.io" target="_blank" rel="noopener">Hexo</a> 驱动 v4.2.0</span><span class="footer-separator">|</span><span>主题 - <a rel="noopener" href="https://github.com/YunYouJun/hexo-theme-yun" target="_blank"><span>Yun</span></a> v0.3.1</span></div><script defer src="https://cdn.jsdelivr.net/npm/valine@latest/dist/Valine.min.js"></script><script>function initValine() {
  new Valine(CONFIG.valine);
}
document.addEventListener("DOMContentLoaded", function() {
  initValine();
});</script></footer><a class="hty-icon-button" id="goUp" aria-label="back-to-top" href="#"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-arrow-up-s-line"></use></svg></a></div><script defer src="/js/utils.js"></script><script defer src="/js/hexo-theme-yun.js"></script><script src="https://cdn.jsdelivr.net/npm/live2d-widget@^3.1.3/lib/L2Dwidget.min.js"></script><script>L2Dwidget.init({"pluginRootPath":"live2dw/","pluginJsPath":"lib/","pluginModelPath":"assets/","tagMode":false,"debug":false,"model":{"jsonPath":"https://cdn.jsdelivr.net/npm/live2d-widget-model-hijiki@1.0.5/assets/hijiki.model.json"},"display":{"position":"right","width":200,"height":400},"mobile":{"show":true},"react":{"opacity":0.8},"dialog":{"enable":true,"hitokoto":true},"log":false});</script></body></html>